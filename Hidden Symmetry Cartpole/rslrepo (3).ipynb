{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIaL99HH68Bd",
        "outputId": "29aa17d4-9300-44ef-bf19-6cf5d7ead6ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'rsl_rl' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/leggedrobotics/rsl_rl.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_iyHb3F7Clu",
        "outputId": "2c28153d-1cfb-4288-db2d-d7841e256de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/rsl_rl\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Change to the rsl_rl directory\n",
        "os.chdir('/content/rsl_rl')\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J1b_4kf7FFN",
        "outputId": "436340a2-168d-4f91-d050-bc81d4ba3170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/rsl_rl\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Checking if build backend supports build_editable: started\n",
            "  Checking if build backend supports build_editable: finished with status 'done'\n",
            "  Getting requirements to build editable: started\n",
            "  Getting requirements to build editable: finished with status 'done'\n",
            "  Preparing editable metadata (pyproject.toml): started\n",
            "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: torch>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from rsl-rl-lib==3.2.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from rsl-rl-lib==3.2.0) (0.24.0+cu126)\n",
            "Requirement already satisfied: tensordict>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from rsl-rl-lib==3.2.0) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.12/dist-packages (from rsl-rl-lib==3.2.0) (2.0.2)\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.12/dist-packages (from rsl-rl-lib==3.2.0) (3.1.45)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (from rsl-rl-lib==3.2.0) (1.20.0)\n",
            "Requirement already satisfied: onnxscript>=0.5.4 in /usr/local/lib/python3.12/dist-packages (from rsl-rl-lib==3.2.0) (0.5.7)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnxscript>=0.5.4->rsl-rl-lib==3.2.0) (0.5.4)\n",
            "Requirement already satisfied: onnx_ir<2,>=0.1.12 in /usr/local/lib/python3.12/dist-packages (from onnxscript>=0.5.4->rsl-rl-lib==3.2.0) (0.1.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxscript>=0.5.4->rsl-rl-lib==3.2.0) (25.0)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from onnxscript>=0.5.4->rsl-rl-lib==3.2.0) (4.15.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx->rsl-rl-lib==3.2.0) (5.29.5)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from tensordict>=0.7.0->rsl-rl-lib==3.2.0) (3.1.2)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from tensordict>=0.7.0->rsl-rl-lib==3.2.0) (8.7.0)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.12/dist-packages (from tensordict>=0.7.0->rsl-rl-lib==3.2.0) (3.11.5)\n",
            "Requirement already satisfied: pyvers<0.2.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from tensordict>=0.7.0->rsl-rl-lib==3.2.0) (0.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6.0->rsl-rl-lib==3.2.0) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.5.0->rsl-rl-lib==3.2.0) (11.3.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from GitPython->rsl-rl-lib==3.2.0) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->GitPython->rsl-rl-lib==3.2.0) (5.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.6.0->rsl-rl-lib==3.2.0) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->tensordict>=0.7.0->rsl-rl-lib==3.2.0) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.6.0->rsl-rl-lib==3.2.0) (3.0.3)\n",
            "Building wheels for collected packages: rsl-rl-lib\n",
            "  Building editable for rsl-rl-lib (pyproject.toml): started\n",
            "  Building editable for rsl-rl-lib (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for rsl-rl-lib: filename=rsl_rl_lib-3.2.0-0.editable-py3-none-any.whl size=5934 sha256=505d5a8bc76dc3663d940c836237055a415fca02d46a893a3b2deeb907eb0f09\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_mp6y07y/wheels/b6/b0/60/a45bf1744a4c125432189eb3b101fee48d5d8255d922796663\n",
            "Successfully built rsl-rl-lib\n",
            "Installing collected packages: rsl-rl-lib\n",
            "  Attempting uninstall: rsl-rl-lib\n",
            "    Found existing installation: rsl-rl-lib 3.2.0\n",
            "    Uninstalling rsl-rl-lib-3.2.0:\n",
            "      Successfully uninstalled rsl-rl-lib-3.2.0\n",
            "Successfully installed rsl-rl-lib-3.2.0\n",
            "\n",
            "\n",
            "tensordict installed successfully.\n"
          ]
        }
      ],
      "source": [
        "pip_install_command = '!pip install -e .'\n",
        "\n",
        "# Execute the pip install command\n",
        "import subprocess\n",
        "result = subprocess.run(pip_install_command[1:].split(), capture_output=True, text=True, shell=False)\n",
        "\n",
        "# Print stdout and stderr for visibility\n",
        "print(result.stdout)\n",
        "print(result.stderr)\n",
        "\n",
        "# Verify tensordict installation\n",
        "try:\n",
        "    import tensordict\n",
        "    print(\"tensordict installed successfully.\")\n",
        "except ImportError:\n",
        "    print(\"tensordict not found. Please ensure it's installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIu4m7AC7pJ9",
        "outputId": "15ef8a9e-8055-41f1-919b-efec3cb3e37b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rsl_rl components imported successfully.\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "from tensordict import TensorDict\n",
        "\n",
        "# Import RSL-RL components\n",
        "from rsl_rl.modules import ActorCritic\n",
        "from rsl_rl.storage import RolloutStorage\n",
        "from rsl_rl.algorithms import PPO\n",
        "from rsl_rl.utils import resolve_obs_groups\n",
        "from rsl_rl.modules import ActorCritic\n",
        "print(\"rsl_rl components imported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General Steps followed on repository\n"
      ],
      "metadata": {
        "id": "rzEvxsyOttwe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_wxxzAf1Bm8",
        "outputId": "2d606c3a-fada-4178-a0de-37718b3967df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'runner': {'class_name': 'OnPolicyRunner', 'num_steps_per_env': 24, 'max_iterations': 1500, 'seed': 1, 'obs_groups': {'policy': ['policy'], 'critic': ['policy', 'privileged']}, 'save_interval': 50, 'experiment_name': 'walking_experiment', 'run_name': '', 'logger': 'tensorboard', 'neptune_project': 'legged_gym', 'wandb_project': 'legged_gym', 'policy': {'class_name': 'ActorCritic', 'activation': 'elu', 'actor_obs_normalization': False, 'critic_obs_normalization': False, 'actor_hidden_dims': [256, 256, 256], 'critic_hidden_dims': [256, 256, 256], 'init_noise_std': 1.0, 'noise_std_type': 'scalar', 'state_dependent_std': False}, 'algorithm': {'class_name': 'PPO', 'learning_rate': 0.001, 'num_learning_epochs': 5, 'num_mini_batches': 4, 'schedule': 'adaptive', 'value_loss_coef': 1.0, 'clip_param': 0.2, 'use_clipped_value_loss': True, 'desired_kl': 0.01, 'entropy_coef': 0.01, 'gamma': 0.99, 'lam': 0.95, 'max_grad_norm': 1.0, 'normalize_advantage_per_mini_batch': False, 'rnd_cfg': {'weight': 0.0, 'weight_schedule': None, 'reward_normalization': False, 'learning_rate': 0.001, 'num_outputs': 1, 'predictor_hidden_dims': [-1], 'target_hidden_dims': [-1]}, 'symmetry_cfg': {'use_data_augmentation': True, 'use_mirror_loss': False, 'data_augmentation_func': None, 'mirror_loss_coeff': 0.0}}}}\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "\n",
        "yaml_config_string = \"\"\"\n",
        "runner:\n",
        "  class_name: OnPolicyRunner\n",
        "  # General\n",
        "  num_steps_per_env: 24  # Number of steps per environment per iteration\n",
        "  max_iterations: 1500   # Number of policy updates\n",
        "  seed: 1\n",
        "  # Observations\n",
        "  obs_groups: {\"policy\": [\"policy\"], \"critic\": [\"policy\", \"privileged\"]}  # Maps observation groups to sets. See `vec_env.py` for more information\n",
        "  # Logging parameters\n",
        "  save_interval: 50  # Check for potential saves every `save_interval` iterations\n",
        "  experiment_name: walking_experiment\n",
        "  run_name: \"\"\n",
        "  # Logging writer\n",
        "  logger: tensorboard  # tensorboard, neptune, wandb\n",
        "  neptune_project: legged_gym\n",
        "  wandb_project: legged_gym\n",
        "\n",
        "  # Policy\n",
        "  policy:\n",
        "    class_name: ActorCritic\n",
        "    activation: elu\n",
        "    actor_obs_normalization: false\n",
        "    critic_obs_normalization: false\n",
        "    actor_hidden_dims: [256, 256, 256]\n",
        "    critic_hidden_dims: [256, 256, 256]\n",
        "    init_noise_std: 1.0\n",
        "    noise_std_type: \"scalar\"  # 'scalar' or 'log'\n",
        "    state_dependent_std: false\n",
        "\n",
        "  # Algorithm\n",
        "  algorithm:\n",
        "    class_name: PPO\n",
        "    # Training\n",
        "    learning_rate: 0.001\n",
        "    num_learning_epochs: 5\n",
        "    num_mini_batches: 4  # mini batch size = num_envs * num_steps / num_mini_batches\n",
        "    schedule: adaptive  # adaptive, fixed\n",
        "    # Value function\n",
        "    value_loss_coef: 1.0\n",
        "    clip_param: 0.2\n",
        "    use_clipped_value_loss: true\n",
        "    # Surrogate loss\n",
        "    desired_kl: 0.01\n",
        "    entropy_coef: 0.01\n",
        "    gamma: 0.99\n",
        "    lam: 0.95\n",
        "    max_grad_norm: 1.0\n",
        "    # Miscellaneous\n",
        "    normalize_advantage_per_mini_batch: false\n",
        "\n",
        "    # Random network distillation\n",
        "    rnd_cfg:\n",
        "      weight: 0.0  # Initial weight of the RND reward\n",
        "      weight_schedule: null  # This is a dictionary with a required key called \"mode\". Please check the RND module for more information\n",
        "      reward_normalization: false  # Whether to normalize RND reward\n",
        "      # Learning parameters\n",
        "      learning_rate: 0.001  # Learning rate for RND\n",
        "      # Network parameters\n",
        "      num_outputs: 1  # Number of outputs of RND network. Note: if -1, then the network will use dimensions of the observation\n",
        "      predictor_hidden_dims: [-1]  # Hidden dimensions of predictor network\n",
        "      target_hidden_dims: [-1]  # Hidden dimensions of target network\n",
        "\n",
        "    # Symmetry augmentation\n",
        "    symmetry_cfg:\n",
        "      use_data_augmentation: true  # This adds symmetric trajectories to the batch\n",
        "      use_mirror_loss: false  # This adds symmetry loss term to the loss function\n",
        "      data_augmentation_func: null  # String containing the module and function name to import\n",
        "      # Example: \"legged_gym.envs.locomotion.anymal_c.symmetry:get_symmetric_states\"\n",
        "      mirror_loss_coeff: 0.0  # Coefficient for symmetry loss term. If 0, no symmetry loss is used\n",
        "\"\"\"\n",
        "\n",
        "cfg = yaml.safe_load(yaml_config_string)\n",
        "print(cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4s360MA-1xip"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION\n",
        "# All rights reserved.\n",
        "#\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import warnings\n",
        "from tensordict import TensorDict\n",
        "\n",
        "from rsl_rl.algorithms import PPO\n",
        "from rsl_rl.env import VecEnv\n",
        "from rsl_rl.modules import (\n",
        "    ActorCritic,\n",
        "    ActorCriticCNN,\n",
        "    ActorCriticRecurrent,\n",
        "    resolve_rnd_config,\n",
        "    resolve_symmetry_config,\n",
        "    symmetry,\n",
        ")\n",
        "from rsl_rl.storage import RolloutStorage\n",
        "from rsl_rl.utils import resolve_obs_groups\n",
        "from rsl_rl.utils.logger import Logger\n",
        "\n",
        "\n",
        "class OnPolicyRunner:\n",
        "    \"\"\"On-policy runner for training and evaluation of actor-critic methods.\"\"\"\n",
        "\n",
        "    def __init__(self, env: CartPoleVecEnv, train_cfg: dict, log_dir: str | None = None, device: str = \"cpu\") -> None:\n",
        "        self.cfg = train_cfg\n",
        "        self.policy_cfg = train_cfg[\"policy\"]\n",
        "        self.alg_cfg = train_cfg[\"algorithm\"]\n",
        "        self.device = device\n",
        "        self.env = env\n",
        "\n",
        "        # Setup multi-GPU training if enabled\n",
        "        self._configure_multi_gpu()\n",
        "\n",
        "        # Query observations from environment for algorithm construction\n",
        "        obs = self.env.get_observations()\n",
        "        self.cfg[\"obs_groups\"] = resolve_obs_groups(obs, self.cfg[\"obs_groups\"], self._get_default_obs_sets())\n",
        "\n",
        "        # Create the algorithm\n",
        "        self.alg = self._construct_algorithm(obs)\n",
        "\n",
        "        # Create the logger\n",
        "        self.logger = Logger(\n",
        "            log_dir=log_dir,\n",
        "            cfg=self.cfg,\n",
        "            env_cfg=self.env.cfg,\n",
        "            num_envs=self.env.num_envs,\n",
        "            is_distributed=self.is_distributed,\n",
        "            gpu_world_size=self.gpu_world_size,\n",
        "            gpu_global_rank=self.gpu_global_rank,\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        self.current_learning_iteration = 0\n",
        "\n",
        "    def learn(self, num_learning_iterations: int, init_at_random_ep_len: bool = False) -> None:\n",
        "        # Randomize initial episode lengths (for exploration)\n",
        "        if init_at_random_ep_len:\n",
        "            self.env.episode_length_buf = torch.randint_like(\n",
        "                self.env.episode_length_buf, high=int(self.env.max_episode_length)\n",
        "            )\n",
        "\n",
        "        # Start learning\n",
        "        obs = self.env.get_observations().to(self.device)\n",
        "        self.train_mode()  # switch to train mode (for dropout for example)\n",
        "\n",
        "        # Ensure all parameters are in-synced\n",
        "        if self.is_distributed:\n",
        "            print(f\"Synchronizing parameters for rank {self.gpu_global_rank}...\")\n",
        "            self.alg.broadcast_parameters()\n",
        "\n",
        "        # Start training\n",
        "        start_it = self.current_learning_iteration\n",
        "        total_it = start_it + num_learning_iterations\n",
        "        for it in range(start_it, total_it):\n",
        "            start = time.time()\n",
        "            # Rollout\n",
        "            with torch.inference_mode():\n",
        "                for _ in range(self.cfg[\"num_steps_per_env\"]):\n",
        "                    # Sample actions\n",
        "                    actions = self.alg.act(obs)\n",
        "                    # Step the environment\n",
        "                    obs, rewards, dones, extras = self.env.step(actions.to(self.env.device))\n",
        "                    # Move to device\n",
        "                    obs, rewards, dones = (obs.to(self.device), rewards.to(self.device), dones.to(self.device))\n",
        "                    # Process the step\n",
        "                    self.alg.process_env_step(obs, rewards, dones, extras)\n",
        "                    # Extract intrinsic rewards (only for logging)\n",
        "                    intrinsic_rewards = self.alg.intrinsic_rewards if self.alg_cfg[\"rnd_cfg\"] else None\n",
        "                    # Book keeping\n",
        "                    self.logger.process_env_step(rewards, dones, extras, intrinsic_rewards)\n",
        "\n",
        "                stop = time.time()\n",
        "                collect_time = stop - start\n",
        "                start = stop\n",
        "\n",
        "                # Compute returns\n",
        "                self.alg.compute_returns(obs)\n",
        "\n",
        "            # Update policy\n",
        "            loss_dict = self.alg.update()\n",
        "\n",
        "            stop = time.time()\n",
        "            learn_time = stop - start\n",
        "            self.current_learning_iteration = it\n",
        "\n",
        "            # Log information\n",
        "            self.logger.log(\n",
        "                it=it,\n",
        "                start_it=start_it,\n",
        "                total_it=total_it,\n",
        "                collect_time=collect_time,\n",
        "                learn_time=learn_time,\n",
        "                loss_dict=loss_dict,\n",
        "                learning_rate=self.alg.learning_rate,\n",
        "                action_std=self.alg.policy.action_std,\n",
        "                rnd_weight=self.alg.rnd.weight if self.alg_cfg[\"rnd_cfg\"] else None,\n",
        "            )\n",
        "\n",
        "            # Save model\n",
        "            if it % self.cfg[\"save_interval\"] == 0:\n",
        "                self.save(os.path.join(self.logger.log_dir, f\"model_{it}.pt\"))  # type: ignore\n",
        "\n",
        "        # Save the final model after training\n",
        "        if self.logger.log_dir is not None and not self.logger.disable_logs:\n",
        "            self.save(os.path.join(self.logger.log_dir, f\"model_{self.current_learning_iteration}.pt\"))\n",
        "\n",
        "    def save(self, path: str, infos: dict | None = None) -> None:\n",
        "        # Save model\n",
        "        saved_dict = {\n",
        "            \"model_state_dict\": self.alg.policy.state_dict(),\n",
        "            \"optimizer_state_dict\": self.alg.optimizer.state_dict(),\n",
        "            \"iter\": self.current_learning_iteration,\n",
        "            \"infos\": infos,\n",
        "        }\n",
        "        # Save RND model if used\n",
        "        if self.alg_cfg[\"rnd_cfg\"]:\n",
        "            saved_dict[\"rnd_state_dict\"] = self.alg.rnd.state_dict()\n",
        "            if self.alg.rnd_optimizer:\n",
        "                saved_dict[\"rnd_optimizer_state_dict\"] = self.alg.rnd_optimizer.state_dict()\n",
        "        torch.save(saved_dict, path)\n",
        "\n",
        "        # Upload model to external logging services\n",
        "        self.logger.save_model(path, self.current_learning_iteration)\n",
        "\n",
        "    def load(self, path: str, load_optimizer: bool = True, map_location: str | None = None) -> dict:\n",
        "        loaded_dict = torch.load(path, weights_only=False, map_location=map_location)\n",
        "        # Load model\n",
        "        resumed_training = self.alg.policy.load_state_dict(loaded_dict[\"model_state_dict\"])\n",
        "        # Load RND model if used\n",
        "        if self.alg_cfg[\"rnd_cfg\"]:\n",
        "            self.alg.rnd.load_state_dict(loaded_dict[\"rnd_state_dict\"])\n",
        "        # Load optimizer if used\n",
        "        if load_optimizer and resumed_training:\n",
        "            # Algorithm optimizer\n",
        "            self.alg.optimizer.load_state_dict(loaded_dict[\"optimizer_state_dict\"])\n",
        "            # RND optimizer if used\n",
        "            if self.alg_cfg[\"rnd_cfg\"]:\n",
        "                self.alg.rnd_optimizer.load_state_dict(loaded_dict[\"rnd_optimizer_state_dict\"])\n",
        "        # Load current learning iteration\n",
        "        if resumed_training:\n",
        "            self.current_learning_iteration = loaded_dict[\"iter\"]\n",
        "        return loaded_dict[\"infos\"]\n",
        "\n",
        "    def get_inference_policy(self, device: str | None = None) -> callable:\n",
        "        self.eval_mode()  # Switch to evaluation mode (e.g. for dropout)\n",
        "        if device is not None:\n",
        "            self.alg.policy.to(device)\n",
        "        return self.alg.policy.act_inference\n",
        "\n",
        "    def train_mode(self) -> None:\n",
        "        # PPO\n",
        "        self.alg.policy.train()\n",
        "        # RND\n",
        "        if self.alg_cfg[\"rnd_cfg\"]:\n",
        "            self.alg.rnd.train()\n",
        "\n",
        "    def eval_mode(self) -> None:\n",
        "        # PPO\n",
        "        self.alg.policy.eval()\n",
        "        # RND\n",
        "        if self.alg_cfg[\"rnd_cfg\"]:\n",
        "            self.alg.rnd.eval()\n",
        "\n",
        "    def add_git_repo_to_log(self, repo_file_path: str) -> None:\n",
        "        self.logger.git_status_repos.append(repo_file_path)\n",
        "\n",
        "    def _get_default_obs_sets(self) -> list[str]:\n",
        "        \"\"\"Get the the default observation sets required for the algorithm.\n",
        "\n",
        "        .. note::\n",
        "            See :func:`resolve_obs_groups` for more details on the handling of observation sets.\n",
        "        \"\"\"\n",
        "        default_sets = [\"critic\"]\n",
        "        if \"rnd_cfg\" in self.alg_cfg and self.alg_cfg[\"rnd_cfg\"] is not None:\n",
        "            default_sets.append(\"rnd_state\")\n",
        "        return default_sets\n",
        "\n",
        "    def _configure_multi_gpu(self) -> None:\n",
        "        \"\"\"Configure multi-gpu training.\"\"\"\n",
        "        # Check if distributed training is enabled\n",
        "        self.gpu_world_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n",
        "        self.is_distributed = self.gpu_world_size > 1\n",
        "\n",
        "        # If not distributed training, set local and global rank to 0 and return\n",
        "        if not self.is_distributed:\n",
        "            self.gpu_local_rank = 0\n",
        "            self.gpu_global_rank = 0\n",
        "            self.multi_gpu_cfg = None\n",
        "            return\n",
        "\n",
        "        # Get rank and world size\n",
        "        self.gpu_local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
        "        self.gpu_global_rank = int(os.getenv(\"RANK\", \"0\"))\n",
        "\n",
        "        # Make a configuration dictionary\n",
        "        self.multi_gpu_cfg = {\n",
        "            \"global_rank\": self.gpu_global_rank,  # Rank of the main process\n",
        "            \"local_rank\": self.gpu_local_rank,  # Rank of the current process\n",
        "            \"world_size\": self.gpu_world_size,  # Total number of processes\n",
        "        }\n",
        "\n",
        "        # Check if user has device specified for local rank\n",
        "        if self.device != f\"cuda:{self.gpu_local_rank}\":\n",
        "            raise ValueError(\n",
        "                f\"Device '{self.device}' does not match expected device for local rank '{self.gpu_local_rank}'.\"\n",
        "            )\n",
        "        # Validate multi-GPU configuration\n",
        "        if self.gpu_local_rank >= self.gpu_world_size:\n",
        "            raise ValueError(\n",
        "                f\"Local rank '{self.gpu_local_rank}' is greater than or equal to world size '{self.gpu_world_size}'.\"\n",
        "            )\n",
        "        if self.gpu_global_rank >= self.gpu_world_size:\n",
        "            raise ValueError(\n",
        "                f\"Global rank '{self.gpu_global_rank}' is greater than or equal to world size '{self.gpu_world_size}'.\"\n",
        "            )\n",
        "\n",
        "        # Initialize torch distributed\n",
        "        torch.distributed.init_process_group(backend=\"nccl\", rank=self.gpu_global_rank, world_size=self.gpu_world_size)\n",
        "        # Set device to the local rank\n",
        "        torch.cuda.set_device(self.gpu_local_rank)\n",
        "\n",
        "    def _construct_algorithm(self, obs: TensorDict) -> PPO:\n",
        "        \"\"\"Construct the actor-critic algorithm.\"\"\"\n",
        "        # Resolve RND config if used\n",
        "        self.alg_cfg = resolve_rnd_config(self.alg_cfg, obs, self.cfg[\"obs_groups\"], self.env)\n",
        "\n",
        "        # Resolve symmetry config if used\n",
        "        self.alg_cfg = resolve_symmetry_config(self.alg_cfg, self.env)\n",
        "\n",
        "        # Resolve deprecated normalization config\n",
        "        if self.cfg.get(\"empirical_normalization\") is not None:\n",
        "            warnings.warn(\n",
        "                \"The `empirical_normalization` parameter is deprecated. Please set `actor_obs_normalization` and \"\n",
        "                \"`critic_obs_normalization` as part of the `policy` configuration instead.\",\n",
        "                DeprecationWarning,\n",
        "            )\n",
        "            if self.policy_cfg.get(\"actor_obs_normalization\") is None:\n",
        "                self.policy_cfg[\"actor_obs_normalization\"] = self.cfg[\"empirical_normalization\"]\n",
        "            if self.policy_cfg.get(\"critic_obs_normalization\") is None:\n",
        "                self.policy_cfg[\"critic_obs_normalization\"] = self.cfg[\"empirical_normalization\"]\n",
        "\n",
        "        # Initialize the policy\n",
        "        actor_critic_class = eval(self.policy_cfg.pop(\"class_name\"))\n",
        "        actor_critic: ActorCritic | ActorCriticRecurrent | ActorCriticCNN = actor_critic_class(\n",
        "            obs, self.cfg[\"obs_groups\"], self.env.num_actions, **self.policy_cfg\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Initialize the storage\n",
        "        storage = RolloutStorage(\n",
        "            \"rl\", self.env.num_envs, self.cfg[\"num_steps_per_env\"], obs, [self.env.num_actions], self.device\n",
        "        )\n",
        "\n",
        "        # Initialize the algorithm\n",
        "        alg_class = eval(self.alg_cfg.pop(\"class_name\"))\n",
        "        alg: PPO = alg_class(\n",
        "            actor_critic, storage, device=self.device, **self.alg_cfg, multi_gpu_cfg=self.multi_gpu_cfg\n",
        "        )\n",
        "\n",
        "        return alg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NvT_nsNFC-7x"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "from gymnasium import spaces\n",
        "from tensordict import TensorDict\n",
        "from rsl_rl.env import VecEnv\n",
        "\n",
        "\n",
        "class CartPoleVecEnv(VecEnv):\n",
        "    def __init__(self, num_envs: int = 8, device: str = \"cpu\", max_episode_length: int = 500):\n",
        "        self.num_envs = num_envs\n",
        "        self.device = torch.device(device)\n",
        "        self.max_episode_length = max_episode_length\n",
        "        self.episode_length_buf = torch.zeros(num_envs, dtype=torch.long, device=self.device)\n",
        "\n",
        "        # underlying envs\n",
        "        self.envs = [gym.make(\"CartPole-v1\") for _ in range(num_envs)]\n",
        "        base_env = self.envs[0]\n",
        "\n",
        "        # expose gym-like spaces so rsl_rl infers dims cleanly\n",
        "        self.observation_space = base_env.observation_space  # Box(4,)\n",
        "        self.action_space = base_env.action_space            # Discrete(2)\n",
        "\n",
        "        self.num_actions = int(self.action_space.n)\n",
        "\n",
        "        obs0, _ = base_env.reset()\n",
        "        obs_dim = obs0.shape[0]\n",
        "        self.obs_buf = torch.zeros(num_envs, obs_dim, device=self.device, dtype=torch.float32)\n",
        "\n",
        "        self.cfg = {\"env_name\": \"CartPole-v1\"}\n",
        "\n",
        "        class _DummyUnwrapped:\n",
        "            def __init__(self, step_dt: float):\n",
        "                self.step_dt = step_dt\n",
        "\n",
        "        # CartPole dt\n",
        "        self.unwrapped = _DummyUnwrapped(step_dt=0.02)\n",
        "\n",
        "        self._reset_all()\n",
        "\n",
        "    def _reset_all(self):\n",
        "        for i, env in enumerate(self.envs):\n",
        "            o, _ = env.reset()\n",
        "            self.obs_buf[i] = torch.as_tensor(o, device=self.device, dtype=torch.float32)\n",
        "        self.episode_length_buf.zero_()\n",
        "\n",
        "    def get_observations(self) -> TensorDict:\n",
        "        return TensorDict(\n",
        "            {\n",
        "                \"policy\": self.obs_buf.clone(),\n",
        "                \"privileged\": self.obs_buf.clone(),\n",
        "            },\n",
        "            batch_size=[self.num_envs],\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "    def step(self, actions: torch.Tensor):\n",
        "        # Handle different action shapes from the policy:\n",
        "        # (num_envs, num_actions) → choose greedy action\n",
        "        if actions.dim() == 2 and actions.shape[1] > 1:\n",
        "            actions = torch.argmax(actions, dim=-1)\n",
        "\n",
        "        # (num_envs, 1) → squeeze to (num_envs,)\n",
        "        if actions.dim() == 2:\n",
        "            actions = actions.squeeze(-1)\n",
        "\n",
        "        rewards = torch.zeros(self.num_envs, device=self.device)\n",
        "        dones = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)\n",
        "        time_outs = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)\n",
        "\n",
        "        for i, env in enumerate(self.envs):\n",
        "            a = int(actions[i].item())\n",
        "            o, r, terminated, truncated, info = env.step(a)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            self.obs_buf[i] = torch.as_tensor(o, device=self.device, dtype=torch.float32)\n",
        "            rewards[i] = r\n",
        "            dones[i] = done\n",
        "\n",
        "            self.episode_length_buf[i] += 1\n",
        "            if self.episode_length_buf[i] >= self.max_episode_length:\n",
        "                time_outs[i] = True\n",
        "                done = True\n",
        "\n",
        "            if done:\n",
        "                o, _ = env.reset()\n",
        "                self.obs_buf[i] = torch.as_tensor(o, device=self.device, dtype=torch.float32)\n",
        "                self.episode_length_buf[i] = 0\n",
        "\n",
        "        obs_td = self.get_observations()\n",
        "        extras = {\n",
        "            \"time_outs\": time_outs,\n",
        "            \"log\": {},\n",
        "        }\n",
        "        return obs_td, rewards, dones, extras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfI4KuW1SAvn",
        "outputId": "208eaf88-9f1b-4813-d242-3db89e5c4559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<module 'rsl_rl.modules.symmetry' from '/content/rsl_rl/rsl_rl/modules/symmetry.py'>\n"
          ]
        }
      ],
      "source": [
        "from rsl_rl.modules import symmetry\n",
        "print(symmetry)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/rsl_rl/rsl_rl/modules/cartpole_symmetry_utils.py\n",
        "# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION\n",
        "# All rights reserved.\n",
        "#\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from rsl_rl.env import VecEnv\n",
        "\n",
        "\n",
        "import torch\n",
        "from tensordict import TensorDict\n",
        "from typing import TYPE_CHECKING\n",
        "\n",
        "# cartpole_symmetry.py\n",
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def cartpole_symmetry(obs=None, actions=None, env=None, obs_type: str = \"policy\"):\n",
        "    \"\"\"Augments the given observations and actions by applying symmetry transformations.\n",
        "\n",
        "    This function creates augmented versions of the provided observations and actions by applying\n",
        "    two symmetrical transformations: original, left-right. The symmetry\n",
        "    transformations are beneficial for reinforcement learning tasks by providing additional\n",
        "    diverse data without requiring additional data collection.\n",
        "\n",
        "    Args:\n",
        "        env: The environment instance.\n",
        "        obs: The original observation tensor dictionary. Defaults to None.\n",
        "        actions: The original actions tensor. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        Augmented observations and actions tensors, or None if the respective input was None.\n",
        "    \"\"\"\n",
        "\n",
        "    # observations\n",
        "    if obs is not None:\n",
        "        batch_size = obs.batch_size[0]\n",
        "        # since we have 2 different symmetries, we need to augment the batch size by 2\n",
        "        obs_aug = obs.repeat(2)\n",
        "        # -- original\n",
        "        obs_aug[\"policy\"][:batch_size] = obs[\"policy\"][:]\n",
        "        # -- left-right\n",
        "        obs_aug[\"policy\"][batch_size : 2 * batch_size] = -obs[\"policy\"]\n",
        "    else:\n",
        "        obs_aug = None\n",
        "\n",
        "    # actions\n",
        "    if actions is not None:\n",
        "        batch_size = actions.shape[0]\n",
        "        # since we have 4 different symmetries, we need to augment the batch size by 4\n",
        "        actions_aug = torch.zeros(batch_size * 2, actions.shape[1], device=actions.device)\n",
        "        # -- original\n",
        "        actions_aug[:batch_size] = actions[:]\n",
        "        # -- left-right\n",
        "        actions_aug[batch_size : 2 * batch_size] = 1 - actions\n",
        "\n",
        "\n",
        "    return obs_aug, actions_aug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D-cyQEa3AmW",
        "outputId": "3439f688-16d6-4e5c-8826-cd773f1af4ed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/rsl_rl/rsl_rl/modules/cartpole_symmetry_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Flipping code\n"
      ],
      "metadata": {
        "id": "cNZ2S4AluCzJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "j21L1mnQMn2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6629a636-6ba4-45b9-ac22-780685a96298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/rsl_rl/rsl_rl/modules/cartpole_symmetry_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/rsl_rl/rsl_rl/modules/cartpole_symmetry_utils.py\n",
        "# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION\n",
        "# All rights reserved.\n",
        "#\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from rsl_rl.env import VecEnv\n",
        "\n",
        "\n",
        "# cartpole_symmetry.py\n",
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def cartpole_symmetry(obs=None, actions=None, env=None, obs_type: str = \"policy\"):\n",
        "    # obs: TensorDict or None\n",
        "    # actions: tensor or None\n",
        "    # env: your CartPoleVecEnv\n",
        "\n",
        "    obs_sym = obs\n",
        "    act_sym = actions\n",
        "\n",
        "    if obs is not None:\n",
        "        s = obs[\"policy\"]              # (B, 4)\n",
        "        s_sym = -s                     # simple mirror\n",
        "        obs_sym = obs.clone()\n",
        "        obs_sym[\"policy\"] = s_sym\n",
        "        if \"privileged\" in obs_sym.keys():\n",
        "            obs_sym[\"privileged\"] = s_sym\n",
        "\n",
        "    if actions is not None:\n",
        "        act_sym = 1 - actions          # assumes discrete {0,1}\n",
        "\n",
        "    return obs_sym, act_sym\n",
        "\n",
        "\n",
        "def resolve_symmetry_config(alg_cfg: dict, env: VecEnv) -> dict:\n",
        "    \"\"\"Resolve the symmetry configuration.\n",
        "\n",
        "    Args:\n",
        "        alg_cfg: Algorithm configuration dictionary.\n",
        "        env: Environment object.\n",
        "\n",
        "    Returns:\n",
        "        The resolved algorithm configuration dictionary.\n",
        "    \"\"\"\n",
        "    # If using symmetry then pass the environment config object\n",
        "    # Note: This is used by the symmetry function for handling different observation terms\n",
        "    if \"symmetry_cfg\" in alg_cfg and alg_cfg[\"symmetry_cfg\"] is not None:\n",
        "        alg_cfg[\"symmetry_cfg\"][\"_env\"] = env\n",
        "    else:\n",
        "        alg_cfg[\"symmetry_cfg\"] = None\n",
        "    return alg_cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VYlFfKHr6xvV",
        "outputId": "001fde31-2250-49aa-8d00-6102631d021b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OnPolicyRunner\n",
            "--------------------------------------------------------------------------------\n",
            "Resolved observation sets: \n",
            "\t policy :  ['policy']\n",
            "\t critic :  ['policy', 'privileged']\n",
            "\t rnd_state :  ['policy']\n",
            "--------------------------------------------------------------------------------\n",
            "Actor MLP: MLP(\n",
            "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n",
            "Critic MLP: MLP(\n",
            "  (0): Linear(in_features=8, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/rsl_rl/rsl_rl/utils/utils.py:273: UserWarning: The observation configuration dictionary 'obs_groups' must contain the 'rnd_state' key. As the configuration for 'rnd_state' is missing, the observations from the 'policy' set are used. Consider adding the 'rnd_state' key to the 'obs_groups' dictionary for clarity. This behavior will be removed in a future version.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 0/1500                            \u001b[0m \n",
            "\n",
            "                            Total steps: 192 \n",
            "                       Steps per second: 253 \n",
            "                        Collection time: 0.429s \n",
            "                          Learning time: 0.327s \n",
            "                        Mean value loss: 59.3835\n",
            "                    Mean surrogate loss: 0.1684\n",
            "                      Mean entropy loss: 2.8420\n",
            "                          Mean rnd loss: 1.4766\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 18.67\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 18.67\n",
            "                    Mean episode length: 18.67\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.76s\n",
            "                           Time elapsed: 00:00:00\n",
            "                                    ETA: 00:18:53\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 1/1500                            \u001b[0m \n",
            "\n",
            "                            Total steps: 384 \n",
            "                       Steps per second: 478 \n",
            "                        Collection time: 0.138s \n",
            "                          Learning time: 0.263s \n",
            "                        Mean value loss: 50.2209\n",
            "                    Mean surrogate loss: 0.1321\n",
            "                      Mean entropy loss: 2.8426\n",
            "                          Mean rnd loss: 1.5465\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 19.08\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 19.08\n",
            "                    Mean episode length: 19.08\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.40s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:14:26\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 2/1500                            \u001b[0m \n",
            "\n",
            "                            Total steps: 576 \n",
            "                       Steps per second: 806 \n",
            "                        Collection time: 0.086s \n",
            "                          Learning time: 0.152s \n",
            "                        Mean value loss: 45.1644\n",
            "                    Mean surrogate loss: 0.2330\n",
            "                      Mean entropy loss: 2.8426\n",
            "                          Mean rnd loss: 1.3545\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 20.71\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 20.71\n",
            "                    Mean episode length: 20.71\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:11:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 3/1500                            \u001b[0m \n",
            "\n",
            "                            Total steps: 768 \n",
            "                       Steps per second: 869 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.148s \n",
            "                        Mean value loss: 49.7380\n",
            "                    Mean surrogate loss: 0.1588\n",
            "                      Mean entropy loss: 2.8426\n",
            "                          Mean rnd loss: 1.2566\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 20.66\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 20.66\n",
            "                    Mean episode length: 20.66\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:10:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 4/1500                            \u001b[0m \n",
            "\n",
            "                            Total steps: 960 \n",
            "                       Steps per second: 845 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.153s \n",
            "                        Mean value loss: 38.6033\n",
            "                    Mean surrogate loss: 0.1100\n",
            "                      Mean entropy loss: 2.8427\n",
            "                          Mean rnd loss: 1.1026\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 21.76\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 21.76\n",
            "                    Mean episode length: 21.76\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:09:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 5/1500                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1152 \n",
            "                       Steps per second: 817 \n",
            "                        Collection time: 0.085s \n",
            "                          Learning time: 0.150s \n",
            "                        Mean value loss: 52.7304\n",
            "                    Mean surrogate loss: 0.2343\n",
            "                      Mean entropy loss: 2.8427\n",
            "                          Mean rnd loss: 0.8925\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 23.25\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 23.25\n",
            "                    Mean episode length: 23.25\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:08:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 6/1500                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1344 \n",
            "                       Steps per second: 857 \n",
            "                        Collection time: 0.071s \n",
            "                          Learning time: 0.153s \n",
            "                        Mean value loss: 40.6476\n",
            "                    Mean surrogate loss: 0.2209\n",
            "                      Mean entropy loss: 2.8427\n",
            "                          Mean rnd loss: 0.7597\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 23.25\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 23.25\n",
            "                    Mean episode length: 23.25\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:08:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 7/1500                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1536 \n",
            "                       Steps per second: 876 \n",
            "                        Collection time: 0.071s \n",
            "                          Learning time: 0.148s \n",
            "                        Mean value loss: 43.1735\n",
            "                    Mean surrogate loss: 0.1153\n",
            "                      Mean entropy loss: 2.8427\n",
            "                          Mean rnd loss: 0.6534\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 23.56\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 23.56\n",
            "                    Mean episode length: 23.56\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:07:50\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 8/1500                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1728 \n",
            "                       Steps per second: 857 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.149s \n",
            "                        Mean value loss: 53.7407\n",
            "                    Mean surrogate loss: 0.1008\n",
            "                      Mean entropy loss: 2.8427\n",
            "                          Mean rnd loss: 0.5477\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 23.95\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 23.95\n",
            "                    Mean episode length: 23.95\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:07:34\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 9/1500                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1920 \n",
            "                       Steps per second: 810 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.164s \n",
            "                        Mean value loss: 50.6935\n",
            "                    Mean surrogate loss: 0.1599\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.4483\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 24.25\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 24.25\n",
            "                    Mean episode length: 24.25\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:07:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 10/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 2112 \n",
            "                       Steps per second: 855 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.151s \n",
            "                        Mean value loss: 53.1962\n",
            "                    Mean surrogate loss: 0.1008\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.3613\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 25.18\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 25.18\n",
            "                    Mean episode length: 25.18\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:07:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 11/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 2304 \n",
            "                       Steps per second: 870 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.148s \n",
            "                        Mean value loss: 44.9181\n",
            "                    Mean surrogate loss: 0.0989\n",
            "                      Mean entropy loss: 2.8427\n",
            "                          Mean rnd loss: 0.3001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 24.94\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 24.94\n",
            "                    Mean episode length: 24.94\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:07:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 12/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 2496 \n",
            "                       Steps per second: 854 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.152s \n",
            "                        Mean value loss: 30.4853\n",
            "                    Mean surrogate loss: 0.1216\n",
            "                      Mean entropy loss: 2.8427\n",
            "                          Mean rnd loss: 0.2169\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 25.37\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 25.37\n",
            "                    Mean episode length: 25.37\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:06:57\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 13/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 2688 \n",
            "                       Steps per second: 867 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.149s \n",
            "                        Mean value loss: 55.4754\n",
            "                    Mean surrogate loss: 0.0959\n",
            "                      Mean entropy loss: 2.8427\n",
            "                          Mean rnd loss: 0.1989\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 25.03\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 25.03\n",
            "                    Mean episode length: 25.03\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:06:51\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 14/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 2880 \n",
            "                       Steps per second: 745 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.177s \n",
            "                        Mean value loss: 48.1195\n",
            "                    Mean surrogate loss: 0.0748\n",
            "                      Mean entropy loss: 2.8427\n",
            "                          Mean rnd loss: 0.1593\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 25.79\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 25.79\n",
            "                    Mean episode length: 25.79\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:06:48\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 15/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 3072 \n",
            "                       Steps per second: 773 \n",
            "                        Collection time: 0.087s \n",
            "                          Learning time: 0.161s \n",
            "                        Mean value loss: 42.3968\n",
            "                    Mean surrogate loss: 0.0785\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.1185\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 26.88\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 26.88\n",
            "                    Mean episode length: 26.88\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:06:46\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 16/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 3264 \n",
            "                       Steps per second: 844 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.154s \n",
            "                        Mean value loss: 53.0447\n",
            "                    Mean surrogate loss: 0.0365\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.1216\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 26.65\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 26.65\n",
            "                    Mean episode length: 26.65\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:06:41\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 17/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 3456 \n",
            "                       Steps per second: 848 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.150s \n",
            "                        Mean value loss: 30.7472\n",
            "                    Mean surrogate loss: 0.0716\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0979\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 27.69\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 27.69\n",
            "                    Mean episode length: 27.69\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:06:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 18/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 3648 \n",
            "                       Steps per second: 790 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.171s \n",
            "                        Mean value loss: 49.3250\n",
            "                    Mean surrogate loss: 0.0260\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0927\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 27.30\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 27.30\n",
            "                    Mean episode length: 27.30\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:06:35\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 19/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 3840 \n",
            "                       Steps per second: 825 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.160s \n",
            "                        Mean value loss: 48.7690\n",
            "                    Mean surrogate loss: 0.0237\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0710\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 27.89\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 27.89\n",
            "                    Mean episode length: 27.89\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:06:32\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 20/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 4032 \n",
            "                       Steps per second: 841 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.151s \n",
            "                        Mean value loss: 48.3162\n",
            "                    Mean surrogate loss: 0.0175\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0767\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 27.83\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 27.83\n",
            "                    Mean episode length: 27.83\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:06:29\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 21/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 4224 \n",
            "                       Steps per second: 840 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 44.0605\n",
            "                    Mean surrogate loss: 0.0083\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0593\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 28.38\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 28.38\n",
            "                    Mean episode length: 28.38\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:06:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 22/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 4416 \n",
            "                       Steps per second: 853 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.152s \n",
            "                        Mean value loss: 39.4924\n",
            "                    Mean surrogate loss: 0.0554\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0821\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 28.38\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 28.38\n",
            "                    Mean episode length: 28.38\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:06:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 23/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 4608 \n",
            "                       Steps per second: 799 \n",
            "                        Collection time: 0.084s \n",
            "                          Learning time: 0.156s \n",
            "                        Mean value loss: 46.7698\n",
            "                    Mean surrogate loss: 0.0351\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0444\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 28.76\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 28.76\n",
            "                    Mean episode length: 28.76\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:06:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 24/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 4800 \n",
            "                       Steps per second: 874 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.147s \n",
            "                        Mean value loss: 46.7464\n",
            "                    Mean surrogate loss: 0.0248\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0379\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 28.87\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 28.87\n",
            "                    Mean episode length: 28.87\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:06:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 25/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 4992 \n",
            "                       Steps per second: 831 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.151s \n",
            "                        Mean value loss: 51.1057\n",
            "                    Mean surrogate loss: 0.0155\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0405\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 28.66\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 28.66\n",
            "                    Mean episode length: 28.66\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:06:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 26/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 5184 \n",
            "                       Steps per second: 819 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.160s \n",
            "                        Mean value loss: 43.7411\n",
            "                    Mean surrogate loss: 0.0041\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0381\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 29.78\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 29.78\n",
            "                    Mean episode length: 29.78\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:06:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 27/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 5376 \n",
            "                       Steps per second: 773 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.173s \n",
            "                        Mean value loss: 45.3655\n",
            "                    Mean surrogate loss: 0.0184\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0235\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 29.63\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 29.63\n",
            "                    Mean episode length: 29.63\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:06:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 28/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 5568 \n",
            "                       Steps per second: 851 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.151s \n",
            "                        Mean value loss: 50.3706\n",
            "                    Mean surrogate loss: 0.0256\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0181\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 30.85\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 30.85\n",
            "                    Mean episode length: 30.85\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:06:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 29/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 5760 \n",
            "                       Steps per second: 860 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.151s \n",
            "                        Mean value loss: 40.3552\n",
            "                    Mean surrogate loss: 0.0150\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0209\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 31.94\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 31.94\n",
            "                    Mean episode length: 31.94\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:06:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 30/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 5952 \n",
            "                       Steps per second: 848 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.152s \n",
            "                        Mean value loss: 44.6090\n",
            "                    Mean surrogate loss: 0.0337\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0137\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 32.11\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 32.11\n",
            "                    Mean episode length: 32.11\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:06:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 31/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 6144 \n",
            "                       Steps per second: 815 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.162s \n",
            "                        Mean value loss: 44.4479\n",
            "                    Mean surrogate loss: 0.0102\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0123\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 31.94\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 31.94\n",
            "                    Mean episode length: 31.94\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:06:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 32/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 6336 \n",
            "                       Steps per second: 782 \n",
            "                        Collection time: 0.091s \n",
            "                          Learning time: 0.154s \n",
            "                        Mean value loss: 29.8762\n",
            "                    Mean surrogate loss: -0.0036\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0064\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 32.49\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 32.49\n",
            "                    Mean episode length: 32.49\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:06:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 33/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 6528 \n",
            "                       Steps per second: 801 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.167s \n",
            "                        Mean value loss: 52.1561\n",
            "                    Mean surrogate loss: -0.0035\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0047\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 32.50\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 32.50\n",
            "                    Mean episode length: 32.50\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:06:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 34/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 6720 \n",
            "                       Steps per second: 851 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.153s \n",
            "                        Mean value loss: 36.7838\n",
            "                    Mean surrogate loss: -0.0079\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0038\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 33.03\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.03\n",
            "                    Mean episode length: 33.03\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:06:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 35/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 6912 \n",
            "                       Steps per second: 823 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.161s \n",
            "                        Mean value loss: 30.2019\n",
            "                    Mean surrogate loss: -0.0068\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0029\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 33.05\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.05\n",
            "                    Mean episode length: 33.05\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:06:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 36/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 7104 \n",
            "                       Steps per second: 786 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.167s \n",
            "                        Mean value loss: 56.0629\n",
            "                    Mean surrogate loss: -0.0013\n",
            "                      Mean entropy loss: 2.8427\n",
            "                          Mean rnd loss: 0.0014\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 33.21\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.21\n",
            "                    Mean episode length: 33.21\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:06:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 37/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 7296 \n",
            "                       Steps per second: 846 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.154s \n",
            "                        Mean value loss: 35.6805\n",
            "                    Mean surrogate loss: 0.0006\n",
            "                      Mean entropy loss: 2.8427\n",
            "                          Mean rnd loss: 0.0013\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 34.03\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 34.03\n",
            "                    Mean episode length: 34.03\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:06:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 38/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 7488 \n",
            "                       Steps per second: 845 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.153s \n",
            "                        Mean value loss: 49.1406\n",
            "                    Mean surrogate loss: 0.0026\n",
            "                      Mean entropy loss: 2.8427\n",
            "                          Mean rnd loss: 0.0020\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 33.42\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.42\n",
            "                    Mean episode length: 33.42\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:06:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 39/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 7680 \n",
            "                       Steps per second: 828 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.160s \n",
            "                        Mean value loss: 41.1307\n",
            "                    Mean surrogate loss: -0.0014\n",
            "                      Mean entropy loss: 2.8427\n",
            "                          Mean rnd loss: 0.0013\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 33.68\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.68\n",
            "                    Mean episode length: 33.68\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:06:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 40/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 7872 \n",
            "                       Steps per second: 814 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.163s \n",
            "                        Mean value loss: 40.3456\n",
            "                    Mean surrogate loss: 0.0011\n",
            "                      Mean entropy loss: 2.8427\n",
            "                          Mean rnd loss: 0.0020\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 34.33\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 34.33\n",
            "                    Mean episode length: 34.33\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:06:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 41/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 8064 \n",
            "                       Steps per second: 873 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.148s \n",
            "                        Mean value loss: 42.1256\n",
            "                    Mean surrogate loss: 0.0073\n",
            "                      Mean entropy loss: 2.8427\n",
            "                          Mean rnd loss: 0.0008\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 34.49\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 34.49\n",
            "                    Mean episode length: 34.49\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:06:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 42/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 8256 \n",
            "                       Steps per second: 853 \n",
            "                        Collection time: 0.071s \n",
            "                          Learning time: 0.154s \n",
            "                        Mean value loss: 44.4887\n",
            "                    Mean surrogate loss: -0.0090\n",
            "                      Mean entropy loss: 2.8427\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 34.27\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 34.27\n",
            "                    Mean episode length: 34.27\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:06:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 43/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 8448 \n",
            "                       Steps per second: 861 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.149s \n",
            "                        Mean value loss: 34.5208\n",
            "                    Mean surrogate loss: -0.0172\n",
            "                      Mean entropy loss: 2.8426\n",
            "                          Mean rnd loss: 0.0007\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 34.42\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 34.42\n",
            "                    Mean episode length: 34.42\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:05:59\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 44/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 8640 \n",
            "                       Steps per second: 831 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 49.6341\n",
            "                    Mean surrogate loss: -0.0025\n",
            "                      Mean entropy loss: 2.8426\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 33.71\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.71\n",
            "                    Mean episode length: 33.71\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:05:58\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 45/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 8832 \n",
            "                       Steps per second: 537 \n",
            "                        Collection time: 0.131s \n",
            "                          Learning time: 0.227s \n",
            "                        Mean value loss: 37.7248\n",
            "                    Mean surrogate loss: 0.0007\n",
            "                      Mean entropy loss: 2.8426\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 34.58\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 34.58\n",
            "                    Mean episode length: 34.58\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.36s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:06:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 46/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 9024 \n",
            "                       Steps per second: 624 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.213s \n",
            "                        Mean value loss: 40.7118\n",
            "                    Mean surrogate loss: -0.0077\n",
            "                      Mean entropy loss: 2.8426\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 33.94\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.94\n",
            "                    Mean episode length: 33.94\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:06:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 47/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 9216 \n",
            "                       Steps per second: 628 \n",
            "                        Collection time: 0.091s \n",
            "                          Learning time: 0.215s \n",
            "                        Mean value loss: 29.8308\n",
            "                    Mean surrogate loss: -0.0128\n",
            "                      Mean entropy loss: 2.8426\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 33.56\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.56\n",
            "                    Mean episode length: 33.56\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:06:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 48/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 9408 \n",
            "                       Steps per second: 590 \n",
            "                        Collection time: 0.093s \n",
            "                          Learning time: 0.232s \n",
            "                        Mean value loss: 42.2622\n",
            "                    Mean surrogate loss: -0.0052\n",
            "                      Mean entropy loss: 2.8426\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 33.44\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.44\n",
            "                    Mean episode length: 33.44\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.33s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:06:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 49/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 9600 \n",
            "                       Steps per second: 656 \n",
            "                        Collection time: 0.086s \n",
            "                          Learning time: 0.207s \n",
            "                        Mean value loss: 44.1461\n",
            "                    Mean surrogate loss: -0.0005\n",
            "                      Mean entropy loss: 2.8426\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 33.09\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.09\n",
            "                    Mean episode length: 33.09\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:06:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 50/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 9792 \n",
            "                       Steps per second: 592 \n",
            "                        Collection time: 0.107s \n",
            "                          Learning time: 0.218s \n",
            "                        Mean value loss: 54.0411\n",
            "                    Mean surrogate loss: -0.0047\n",
            "                      Mean entropy loss: 2.8426\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 33.33\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.33\n",
            "                    Mean episode length: 33.33\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.32s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:06:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 51/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 9984 \n",
            "                       Steps per second: 530 \n",
            "                        Collection time: 0.110s \n",
            "                          Learning time: 0.252s \n",
            "                        Mean value loss: 48.1594\n",
            "                    Mean surrogate loss: -0.0057\n",
            "                      Mean entropy loss: 2.8425\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 33.68\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.68\n",
            "                    Mean episode length: 33.68\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.36s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:06:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 52/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 10176 \n",
            "                       Steps per second: 711 \n",
            "                        Collection time: 0.112s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 43.9236\n",
            "                    Mean surrogate loss: 0.0047\n",
            "                      Mean entropy loss: 2.8425\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 33.46\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.46\n",
            "                    Mean episode length: 33.46\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:06:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 53/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 10368 \n",
            "                       Steps per second: 861 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.151s \n",
            "                        Mean value loss: 38.8401\n",
            "                    Mean surrogate loss: -0.0150\n",
            "                      Mean entropy loss: 2.8425\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 34.67\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 34.67\n",
            "                    Mean episode length: 34.67\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:06:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 54/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 10560 \n",
            "                       Steps per second: 759 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.180s \n",
            "                        Mean value loss: 43.9445\n",
            "                    Mean surrogate loss: -0.0103\n",
            "                      Mean entropy loss: 2.8425\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 33.78\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.78\n",
            "                    Mean episode length: 33.78\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:06:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 55/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 10752 \n",
            "                       Steps per second: 844 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 41.8440\n",
            "                    Mean surrogate loss: -0.0052\n",
            "                      Mean entropy loss: 2.8424\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 34.17\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 34.17\n",
            "                    Mean episode length: 34.17\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:06:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 56/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 10944 \n",
            "                       Steps per second: 809 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.156s \n",
            "                        Mean value loss: 51.6766\n",
            "                    Mean surrogate loss: -0.0013\n",
            "                      Mean entropy loss: 2.8424\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 33.77\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.77\n",
            "                    Mean episode length: 33.77\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:06:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 57/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 11136 \n",
            "                       Steps per second: 861 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.147s \n",
            "                        Mean value loss: 46.9508\n",
            "                    Mean surrogate loss: 0.0027\n",
            "                      Mean entropy loss: 2.8424\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 35.06\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 35.06\n",
            "                    Mean episode length: 35.06\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:06:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 58/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 11328 \n",
            "                       Steps per second: 825 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.159s \n",
            "                        Mean value loss: 43.1526\n",
            "                    Mean surrogate loss: -0.0055\n",
            "                      Mean entropy loss: 2.8423\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 34.91\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 34.91\n",
            "                    Mean episode length: 34.91\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:06:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 59/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 11520 \n",
            "                       Steps per second: 835 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.156s \n",
            "                        Mean value loss: 53.3553\n",
            "                    Mean surrogate loss: -0.0016\n",
            "                      Mean entropy loss: 2.8423\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 34.95\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 34.95\n",
            "                    Mean episode length: 34.95\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:06:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 60/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 11712 \n",
            "                       Steps per second: 795 \n",
            "                        Collection time: 0.071s \n",
            "                          Learning time: 0.170s \n",
            "                        Mean value loss: 54.0825\n",
            "                    Mean surrogate loss: -0.0007\n",
            "                      Mean entropy loss: 2.8423\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 35.58\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 35.58\n",
            "                    Mean episode length: 35.58\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:06:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 61/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 11904 \n",
            "                       Steps per second: 811 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.161s \n",
            "                        Mean value loss: 53.5043\n",
            "                    Mean surrogate loss: 0.0019\n",
            "                      Mean entropy loss: 2.8423\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 36.38\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 36.38\n",
            "                    Mean episode length: 36.38\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:06:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 62/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 12096 \n",
            "                       Steps per second: 799 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.166s \n",
            "                        Mean value loss: 47.7677\n",
            "                    Mean surrogate loss: 0.0009\n",
            "                      Mean entropy loss: 2.8423\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 36.78\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 36.78\n",
            "                    Mean episode length: 36.78\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:06:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 63/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 12288 \n",
            "                       Steps per second: 849 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.154s \n",
            "                        Mean value loss: 49.9973\n",
            "                    Mean surrogate loss: 0.0017\n",
            "                      Mean entropy loss: 2.8423\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 37.92\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 37.92\n",
            "                    Mean episode length: 37.92\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:06:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 64/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 12480 \n",
            "                       Steps per second: 764 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.178s \n",
            "                        Mean value loss: 50.1048\n",
            "                    Mean surrogate loss: -0.0062\n",
            "                      Mean entropy loss: 2.8423\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 37.45\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 37.45\n",
            "                    Mean episode length: 37.45\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:06:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 65/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 12672 \n",
            "                       Steps per second: 839 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 49.4890\n",
            "                    Mean surrogate loss: 0.0006\n",
            "                      Mean entropy loss: 2.8423\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 37.30\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 37.30\n",
            "                    Mean episode length: 37.30\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:06:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 66/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 12864 \n",
            "                       Steps per second: 813 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.164s \n",
            "                        Mean value loss: 52.0081\n",
            "                    Mean surrogate loss: -0.0037\n",
            "                      Mean entropy loss: 2.8423\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 37.46\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 37.46\n",
            "                    Mean episode length: 37.46\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:06:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 67/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 13056 \n",
            "                       Steps per second: 855 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.151s \n",
            "                        Mean value loss: 54.5047\n",
            "                    Mean surrogate loss: -0.0007\n",
            "                      Mean entropy loss: 2.8423\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 38.05\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 38.05\n",
            "                    Mean episode length: 38.05\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:06:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 68/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 13248 \n",
            "                       Steps per second: 865 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.149s \n",
            "                        Mean value loss: 54.0521\n",
            "                    Mean surrogate loss: -0.0118\n",
            "                      Mean entropy loss: 2.8423\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 38.24\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 38.24\n",
            "                    Mean episode length: 38.24\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:05:59\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 69/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 13440 \n",
            "                       Steps per second: 785 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.150s \n",
            "                        Mean value loss: 54.8022\n",
            "                    Mean surrogate loss: 0.0078\n",
            "                      Mean entropy loss: 2.8423\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 38.02\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 38.02\n",
            "                    Mean episode length: 38.02\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:05:59\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 70/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 13632 \n",
            "                       Steps per second: 859 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.151s \n",
            "                        Mean value loss: 47.9020\n",
            "                    Mean surrogate loss: 0.0161\n",
            "                      Mean entropy loss: 2.8423\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 41.34\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 41.34\n",
            "                    Mean episode length: 41.34\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:05:58\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 71/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 13824 \n",
            "                       Steps per second: 845 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.154s \n",
            "                        Mean value loss: 49.8506\n",
            "                    Mean surrogate loss: -0.0038\n",
            "                      Mean entropy loss: 2.8423\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 41.27\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 41.27\n",
            "                    Mean episode length: 41.27\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:05:58\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 72/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 14016 \n",
            "                       Steps per second: 857 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.150s \n",
            "                        Mean value loss: 55.9419\n",
            "                    Mean surrogate loss: -0.0082\n",
            "                      Mean entropy loss: 2.8422\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 40.62\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 40.62\n",
            "                    Mean episode length: 40.62\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:05:57\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 73/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 14208 \n",
            "                       Steps per second: 780 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.174s \n",
            "                        Mean value loss: 50.0164\n",
            "                    Mean surrogate loss: -0.0052\n",
            "                      Mean entropy loss: 2.8422\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 42.92\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 42.92\n",
            "                    Mean episode length: 42.92\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:05:56\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 74/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 14400 \n",
            "                       Steps per second: 835 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.153s \n",
            "                        Mean value loss: 54.4826\n",
            "                    Mean surrogate loss: -0.0005\n",
            "                      Mean entropy loss: 2.8422\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 42.71\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 42.71\n",
            "                    Mean episode length: 42.71\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:05:56\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 75/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 14592 \n",
            "                       Steps per second: 852 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.152s \n",
            "                        Mean value loss: 50.1330\n",
            "                    Mean surrogate loss: 0.0017\n",
            "                      Mean entropy loss: 2.8422\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 43.44\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 43.44\n",
            "                    Mean episode length: 43.44\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:05:55\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 76/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 14784 \n",
            "                       Steps per second: 815 \n",
            "                        Collection time: 0.087s \n",
            "                          Learning time: 0.149s \n",
            "                        Mean value loss: 57.6719\n",
            "                    Mean surrogate loss: -0.0026\n",
            "                      Mean entropy loss: 2.8422\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 43.17\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 43.17\n",
            "                    Mean episode length: 43.17\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:05:55\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 77/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 14976 \n",
            "                       Steps per second: 843 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.154s \n",
            "                        Mean value loss: 53.1787\n",
            "                    Mean surrogate loss: -0.0080\n",
            "                      Mean entropy loss: 2.8421\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 44.95\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 44.95\n",
            "                    Mean episode length: 44.95\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:05:54\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 78/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 15168 \n",
            "                       Steps per second: 785 \n",
            "                        Collection time: 0.087s \n",
            "                          Learning time: 0.158s \n",
            "                        Mean value loss: 54.2039\n",
            "                    Mean surrogate loss: -0.0025\n",
            "                      Mean entropy loss: 2.8421\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 45.43\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 45.43\n",
            "                    Mean episode length: 45.43\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:05:54\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 79/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 15360 \n",
            "                       Steps per second: 839 \n",
            "                        Collection time: 0.071s \n",
            "                          Learning time: 0.158s \n",
            "                        Mean value loss: 59.3124\n",
            "                    Mean surrogate loss: 0.0017\n",
            "                      Mean entropy loss: 2.8421\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 45.28\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 45.28\n",
            "                    Mean episode length: 45.28\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:05:53\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 80/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 15552 \n",
            "                       Steps per second: 846 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 57.4780\n",
            "                    Mean surrogate loss: -0.0066\n",
            "                      Mean entropy loss: 2.8421\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 45.86\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 45.86\n",
            "                    Mean episode length: 45.86\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:05:52\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 81/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 15744 \n",
            "                       Steps per second: 835 \n",
            "                        Collection time: 0.071s \n",
            "                          Learning time: 0.159s \n",
            "                        Mean value loss: 58.8304\n",
            "                    Mean surrogate loss: 0.0147\n",
            "                      Mean entropy loss: 2.8421\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 44.85\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 44.85\n",
            "                    Mean episode length: 44.85\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:05:52\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 82/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 15936 \n",
            "                       Steps per second: 767 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.177s \n",
            "                        Mean value loss: 55.2813\n",
            "                    Mean surrogate loss: 0.0111\n",
            "                      Mean entropy loss: 2.8421\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 45.24\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 45.24\n",
            "                    Mean episode length: 45.24\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:05:52\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 83/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 16128 \n",
            "                       Steps per second: 835 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 55.2616\n",
            "                    Mean surrogate loss: 0.0076\n",
            "                      Mean entropy loss: 2.8422\n",
            "                          Mean rnd loss: 0.0007\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 45.89\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 45.89\n",
            "                    Mean episode length: 45.89\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:05:51\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 84/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 16320 \n",
            "                       Steps per second: 854 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.152s \n",
            "                        Mean value loss: 59.9392\n",
            "                    Mean surrogate loss: -0.0024\n",
            "                      Mean entropy loss: 2.8422\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 48.35\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 48.35\n",
            "                    Mean episode length: 48.35\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:05:50\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 85/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 16512 \n",
            "                       Steps per second: 736 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.182s \n",
            "                        Mean value loss: 64.1145\n",
            "                    Mean surrogate loss: -0.0007\n",
            "                      Mean entropy loss: 2.8422\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 47.42\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 47.42\n",
            "                    Mean episode length: 47.42\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:05:50\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 86/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 16704 \n",
            "                       Steps per second: 778 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.173s \n",
            "                        Mean value loss: 58.9425\n",
            "                    Mean surrogate loss: -0.0078\n",
            "                      Mean entropy loss: 2.8422\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 46.32\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 46.32\n",
            "                    Mean episode length: 46.32\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:05:50\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 87/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 16896 \n",
            "                       Steps per second: 851 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.152s \n",
            "                        Mean value loss: 57.1319\n",
            "                    Mean surrogate loss: -0.0012\n",
            "                      Mean entropy loss: 2.8422\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 46.08\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 46.08\n",
            "                    Mean episode length: 46.08\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:05:49\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 88/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 17088 \n",
            "                       Steps per second: 835 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 59.7818\n",
            "                    Mean surrogate loss: 0.0247\n",
            "                      Mean entropy loss: 2.8422\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 46.37\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 46.37\n",
            "                    Mean episode length: 46.37\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:05:49\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 89/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 17280 \n",
            "                       Steps per second: 856 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.152s \n",
            "                        Mean value loss: 55.8239\n",
            "                    Mean surrogate loss: -0.0009\n",
            "                      Mean entropy loss: 2.8422\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 47.24\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 47.24\n",
            "                    Mean episode length: 47.24\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:05:48\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 90/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 17472 \n",
            "                       Steps per second: 864 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.149s \n",
            "                        Mean value loss: 61.3806\n",
            "                    Mean surrogate loss: 0.0043\n",
            "                      Mean entropy loss: 2.8422\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 47.39\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 47.39\n",
            "                    Mean episode length: 47.39\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:05:48\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 91/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 17664 \n",
            "                       Steps per second: 807 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.163s \n",
            "                        Mean value loss: 61.7176\n",
            "                    Mean surrogate loss: -0.0045\n",
            "                      Mean entropy loss: 2.8422\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 49.68\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 49.68\n",
            "                    Mean episode length: 49.68\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:05:47\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 92/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 17856 \n",
            "                       Steps per second: 823 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 66.5956\n",
            "                    Mean surrogate loss: -0.0026\n",
            "                      Mean entropy loss: 2.8421\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 50.48\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 50.48\n",
            "                    Mean episode length: 50.48\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:05:47\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 93/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 18048 \n",
            "                       Steps per second: 837 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.158s \n",
            "                        Mean value loss: 70.3960\n",
            "                    Mean surrogate loss: -0.0052\n",
            "                      Mean entropy loss: 2.8421\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 51.74\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 51.74\n",
            "                    Mean episode length: 51.74\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:05:46\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 94/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 18240 \n",
            "                       Steps per second: 821 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.160s \n",
            "                        Mean value loss: 69.3599\n",
            "                    Mean surrogate loss: 0.0058\n",
            "                      Mean entropy loss: 2.8420\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 52.26\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 52.26\n",
            "                    Mean episode length: 52.26\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:05:46\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 95/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 18432 \n",
            "                       Steps per second: 549 \n",
            "                        Collection time: 0.119s \n",
            "                          Learning time: 0.231s \n",
            "                        Mean value loss: 57.1763\n",
            "                    Mean surrogate loss: 0.0056\n",
            "                      Mean entropy loss: 2.8420\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 52.39\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 52.39\n",
            "                    Mean episode length: 52.39\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.35s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:05:47\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 96/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 18624 \n",
            "                       Steps per second: 629 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.205s \n",
            "                        Mean value loss: 76.9329\n",
            "                    Mean surrogate loss: 0.0054\n",
            "                      Mean entropy loss: 2.8419\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 50.15\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 50.15\n",
            "                    Mean episode length: 50.15\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:05:48\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 97/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 18816 \n",
            "                       Steps per second: 625 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.209s \n",
            "                        Mean value loss: 57.0625\n",
            "                    Mean surrogate loss: -0.0045\n",
            "                      Mean entropy loss: 2.8419\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 50.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 50.07\n",
            "                    Mean episode length: 50.07\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:05:48\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 98/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 19008 \n",
            "                       Steps per second: 632 \n",
            "                        Collection time: 0.093s \n",
            "                          Learning time: 0.211s \n",
            "                        Mean value loss: 59.5850\n",
            "                    Mean surrogate loss: -0.0116\n",
            "                      Mean entropy loss: 2.8419\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 51.16\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 51.16\n",
            "                    Mean episode length: 51.16\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:05:49\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 99/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 19200 \n",
            "                       Steps per second: 625 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.213s \n",
            "                        Mean value loss: 67.6315\n",
            "                    Mean surrogate loss: 0.0015\n",
            "                      Mean entropy loss: 2.8419\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 51.97\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 51.97\n",
            "                    Mean episode length: 51.97\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:05:49\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 100/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 19392 \n",
            "                       Steps per second: 578 \n",
            "                        Collection time: 0.110s \n",
            "                          Learning time: 0.222s \n",
            "                        Mean value loss: 69.8045\n",
            "                    Mean surrogate loss: -0.0110\n",
            "                      Mean entropy loss: 2.8419\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 53.19\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 53.19\n",
            "                    Mean episode length: 53.19\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.33s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:05:50\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 101/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 19584 \n",
            "                       Steps per second: 540 \n",
            "                        Collection time: 0.116s \n",
            "                          Learning time: 0.239s \n",
            "                        Mean value loss: 76.1391\n",
            "                    Mean surrogate loss: -0.0079\n",
            "                      Mean entropy loss: 2.8419\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 51.91\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 51.91\n",
            "                    Mean episode length: 51.91\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.36s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:05:51\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 102/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 19776 \n",
            "                       Steps per second: 628 \n",
            "                        Collection time: 0.119s \n",
            "                          Learning time: 0.186s \n",
            "                        Mean value loss: 59.5021\n",
            "                    Mean surrogate loss: -0.0055\n",
            "                      Mean entropy loss: 2.8419\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 51.81\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 51.81\n",
            "                    Mean episode length: 51.81\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:05:52\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 103/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 19968 \n",
            "                       Steps per second: 795 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.167s \n",
            "                        Mean value loss: 58.8814\n",
            "                    Mean surrogate loss: 0.0025\n",
            "                      Mean entropy loss: 2.8419\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 51.80\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 51.80\n",
            "                    Mean episode length: 51.80\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:05:51\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 104/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 20160 \n",
            "                       Steps per second: 831 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.156s \n",
            "                        Mean value loss: 77.8597\n",
            "                    Mean surrogate loss: 0.0017\n",
            "                      Mean entropy loss: 2.8419\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 51.85\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 51.85\n",
            "                    Mean episode length: 51.85\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:05:51\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 105/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 20352 \n",
            "                       Steps per second: 842 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.151s \n",
            "                        Mean value loss: 92.5095\n",
            "                    Mean surrogate loss: -0.0223\n",
            "                      Mean entropy loss: 2.8419\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 52.29\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 52.29\n",
            "                    Mean episode length: 52.29\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:05:50\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 106/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 20544 \n",
            "                       Steps per second: 794 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.163s \n",
            "                        Mean value loss: 61.7078\n",
            "                    Mean surrogate loss: -0.0094\n",
            "                      Mean entropy loss: 2.8419\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 53.20\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 53.20\n",
            "                    Mean episode length: 53.20\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:05:50\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 107/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 20736 \n",
            "                       Steps per second: 836 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.156s \n",
            "                        Mean value loss: 77.6254\n",
            "                    Mean surrogate loss: -0.0077\n",
            "                      Mean entropy loss: 2.8419\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 53.47\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 53.47\n",
            "                    Mean episode length: 53.47\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:05:49\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 108/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 20928 \n",
            "                       Steps per second: 842 \n",
            "                        Collection time: 0.071s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 68.8427\n",
            "                    Mean surrogate loss: 0.0105\n",
            "                      Mean entropy loss: 2.8419\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 53.67\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 53.67\n",
            "                    Mean episode length: 53.67\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:05:49\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 109/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 21120 \n",
            "                       Steps per second: 837 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 69.8394\n",
            "                    Mean surrogate loss: 0.0150\n",
            "                      Mean entropy loss: 2.8418\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 53.85\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 53.85\n",
            "                    Mean episode length: 53.85\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:05:48\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 110/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 21312 \n",
            "                       Steps per second: 769 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.178s \n",
            "                        Mean value loss: 60.7773\n",
            "                    Mean surrogate loss: 0.0032\n",
            "                      Mean entropy loss: 2.8418\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 54.60\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 54.60\n",
            "                    Mean episode length: 54.60\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:05:48\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 111/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 21504 \n",
            "                       Steps per second: 847 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 61.6214\n",
            "                    Mean surrogate loss: 0.0088\n",
            "                      Mean entropy loss: 2.8417\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 54.25\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 54.25\n",
            "                    Mean episode length: 54.25\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:05:48\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 112/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 21696 \n",
            "                       Steps per second: 843 \n",
            "                        Collection time: 0.071s \n",
            "                          Learning time: 0.156s \n",
            "                        Mean value loss: 66.5105\n",
            "                    Mean surrogate loss: 0.0023\n",
            "                      Mean entropy loss: 2.8417\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 54.33\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 54.33\n",
            "                    Mean episode length: 54.33\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:05:47\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 113/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 21888 \n",
            "                       Steps per second: 824 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.159s \n",
            "                        Mean value loss: 73.2161\n",
            "                    Mean surrogate loss: -0.0021\n",
            "                      Mean entropy loss: 2.8417\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 55.04\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 55.04\n",
            "                    Mean episode length: 55.04\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:05:47\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 114/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 22080 \n",
            "                       Steps per second: 849 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.153s \n",
            "                        Mean value loss: 82.3542\n",
            "                    Mean surrogate loss: 0.0028\n",
            "                      Mean entropy loss: 2.8416\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 55.35\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 55.35\n",
            "                    Mean episode length: 55.35\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:05:46\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 115/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 22272 \n",
            "                       Steps per second: 769 \n",
            "                        Collection time: 0.084s \n",
            "                          Learning time: 0.166s \n",
            "                        Mean value loss: 71.7587\n",
            "                    Mean surrogate loss: -0.0037\n",
            "                      Mean entropy loss: 2.8415\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 55.75\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 55.75\n",
            "                    Mean episode length: 55.75\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:05:46\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 116/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 22464 \n",
            "                       Steps per second: 826 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.158s \n",
            "                        Mean value loss: 113.8404\n",
            "                    Mean surrogate loss: 0.0002\n",
            "                      Mean entropy loss: 2.8415\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 57.18\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 57.18\n",
            "                    Mean episode length: 57.18\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:05:45\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 117/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 22656 \n",
            "                       Steps per second: 809 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.165s \n",
            "                        Mean value loss: 64.1897\n",
            "                    Mean surrogate loss: -0.0029\n",
            "                      Mean entropy loss: 2.8415\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 56.90\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 56.90\n",
            "                    Mean episode length: 56.90\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:05:45\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 118/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 22848 \n",
            "                       Steps per second: 792 \n",
            "                        Collection time: 0.087s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 73.0678\n",
            "                    Mean surrogate loss: -0.0041\n",
            "                      Mean entropy loss: 2.8415\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 56.52\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 56.52\n",
            "                    Mean episode length: 56.52\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:05:45\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 119/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 23040 \n",
            "                       Steps per second: 793 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.170s \n",
            "                        Mean value loss: 96.3189\n",
            "                    Mean surrogate loss: 0.0023\n",
            "                      Mean entropy loss: 2.8415\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 55.49\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 55.49\n",
            "                    Mean episode length: 55.49\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:05:44\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 120/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 23232 \n",
            "                       Steps per second: 838 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.156s \n",
            "                        Mean value loss: 98.1427\n",
            "                    Mean surrogate loss: 0.0078\n",
            "                      Mean entropy loss: 2.8414\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 55.68\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 55.68\n",
            "                    Mean episode length: 55.68\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:30\n",
            "                                    ETA: 00:05:44\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 121/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 23424 \n",
            "                       Steps per second: 861 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.150s \n",
            "                        Mean value loss: 54.6899\n",
            "                    Mean surrogate loss: -0.0012\n",
            "                      Mean entropy loss: 2.8413\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 54.25\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 54.25\n",
            "                    Mean episode length: 54.25\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:30\n",
            "                                    ETA: 00:05:43\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 122/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 23616 \n",
            "                       Steps per second: 819 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.162s \n",
            "                        Mean value loss: 84.0889\n",
            "                    Mean surrogate loss: 0.0012\n",
            "                      Mean entropy loss: 2.8413\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 54.03\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 54.03\n",
            "                    Mean episode length: 54.03\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:30\n",
            "                                    ETA: 00:05:43\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 123/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 23808 \n",
            "                       Steps per second: 767 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.171s \n",
            "                        Mean value loss: 103.9211\n",
            "                    Mean surrogate loss: 0.0020\n",
            "                      Mean entropy loss: 2.8413\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 54.34\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 54.34\n",
            "                    Mean episode length: 54.34\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:30\n",
            "                                    ETA: 00:05:43\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 124/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 24000 \n",
            "                       Steps per second: 838 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.156s \n",
            "                        Mean value loss: 91.7908\n",
            "                    Mean surrogate loss: -0.0096\n",
            "                      Mean entropy loss: 2.8413\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 54.61\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 54.61\n",
            "                    Mean episode length: 54.61\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:31\n",
            "                                    ETA: 00:05:42\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 125/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 24192 \n",
            "                       Steps per second: 831 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 54.1139\n",
            "                    Mean surrogate loss: 0.0034\n",
            "                      Mean entropy loss: 2.8413\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 55.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 55.07\n",
            "                    Mean episode length: 55.07\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:31\n",
            "                                    ETA: 00:05:42\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 126/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 24384 \n",
            "                       Steps per second: 832 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 80.6684\n",
            "                    Mean surrogate loss: 0.0016\n",
            "                      Mean entropy loss: 2.8414\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 55.54\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 55.54\n",
            "                    Mean episode length: 55.54\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:31\n",
            "                                    ETA: 00:05:41\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 127/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 24576 \n",
            "                       Steps per second: 814 \n",
            "                        Collection time: 0.071s \n",
            "                          Learning time: 0.164s \n",
            "                        Mean value loss: 74.8316\n",
            "                    Mean surrogate loss: -0.0001\n",
            "                      Mean entropy loss: 2.8414\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 54.87\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 54.87\n",
            "                    Mean episode length: 54.87\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:31\n",
            "                                    ETA: 00:05:41\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 128/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 24768 \n",
            "                       Steps per second: 797 \n",
            "                        Collection time: 0.085s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 77.3474\n",
            "                    Mean surrogate loss: 0.0064\n",
            "                      Mean entropy loss: 2.8414\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 55.63\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 55.63\n",
            "                    Mean episode length: 55.63\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:32\n",
            "                                    ETA: 00:05:40\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 129/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 24960 \n",
            "                       Steps per second: 835 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 96.1430\n",
            "                    Mean surrogate loss: -0.0104\n",
            "                      Mean entropy loss: 2.8414\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 55.00\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 55.00\n",
            "                    Mean episode length: 55.00\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:32\n",
            "                                    ETA: 00:05:40\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 130/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 25152 \n",
            "                       Steps per second: 850 \n",
            "                        Collection time: 0.070s \n",
            "                          Learning time: 0.156s \n",
            "                        Mean value loss: 38.6154\n",
            "                    Mean surrogate loss: 0.0095\n",
            "                      Mean entropy loss: 2.8414\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 55.00\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 55.00\n",
            "                    Mean episode length: 55.00\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:32\n",
            "                                    ETA: 00:05:39\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 131/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 25344 \n",
            "                       Steps per second: 808 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.165s \n",
            "                        Mean value loss: 81.7119\n",
            "                    Mean surrogate loss: -0.0018\n",
            "                      Mean entropy loss: 2.8414\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 56.46\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 56.46\n",
            "                    Mean episode length: 56.46\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:32\n",
            "                                    ETA: 00:05:39\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 132/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 25536 \n",
            "                       Steps per second: 788 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.170s \n",
            "                        Mean value loss: 71.4761\n",
            "                    Mean surrogate loss: -0.0060\n",
            "                      Mean entropy loss: 2.8414\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 56.64\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 56.64\n",
            "                    Mean episode length: 56.64\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:33\n",
            "                                    ETA: 00:05:39\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 133/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 25728 \n",
            "                       Steps per second: 832 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 64.6986\n",
            "                    Mean surrogate loss: 0.0036\n",
            "                      Mean entropy loss: 2.8414\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 56.89\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 56.89\n",
            "                    Mean episode length: 56.89\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:33\n",
            "                                    ETA: 00:05:38\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 134/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 25920 \n",
            "                       Steps per second: 861 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.149s \n",
            "                        Mean value loss: 93.2962\n",
            "                    Mean surrogate loss: 0.0072\n",
            "                      Mean entropy loss: 2.8414\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 58.04\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 58.04\n",
            "                    Mean episode length: 58.04\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:33\n",
            "                                    ETA: 00:05:38\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 135/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 26112 \n",
            "                       Steps per second: 845 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.153s \n",
            "                        Mean value loss: 64.5200\n",
            "                    Mean surrogate loss: 0.0015\n",
            "                      Mean entropy loss: 2.8414\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 58.53\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 58.53\n",
            "                    Mean episode length: 58.53\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:33\n",
            "                                    ETA: 00:05:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 136/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 26304 \n",
            "                       Steps per second: 852 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.152s \n",
            "                        Mean value loss: 48.0406\n",
            "                    Mean surrogate loss: 0.0061\n",
            "                      Mean entropy loss: 2.8414\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 58.08\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 58.08\n",
            "                    Mean episode length: 58.08\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:33\n",
            "                                    ETA: 00:05:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 137/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 26496 \n",
            "                       Steps per second: 763 \n",
            "                        Collection time: 0.090s \n",
            "                          Learning time: 0.162s \n",
            "                        Mean value loss: 66.0394\n",
            "                    Mean surrogate loss: 0.0002\n",
            "                      Mean entropy loss: 2.8414\n",
            "                          Mean rnd loss: 0.0009\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 56.39\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 56.39\n",
            "                    Mean episode length: 56.39\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:34\n",
            "                                    ETA: 00:05:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 138/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 26688 \n",
            "                       Steps per second: 869 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.148s \n",
            "                        Mean value loss: 108.6397\n",
            "                    Mean surrogate loss: 0.0017\n",
            "                      Mean entropy loss: 2.8414\n",
            "                          Mean rnd loss: 0.0008\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 58.44\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 58.44\n",
            "                    Mean episode length: 58.44\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:34\n",
            "                                    ETA: 00:05:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 139/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 26880 \n",
            "                       Steps per second: 839 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.156s \n",
            "                        Mean value loss: 107.1965\n",
            "                    Mean surrogate loss: 0.0153\n",
            "                      Mean entropy loss: 2.8413\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 59.76\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 59.76\n",
            "                    Mean episode length: 59.76\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:34\n",
            "                                    ETA: 00:05:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 140/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 27072 \n",
            "                       Steps per second: 803 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.162s \n",
            "                        Mean value loss: 97.9445\n",
            "                    Mean surrogate loss: -0.0011\n",
            "                      Mean entropy loss: 2.8413\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 58.97\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 58.97\n",
            "                    Mean episode length: 58.97\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:34\n",
            "                                    ETA: 00:05:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 141/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 27264 \n",
            "                       Steps per second: 766 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.179s \n",
            "                        Mean value loss: 81.1611\n",
            "                    Mean surrogate loss: -0.0016\n",
            "                      Mean entropy loss: 2.8412\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 58.68\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 58.68\n",
            "                    Mean episode length: 58.68\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:35\n",
            "                                    ETA: 00:05:35\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 142/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 27456 \n",
            "                       Steps per second: 852 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.149s \n",
            "                        Mean value loss: 72.5091\n",
            "                    Mean surrogate loss: 0.0018\n",
            "                      Mean entropy loss: 2.8412\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 58.81\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 58.81\n",
            "                    Mean episode length: 58.81\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:35\n",
            "                                    ETA: 00:05:35\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 143/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 27648 \n",
            "                       Steps per second: 829 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.160s \n",
            "                        Mean value loss: 81.2294\n",
            "                    Mean surrogate loss: -0.0049\n",
            "                      Mean entropy loss: 2.8412\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 56.94\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 56.94\n",
            "                    Mean episode length: 56.94\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:35\n",
            "                                    ETA: 00:05:34\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 144/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 27840 \n",
            "                       Steps per second: 807 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.165s \n",
            "                        Mean value loss: 82.8785\n",
            "                    Mean surrogate loss: 0.0224\n",
            "                      Mean entropy loss: 2.8411\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 57.57\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 57.57\n",
            "                    Mean episode length: 57.57\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:35\n",
            "                                    ETA: 00:05:34\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 145/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 28032 \n",
            "                       Steps per second: 565 \n",
            "                        Collection time: 0.114s \n",
            "                          Learning time: 0.226s \n",
            "                        Mean value loss: 93.5261\n",
            "                    Mean surrogate loss: -0.0223\n",
            "                      Mean entropy loss: 2.8409\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 58.80\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 58.80\n",
            "                    Mean episode length: 58.80\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.34s\n",
            "                           Time elapsed: 00:00:36\n",
            "                                    ETA: 00:05:35\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 146/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 28224 \n",
            "                       Steps per second: 634 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.207s \n",
            "                        Mean value loss: 86.6686\n",
            "                    Mean surrogate loss: 0.0045\n",
            "                      Mean entropy loss: 2.8408\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 57.46\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 57.46\n",
            "                    Mean episode length: 57.46\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:36\n",
            "                                    ETA: 00:05:35\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 147/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 28416 \n",
            "                       Steps per second: 610 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.217s \n",
            "                        Mean value loss: 36.5562\n",
            "                    Mean surrogate loss: -0.0015\n",
            "                      Mean entropy loss: 2.8407\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 57.46\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 57.46\n",
            "                    Mean episode length: 57.46\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:00:36\n",
            "                                    ETA: 00:05:35\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 148/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 28608 \n",
            "                       Steps per second: 640 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.205s \n",
            "                        Mean value loss: 103.2884\n",
            "                    Mean surrogate loss: 0.0092\n",
            "                      Mean entropy loss: 2.8407\n",
            "                          Mean rnd loss: 0.0011\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 58.79\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 58.79\n",
            "                    Mean episode length: 58.79\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:37\n",
            "                                    ETA: 00:05:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 149/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 28800 \n",
            "                       Steps per second: 626 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.204s \n",
            "                        Mean value loss: 94.9784\n",
            "                    Mean surrogate loss: -0.0070\n",
            "                      Mean entropy loss: 2.8407\n",
            "                          Mean rnd loss: 0.0013\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 60.11\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 60.11\n",
            "                    Mean episode length: 60.11\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:00:37\n",
            "                                    ETA: 00:05:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 150/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 28992 \n",
            "                       Steps per second: 551 \n",
            "                        Collection time: 0.107s \n",
            "                          Learning time: 0.241s \n",
            "                        Mean value loss: 105.8393\n",
            "                    Mean surrogate loss: -0.0034\n",
            "                      Mean entropy loss: 2.8407\n",
            "                          Mean rnd loss: 0.0010\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 60.99\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 60.99\n",
            "                    Mean episode length: 60.99\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.35s\n",
            "                           Time elapsed: 00:00:37\n",
            "                                    ETA: 00:05:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 151/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 29184 \n",
            "                       Steps per second: 539 \n",
            "                        Collection time: 0.112s \n",
            "                          Learning time: 0.244s \n",
            "                        Mean value loss: 51.4200\n",
            "                    Mean surrogate loss: 0.0222\n",
            "                      Mean entropy loss: 2.8407\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 61.90\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 61.90\n",
            "                    Mean episode length: 61.90\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.36s\n",
            "                           Time elapsed: 00:00:38\n",
            "                                    ETA: 00:05:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 152/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 29376 \n",
            "                       Steps per second: 601 \n",
            "                        Collection time: 0.115s \n",
            "                          Learning time: 0.204s \n",
            "                        Mean value loss: 57.5901\n",
            "                    Mean surrogate loss: -0.0150\n",
            "                      Mean entropy loss: 2.8406\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 62.16\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 62.16\n",
            "                    Mean episode length: 62.16\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.32s\n",
            "                           Time elapsed: 00:00:38\n",
            "                                    ETA: 00:05:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 153/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 29568 \n",
            "                       Steps per second: 841 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 123.0534\n",
            "                    Mean surrogate loss: 0.0114\n",
            "                      Mean entropy loss: 2.8406\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 62.52\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 62.52\n",
            "                    Mean episode length: 62.52\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:38\n",
            "                                    ETA: 00:05:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 154/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 29760 \n",
            "                       Steps per second: 823 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.159s \n",
            "                        Mean value loss: 54.7371\n",
            "                    Mean surrogate loss: -0.0139\n",
            "                      Mean entropy loss: 2.8405\n",
            "                          Mean rnd loss: 0.0007\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 62.30\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 62.30\n",
            "                    Mean episode length: 62.30\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:38\n",
            "                                    ETA: 00:05:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 155/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 29952 \n",
            "                       Steps per second: 790 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.162s \n",
            "                        Mean value loss: 88.7119\n",
            "                    Mean surrogate loss: 0.0004\n",
            "                      Mean entropy loss: 2.8404\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 63.35\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 63.35\n",
            "                    Mean episode length: 63.35\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:39\n",
            "                                    ETA: 00:05:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 156/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 30144 \n",
            "                       Steps per second: 777 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.175s \n",
            "                        Mean value loss: 107.4921\n",
            "                    Mean surrogate loss: -0.0199\n",
            "                      Mean entropy loss: 2.8403\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 63.39\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 63.39\n",
            "                    Mean episode length: 63.39\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:39\n",
            "                                    ETA: 00:05:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 157/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 30336 \n",
            "                       Steps per second: 832 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 53.8437\n",
            "                    Mean surrogate loss: -0.0224\n",
            "                      Mean entropy loss: 2.8402\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 63.77\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 63.77\n",
            "                    Mean episode length: 63.77\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:39\n",
            "                                    ETA: 00:05:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 158/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 30528 \n",
            "                       Steps per second: 849 \n",
            "                        Collection time: 0.071s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 125.9239\n",
            "                    Mean surrogate loss: 0.0208\n",
            "                      Mean entropy loss: 2.8400\n",
            "                          Mean rnd loss: 0.0013\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 66.85\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 66.85\n",
            "                    Mean episode length: 66.85\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:39\n",
            "                                    ETA: 00:05:35\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 159/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 30720 \n",
            "                       Steps per second: 863 \n",
            "                        Collection time: 0.071s \n",
            "                          Learning time: 0.151s \n",
            "                        Mean value loss: 53.6530\n",
            "                    Mean surrogate loss: 0.0633\n",
            "                      Mean entropy loss: 2.8400\n",
            "                          Mean rnd loss: 0.0009\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 67.48\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 67.48\n",
            "                    Mean episode length: 67.48\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:40\n",
            "                                    ETA: 00:05:35\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 160/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 30912 \n",
            "                       Steps per second: 866 \n",
            "                        Collection time: 0.071s \n",
            "                          Learning time: 0.150s \n",
            "                        Mean value loss: 56.3751\n",
            "                    Mean surrogate loss: 0.0171\n",
            "                      Mean entropy loss: 2.8399\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 68.64\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 68.64\n",
            "                    Mean episode length: 68.64\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:40\n",
            "                                    ETA: 00:05:34\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 161/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 31104 \n",
            "                       Steps per second: 827 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.150s \n",
            "                        Mean value loss: 28.6215\n",
            "                    Mean surrogate loss: -0.0179\n",
            "                      Mean entropy loss: 2.8399\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 68.64\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 68.64\n",
            "                    Mean episode length: 68.64\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:40\n",
            "                                    ETA: 00:05:34\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 162/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 31296 \n",
            "                       Steps per second: 855 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.149s \n",
            "                        Mean value loss: 91.2113\n",
            "                    Mean surrogate loss: -0.0068\n",
            "                      Mean entropy loss: 2.8399\n",
            "                          Mean rnd loss: 0.0012\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 69.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 69.07\n",
            "                    Mean episode length: 69.07\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:40\n",
            "                                    ETA: 00:05:33\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 163/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 31488 \n",
            "                       Steps per second: 845 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.154s \n",
            "                        Mean value loss: 60.3055\n",
            "                    Mean surrogate loss: 0.0071\n",
            "                      Mean entropy loss: 2.8398\n",
            "                          Mean rnd loss: 0.0017\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 70.45\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 70.45\n",
            "                    Mean episode length: 70.45\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:40\n",
            "                                    ETA: 00:05:33\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 164/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 31680 \n",
            "                       Steps per second: 814 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.152s \n",
            "                        Mean value loss: 79.0640\n",
            "                    Mean surrogate loss: 0.0155\n",
            "                      Mean entropy loss: 2.8398\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 71.35\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 71.35\n",
            "                    Mean episode length: 71.35\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:41\n",
            "                                    ETA: 00:05:33\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 165/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 31872 \n",
            "                       Steps per second: 786 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.169s \n",
            "                        Mean value loss: 68.5401\n",
            "                    Mean surrogate loss: -0.0198\n",
            "                      Mean entropy loss: 2.8398\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 72.50\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 72.50\n",
            "                    Mean episode length: 72.50\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:41\n",
            "                                    ETA: 00:05:32\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 166/1500                           \u001b[0m \n",
            "\n",
            "                            Total steps: 32064 \n",
            "                       Steps per second: 841 \n",
            "                        Collection time: 0.071s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 54.5273\n",
            "                    Mean surrogate loss: 0.0010\n",
            "                      Mean entropy loss: 2.8398\n",
            "                          Mean rnd loss: 0.0007\n",
            "                     Mean symmetry loss: nan\n",
            "                  Mean extrinsic reward: 71.94\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 71.94\n",
            "                    Mean episode length: 71.94\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:41\n",
            "                                    ETA: 00:05:32\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1401590998.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCartPoleVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOnPolicyRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_learning_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_iterations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/rsl_rl/rsl_rl/runners/on_policy_runner.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, num_learning_iterations, init_at_random_ep_len)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;31m# Update policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/rsl_rl/rsl_rl/algorithms/ppo.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;31m# Compute the gradients for PPO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m             \u001b[0;31m# Compute the gradients for RND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "'''import yaml\n",
        "from types import SimpleNamespace\n",
        "from rsl_rl.runners import OnPolicyRunner\n",
        "#import cartpole_symmetry\n",
        "\n",
        "yaml_config_string = \"\"\"\n",
        "runner:\n",
        "  class_name: OnPolicyRunner\n",
        "  # General\n",
        "  num_steps_per_env: 24  # Number of steps per environment per iteration\n",
        "  max_iterations: 1500    # Number of policy updates\n",
        "  seed: 1\n",
        "  # Observations\n",
        "  obs_groups: {\"policy\": [\"policy\"], \"critic\": [\"policy\", \"privileged\"]}  # Maps observation groups to sets. See `vec_env.py` for more information\n",
        "  # Logging parameters\n",
        "  save_interval: 50  # Check for potential saves every `save_interval` iterations\n",
        "  experiment_name: walking_experiment\n",
        "  run_name: \"\"\n",
        "  # Logging writer\n",
        "  logger: tensorboard  # tensorboard, neptune, wandb\n",
        "  neptune_project: legged_gym\n",
        "  wandb_project: legged_gym\n",
        "\n",
        "  # Policy\n",
        "  policy:\n",
        "    class_name: ActorCritic\n",
        "    activation: elu\n",
        "    actor_obs_normalization: false\n",
        "    critic_obs_normalization: false\n",
        "    actor_hidden_dims: [256, 256, 256]\n",
        "    critic_hidden_dims: [256, 256, 256]\n",
        "    init_noise_std: 1.0\n",
        "    noise_std_type: \"scalar\"  # 'scalar' or 'log'\n",
        "    state_dependent_std: false\n",
        "\n",
        "  # Algorithm\n",
        "  algorithm:\n",
        "    class_name: PPO\n",
        "    # Training\n",
        "    learning_rate: 0.001\n",
        "    num_learning_epochs: 5\n",
        "    num_mini_batches: 4  # mini batch size = num_envs * num_steps / num_mini_batches\n",
        "    schedule: adaptive  # adaptive, fixed\n",
        "    # Value function\n",
        "    value_loss_coef: 1.0\n",
        "    clip_param: 0.2\n",
        "    use_clipped_value_loss: true\n",
        "    # Surrogate loss\n",
        "    desired_kl: 0.01\n",
        "    entropy_coef: 0.01\n",
        "    gamma: 0.99\n",
        "    lam: 0.95\n",
        "    max_grad_norm: 1.0\n",
        "    # Miscellaneous\n",
        "    normalize_advantage_per_mini_batch: false\n",
        "\n",
        "    # Random network distillation\n",
        "    rnd_cfg:\n",
        "      weight: 0.0  # Initial weight of the RND reward\n",
        "      weight_schedule: null  # This is a dictionary with a required key called \"mode\". Please check the RND module for more information\n",
        "      reward_normalization: false  # Whether to normalize RND reward\n",
        "      # Learning parameters\n",
        "      learning_rate: 0.001  # Learning rate for RND\n",
        "      # Network parameters\n",
        "      num_outputs: 1  # Number of outputs of RND network. Note: if -1, then the network will use dimensions of the observation\n",
        "      predictor_hidden_dims: [-1]  # Hidden dimensions of predictor network\n",
        "      target_hidden_dims: [-1]  # Hidden dimensions of target network\n",
        "\n",
        "    # Symmetry augmentation\n",
        "    symmetry_cfg:\n",
        "      use_data_augmentation: true  # This adds symmetric trajectories to the batch\n",
        "      use_mirror_loss: false  # This adds symmetry loss term to the loss function\n",
        "      data_augmentation_func: \"rsl_rl.modules.cartpole_symmetry_utils:cartpole_symmetry\"  # String containing the module and function name to import\n",
        "      # Example: \"legged_gym.envs.locomotion.anymal_c.symmetry:get_symmetric_states\"\n",
        "      mirror_loss_coeff: 4.0  # Coefficient for symmetry loss term. If 0, no symmetry loss is used\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_train_cfg():\n",
        "    raw = yaml.safe_load(yaml_config_string)\n",
        "    # Optional: turn top-level dict into an object-like cfg\n",
        "    return SimpleNamespace(**raw)\n",
        "import os\n",
        "\n",
        "log_dir = \"./logs_cartpole\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "# Usage:\n",
        "train_cfg = get_train_cfg()\n",
        "print(train_cfg.runner[\"class_name\"])   # 'OnPolicyRunner'\n",
        "# later:\n",
        "env = CartPoleVecEnv(num_envs=8, device=\"cuda:0\")\n",
        "runner = OnPolicyRunner(env, train_cfg.runner, log_dir=log_dir)\n",
        "runner.learn(num_learning_iterations=train_cfg.runner[\"max_iterations\"])\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# manual flip output\n"
      ],
      "metadata": {
        "id": "_SHQZPZ4s9l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "from types import SimpleNamespace\n",
        "from rsl_rl.runners import OnPolicyRunner\n",
        "#import cartpole_symmetry\n",
        "\n",
        "yaml_config_string = \"\"\"\n",
        "runner:\n",
        "  class_name: OnPolicyRunner\n",
        "  # General\n",
        "  num_steps_per_env: 24  # Number of steps per environment per iteration\n",
        "  max_iterations: 100    # Number of policy updates\n",
        "  seed: 1\n",
        "  # Observations\n",
        "  obs_groups: {\"policy\": [\"policy\"], \"critic\": [\"policy\", \"privileged\"]}  # Maps observation groups to sets. See `vec_env.py` for more information\n",
        "  # Logging parameters\n",
        "  save_interval: 50  # Check for potential saves every `save_interval` iterations\n",
        "  experiment_name: walking_experiment\n",
        "  run_name: \"\"\n",
        "  # Logging writer\n",
        "  logger: tensorboard  # tensorboard, neptune, wandb\n",
        "  neptune_project: legged_gym\n",
        "  wandb_project: legged_gym\n",
        "\n",
        "  # Policy\n",
        "  policy:\n",
        "    class_name: ActorCritic\n",
        "    activation: elu\n",
        "    actor_obs_normalization: false\n",
        "    critic_obs_normalization: false\n",
        "    actor_hidden_dims: [256, 256, 256]\n",
        "    critic_hidden_dims: [256, 256, 256]\n",
        "    init_noise_std: 1.0\n",
        "    noise_std_type: \"scalar\"  # 'scalar' or 'log'\n",
        "    state_dependent_std: false\n",
        "\n",
        "  # Algorithm\n",
        "  algorithm:\n",
        "    class_name: PPO\n",
        "    # Training\n",
        "    learning_rate: 0.001\n",
        "    num_learning_epochs: 5\n",
        "    num_mini_batches: 4  # mini batch size = num_envs * num_steps / num_mini_batches\n",
        "    schedule: adaptive  # adaptive, fixed\n",
        "    # Value function\n",
        "    value_loss_coef: 1.0\n",
        "    clip_param: 0.2\n",
        "    use_clipped_value_loss: true\n",
        "    # Surrogate loss\n",
        "    desired_kl: 0.01\n",
        "    entropy_coef: 0.01\n",
        "    gamma: 0.99\n",
        "    lam: 0.95\n",
        "    max_grad_norm: 1.0\n",
        "    # Miscellaneous\n",
        "    normalize_advantage_per_mini_batch: false\n",
        "\n",
        "    # Random network distillation\n",
        "    rnd_cfg:\n",
        "      weight: 0.0  # Initial weight of the RND reward\n",
        "      weight_schedule: null  # This is a dictionary with a required key called \"mode\". Please check the RND module for more information\n",
        "      reward_normalization: false  # Whether to normalize RND reward\n",
        "      # Learning parameters\n",
        "      learning_rate: 0.001  # Learning rate for RND\n",
        "      # Network parameters\n",
        "      num_outputs: 1  # Number of outputs of RND network. Note: if -1, then the network will use dimensions of the observation\n",
        "      predictor_hidden_dims: [-1]  # Hidden dimensions of predictor network\n",
        "      target_hidden_dims: [-1]  # Hidden dimensions of target network\n",
        "\n",
        "    # Symmetry augmentation\n",
        "    symmetry_cfg:\n",
        "      use_data_augmentation: true  # This adds symmetric trajectories to the batch\n",
        "      use_mirror_loss: false  # This adds symmetry loss term to the loss function\n",
        "      data_augmentation_func: \"rsl_rl.modules.cartpole_symmetry_utils:cartpole_symmetry\"  # String containing the module and function name to import\n",
        "      # Example: \"legged_gym.envs.locomotion.anymal_c.symmetry:get_symmetric_states\"\n",
        "      mirror_loss_coeff: 0.0  # Coefficient for symmetry loss term. If 0, no symmetry loss is used\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_train_cfg():\n",
        "    raw = yaml.safe_load(yaml_config_string)\n",
        "    # Optional: turn top-level dict into an object-like cfg\n",
        "    return SimpleNamespace(**raw)\n",
        "import os\n",
        "\n",
        "log_dir = \"./logs_cartpole\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "# Usage:\n",
        "train_cfg = get_train_cfg()\n",
        "print(train_cfg.runner[\"class_name\"])   # 'OnPolicyRunner'\n",
        "# later:\n",
        "env = CartPoleVecEnv(num_envs=8, device=\"cuda:0\")\n",
        "runner = OnPolicyRunner(env, train_cfg.runner, log_dir=log_dir)\n",
        "runner.learn(num_learning_iterations=train_cfg.runner[\"max_iterations\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhT3xybuyaEJ",
        "outputId": "77cb7069-ebcf-4d51-c829-2f8bec0c9a03"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OnPolicyRunner\n",
            "--------------------------------------------------------------------------------\n",
            "Resolved observation sets: \n",
            "\t policy :  ['policy']\n",
            "\t critic :  ['policy', 'privileged']\n",
            "\t rnd_state :  ['policy']\n",
            "--------------------------------------------------------------------------------\n",
            "Actor MLP: MLP(\n",
            "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n",
            "Critic MLP: MLP(\n",
            "  (0): Linear(in_features=8, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/rsl_rl/rsl_rl/utils/utils.py:273: UserWarning: The observation configuration dictionary 'obs_groups' must contain the 'rnd_state' key. As the configuration for 'rnd_state' is missing, the observations from the 'policy' set are used. Consider adding the 'rnd_state' key to the 'obs_groups' dictionary for clarity. This behavior will be removed in a future version.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 0/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 192 \n",
            "                       Steps per second: 169 \n",
            "                        Collection time: 0.502s \n",
            "                          Learning time: 0.629s \n",
            "                        Mean value loss: 50.1557\n",
            "                    Mean surrogate loss: 0.2445\n",
            "                      Mean entropy loss: 2.8346\n",
            "                          Mean rnd loss: 0.0689\n",
            "                     Mean symmetry loss: 0.1200\n",
            "                  Mean extrinsic reward: 15.67\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 15.67\n",
            "                    Mean episode length: 15.67\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 1.13s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:01:51\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 1/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 384 \n",
            "                       Steps per second: 328 \n",
            "                        Collection time: 0.179s \n",
            "                          Learning time: 0.405s \n",
            "                        Mean value loss: 35.4190\n",
            "                    Mean surrogate loss: 0.0133\n",
            "                      Mean entropy loss: 2.8335\n",
            "                          Mean rnd loss: 0.0539\n",
            "                     Mean symmetry loss: 0.0151\n",
            "                  Mean extrinsic reward: 19.06\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 19.06\n",
            "                    Mean episode length: 19.06\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.58s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:01:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 2/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 576 \n",
            "                       Steps per second: 744 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.177s \n",
            "                        Mean value loss: 35.3589\n",
            "                    Mean surrogate loss: 0.0027\n",
            "                      Mean entropy loss: 2.8323\n",
            "                          Mean rnd loss: 0.0154\n",
            "                     Mean symmetry loss: 0.0084\n",
            "                  Mean extrinsic reward: 18.33\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 18.33\n",
            "                    Mean episode length: 18.33\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:01:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 3/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 768 \n",
            "                       Steps per second: 759 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.179s \n",
            "                        Mean value loss: 37.4069\n",
            "                    Mean surrogate loss: 0.0038\n",
            "                      Mean entropy loss: 2.8303\n",
            "                          Mean rnd loss: 0.0158\n",
            "                     Mean symmetry loss: 0.0109\n",
            "                  Mean extrinsic reward: 18.77\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 18.77\n",
            "                    Mean episode length: 18.77\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:00:53\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 4/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 960 \n",
            "                       Steps per second: 705 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.194s \n",
            "                        Mean value loss: 40.6125\n",
            "                    Mean surrogate loss: -0.0070\n",
            "                      Mean entropy loss: 2.8307\n",
            "                          Mean rnd loss: 0.0081\n",
            "                     Mean symmetry loss: 0.0016\n",
            "                  Mean extrinsic reward: 18.79\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 18.79\n",
            "                    Mean episode length: 18.79\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:00:47\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 5/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1152 \n",
            "                       Steps per second: 744 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.184s \n",
            "                        Mean value loss: 50.2605\n",
            "                    Mean surrogate loss: -0.0003\n",
            "                      Mean entropy loss: 2.8352\n",
            "                          Mean rnd loss: 0.0093\n",
            "                     Mean symmetry loss: 0.0026\n",
            "                  Mean extrinsic reward: 19.96\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 19.96\n",
            "                    Mean episode length: 19.96\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:00:43\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 6/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1344 \n",
            "                       Steps per second: 769 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.176s \n",
            "                        Mean value loss: 43.5906\n",
            "                    Mean surrogate loss: -0.0081\n",
            "                      Mean entropy loss: 2.8420\n",
            "                          Mean rnd loss: 0.0043\n",
            "                     Mean symmetry loss: 0.0260\n",
            "                  Mean extrinsic reward: 21.58\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 21.58\n",
            "                    Mean episode length: 21.58\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:39\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 7/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1536 \n",
            "                       Steps per second: 759 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.180s \n",
            "                        Mean value loss: 60.0317\n",
            "                    Mean surrogate loss: -0.0061\n",
            "                      Mean entropy loss: 2.8434\n",
            "                          Mean rnd loss: 0.0026\n",
            "                     Mean symmetry loss: 0.0079\n",
            "                  Mean extrinsic reward: 22.30\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 22.30\n",
            "                    Mean episode length: 22.30\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 8/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1728 \n",
            "                       Steps per second: 728 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.191s \n",
            "                        Mean value loss: 57.7507\n",
            "                    Mean surrogate loss: -0.0171\n",
            "                      Mean entropy loss: 2.8428\n",
            "                          Mean rnd loss: 0.0032\n",
            "                     Mean symmetry loss: 0.0055\n",
            "                  Mean extrinsic reward: 23.32\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 23.32\n",
            "                    Mean episode length: 23.32\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:35\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 9/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1920 \n",
            "                       Steps per second: 750 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.183s \n",
            "                        Mean value loss: 60.0231\n",
            "                    Mean surrogate loss: 0.0090\n",
            "                      Mean entropy loss: 2.8433\n",
            "                          Mean rnd loss: 0.0031\n",
            "                     Mean symmetry loss: 0.0076\n",
            "                  Mean extrinsic reward: 26.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 26.07\n",
            "                    Mean episode length: 26.07\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:33\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 10/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2112 \n",
            "                       Steps per second: 718 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.189s \n",
            "                        Mean value loss: 53.2649\n",
            "                    Mean surrogate loss: 0.0156\n",
            "                      Mean entropy loss: 2.8458\n",
            "                          Mean rnd loss: 0.0059\n",
            "                     Mean symmetry loss: 0.0333\n",
            "                  Mean extrinsic reward: 26.65\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 26.65\n",
            "                    Mean episode length: 26.65\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:32\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 11/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2304 \n",
            "                       Steps per second: 769 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.177s \n",
            "                        Mean value loss: 53.1323\n",
            "                    Mean surrogate loss: 0.0165\n",
            "                      Mean entropy loss: 2.8461\n",
            "                          Mean rnd loss: 0.0048\n",
            "                     Mean symmetry loss: 0.0153\n",
            "                  Mean extrinsic reward: 29.97\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 29.97\n",
            "                    Mean episode length: 29.97\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:31\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 12/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2496 \n",
            "                       Steps per second: 680 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.200s \n",
            "                        Mean value loss: 63.6901\n",
            "                    Mean surrogate loss: -0.0013\n",
            "                      Mean entropy loss: 2.8467\n",
            "                          Mean rnd loss: 0.0024\n",
            "                     Mean symmetry loss: 0.0182\n",
            "                  Mean extrinsic reward: 29.97\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 29.97\n",
            "                    Mean episode length: 29.97\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:30\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 13/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2688 \n",
            "                       Steps per second: 756 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.180s \n",
            "                        Mean value loss: 57.4636\n",
            "                    Mean surrogate loss: 0.0073\n",
            "                      Mean entropy loss: 2.8480\n",
            "                          Mean rnd loss: 0.0034\n",
            "                     Mean symmetry loss: 0.0153\n",
            "                  Mean extrinsic reward: 32.03\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 32.03\n",
            "                    Mean episode length: 32.03\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:29\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 14/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2880 \n",
            "                       Steps per second: 719 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.191s \n",
            "                        Mean value loss: 59.0699\n",
            "                    Mean surrogate loss: 0.0158\n",
            "                      Mean entropy loss: 2.8484\n",
            "                          Mean rnd loss: 0.0040\n",
            "                     Mean symmetry loss: 0.0150\n",
            "                  Mean extrinsic reward: 32.82\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 32.82\n",
            "                    Mean episode length: 32.82\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:28\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 15/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3072 \n",
            "                       Steps per second: 702 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.196s \n",
            "                        Mean value loss: 56.0277\n",
            "                    Mean surrogate loss: 0.0030\n",
            "                      Mean entropy loss: 2.8491\n",
            "                          Mean rnd loss: 0.0023\n",
            "                     Mean symmetry loss: 0.0355\n",
            "                  Mean extrinsic reward: 36.37\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 36.37\n",
            "                    Mean episode length: 36.37\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:28\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 16/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3264 \n",
            "                       Steps per second: 608 \n",
            "                        Collection time: 0.091s \n",
            "                          Learning time: 0.225s \n",
            "                        Mean value loss: 46.3931\n",
            "                    Mean surrogate loss: 0.0007\n",
            "                      Mean entropy loss: 2.8493\n",
            "                          Mean rnd loss: 0.0025\n",
            "                     Mean symmetry loss: 0.0248\n",
            "                  Mean extrinsic reward: 41.39\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 41.39\n",
            "                    Mean episode length: 41.39\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.32s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 17/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3456 \n",
            "                       Steps per second: 687 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.196s \n",
            "                        Mean value loss: 52.0041\n",
            "                    Mean surrogate loss: -0.0118\n",
            "                      Mean entropy loss: 2.8501\n",
            "                          Mean rnd loss: 0.0026\n",
            "                     Mean symmetry loss: 0.0082\n",
            "                  Mean extrinsic reward: 41.89\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 41.89\n",
            "                    Mean episode length: 41.89\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 18/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3648 \n",
            "                       Steps per second: 695 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.197s \n",
            "                        Mean value loss: 45.8421\n",
            "                    Mean surrogate loss: -0.0071\n",
            "                      Mean entropy loss: 2.8509\n",
            "                          Mean rnd loss: 0.0024\n",
            "                     Mean symmetry loss: 0.0072\n",
            "                  Mean extrinsic reward: 45.12\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 45.12\n",
            "                    Mean episode length: 45.12\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:26\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 19/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3840 \n",
            "                       Steps per second: 670 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.208s \n",
            "                        Mean value loss: 51.9133\n",
            "                    Mean surrogate loss: 0.0104\n",
            "                      Mean entropy loss: 2.8489\n",
            "                          Mean rnd loss: 0.0008\n",
            "                     Mean symmetry loss: 0.0032\n",
            "                  Mean extrinsic reward: 45.12\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 45.12\n",
            "                    Mean episode length: 45.12\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:26\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 20/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4032 \n",
            "                       Steps per second: 654 \n",
            "                        Collection time: 0.093s \n",
            "                          Learning time: 0.200s \n",
            "                        Mean value loss: 51.7518\n",
            "                    Mean surrogate loss: 0.0067\n",
            "                      Mean entropy loss: 2.8474\n",
            "                          Mean rnd loss: 0.0010\n",
            "                     Mean symmetry loss: 0.0041\n",
            "                  Mean extrinsic reward: 46.03\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 46.03\n",
            "                    Mean episode length: 46.03\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 21/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4224 \n",
            "                       Steps per second: 617 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.229s \n",
            "                        Mean value loss: 47.5209\n",
            "                    Mean surrogate loss: -0.0085\n",
            "                      Mean entropy loss: 2.8472\n",
            "                          Mean rnd loss: 0.0016\n",
            "                     Mean symmetry loss: 0.0072\n",
            "                  Mean extrinsic reward: 46.95\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 46.95\n",
            "                    Mean episode length: 46.95\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 22/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4416 \n",
            "                       Steps per second: 665 \n",
            "                        Collection time: 0.085s \n",
            "                          Learning time: 0.203s \n",
            "                        Mean value loss: 45.2267\n",
            "                    Mean surrogate loss: 0.0202\n",
            "                      Mean entropy loss: 2.8475\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0326\n",
            "                  Mean extrinsic reward: 49.11\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 49.11\n",
            "                    Mean episode length: 49.11\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 23/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4608 \n",
            "                       Steps per second: 667 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.210s \n",
            "                        Mean value loss: 38.9829\n",
            "                    Mean surrogate loss: -0.0098\n",
            "                      Mean entropy loss: 2.8475\n",
            "                          Mean rnd loss: 0.0009\n",
            "                     Mean symmetry loss: 0.0173\n",
            "                  Mean extrinsic reward: 50.18\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 50.18\n",
            "                    Mean episode length: 50.18\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 24/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4800 \n",
            "                       Steps per second: 726 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 51.9097\n",
            "                    Mean surrogate loss: -0.0005\n",
            "                      Mean entropy loss: 2.8473\n",
            "                          Mean rnd loss: 0.0009\n",
            "                     Mean symmetry loss: 0.0128\n",
            "                  Mean extrinsic reward: 50.29\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 50.29\n",
            "                    Mean episode length: 50.29\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 25/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4992 \n",
            "                       Steps per second: 684 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.202s \n",
            "                        Mean value loss: 39.2129\n",
            "                    Mean surrogate loss: -0.0048\n",
            "                      Mean entropy loss: 2.8465\n",
            "                          Mean rnd loss: 0.0009\n",
            "                     Mean symmetry loss: 0.0050\n",
            "                  Mean extrinsic reward: 50.29\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 50.29\n",
            "                    Mean episode length: 50.29\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 26/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5184 \n",
            "                       Steps per second: 658 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.216s \n",
            "                        Mean value loss: 48.3202\n",
            "                    Mean surrogate loss: -0.0011\n",
            "                      Mean entropy loss: 2.8457\n",
            "                          Mean rnd loss: 0.0010\n",
            "                     Mean symmetry loss: 0.0040\n",
            "                  Mean extrinsic reward: 52.49\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 52.49\n",
            "                    Mean episode length: 52.49\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 27/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5376 \n",
            "                       Steps per second: 688 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.202s \n",
            "                        Mean value loss: 38.6989\n",
            "                    Mean surrogate loss: 0.0126\n",
            "                      Mean entropy loss: 2.8459\n",
            "                          Mean rnd loss: 0.0010\n",
            "                     Mean symmetry loss: 0.0068\n",
            "                  Mean extrinsic reward: 52.49\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 52.49\n",
            "                    Mean episode length: 52.49\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 28/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5568 \n",
            "                       Steps per second: 736 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.185s \n",
            "                        Mean value loss: 44.3507\n",
            "                    Mean surrogate loss: 0.0011\n",
            "                      Mean entropy loss: 2.8466\n",
            "                          Mean rnd loss: 0.0009\n",
            "                     Mean symmetry loss: 0.0058\n",
            "                  Mean extrinsic reward: 54.91\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 54.91\n",
            "                    Mean episode length: 54.91\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 29/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5760 \n",
            "                       Steps per second: 726 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 42.1423\n",
            "                    Mean surrogate loss: 0.0182\n",
            "                      Mean entropy loss: 2.8466\n",
            "                          Mean rnd loss: 0.0008\n",
            "                     Mean symmetry loss: 0.0060\n",
            "                  Mean extrinsic reward: 56.18\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 56.18\n",
            "                    Mean episode length: 56.18\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 30/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5952 \n",
            "                       Steps per second: 740 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.183s \n",
            "                        Mean value loss: 39.7674\n",
            "                    Mean surrogate loss: 0.0061\n",
            "                      Mean entropy loss: 2.8460\n",
            "                          Mean rnd loss: 0.0013\n",
            "                     Mean symmetry loss: 0.0041\n",
            "                  Mean extrinsic reward: 56.18\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 56.18\n",
            "                    Mean episode length: 56.18\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 31/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6144 \n",
            "                       Steps per second: 694 \n",
            "                        Collection time: 0.090s \n",
            "                          Learning time: 0.186s \n",
            "                        Mean value loss: 38.4704\n",
            "                    Mean surrogate loss: 0.0042\n",
            "                      Mean entropy loss: 2.8461\n",
            "                          Mean rnd loss: 0.0015\n",
            "                     Mean symmetry loss: 0.0040\n",
            "                  Mean extrinsic reward: 56.18\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 56.18\n",
            "                    Mean episode length: 56.18\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 32/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6336 \n",
            "                       Steps per second: 692 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.198s \n",
            "                        Mean value loss: 36.9048\n",
            "                    Mean surrogate loss: 0.0013\n",
            "                      Mean entropy loss: 2.8467\n",
            "                          Mean rnd loss: 0.0009\n",
            "                     Mean symmetry loss: 0.0041\n",
            "                  Mean extrinsic reward: 56.18\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 56.18\n",
            "                    Mean episode length: 56.18\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 33/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6528 \n",
            "                       Steps per second: 735 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 51.4245\n",
            "                    Mean surrogate loss: 0.0016\n",
            "                      Mean entropy loss: 2.8482\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: 0.0028\n",
            "                  Mean extrinsic reward: 57.98\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 57.98\n",
            "                    Mean episode length: 57.98\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 34/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6720 \n",
            "                       Steps per second: 713 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.192s \n",
            "                        Mean value loss: 32.2393\n",
            "                    Mean surrogate loss: -0.0077\n",
            "                      Mean entropy loss: 2.8491\n",
            "                          Mean rnd loss: 0.0007\n",
            "                     Mean symmetry loss: 0.0065\n",
            "                  Mean extrinsic reward: 57.98\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 57.98\n",
            "                    Mean episode length: 57.98\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 35/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6912 \n",
            "                       Steps per second: 522 \n",
            "                        Collection time: 0.086s \n",
            "                          Learning time: 0.281s \n",
            "                        Mean value loss: 30.0190\n",
            "                    Mean surrogate loss: -0.0095\n",
            "                      Mean entropy loss: 2.8493\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0083\n",
            "                  Mean extrinsic reward: 57.98\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 57.98\n",
            "                    Mean episode length: 57.98\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.37s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 36/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7104 \n",
            "                       Steps per second: 519 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.271s \n",
            "                        Mean value loss: 31.8976\n",
            "                    Mean surrogate loss: 0.0330\n",
            "                      Mean entropy loss: 2.8479\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: 0.0236\n",
            "                  Mean extrinsic reward: 59.39\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 59.39\n",
            "                    Mean episode length: 59.39\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.37s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 37/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7296 \n",
            "                       Steps per second: 474 \n",
            "                        Collection time: 0.108s \n",
            "                          Learning time: 0.297s \n",
            "                        Mean value loss: 29.9087\n",
            "                    Mean surrogate loss: 0.0014\n",
            "                      Mean entropy loss: 2.8477\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0372\n",
            "                  Mean extrinsic reward: 59.39\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 59.39\n",
            "                    Mean episode length: 59.39\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.40s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 38/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7488 \n",
            "                       Steps per second: 524 \n",
            "                        Collection time: 0.108s \n",
            "                          Learning time: 0.258s \n",
            "                        Mean value loss: 61.1418\n",
            "                    Mean surrogate loss: 0.0081\n",
            "                      Mean entropy loss: 2.8477\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0261\n",
            "                  Mean extrinsic reward: 62.35\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 62.35\n",
            "                    Mean episode length: 62.35\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.37s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 39/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7680 \n",
            "                       Steps per second: 560 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.248s \n",
            "                        Mean value loss: 22.1932\n",
            "                    Mean surrogate loss: 0.0037\n",
            "                      Mean entropy loss: 2.8476\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0111\n",
            "                  Mean extrinsic reward: 62.35\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 62.35\n",
            "                    Mean episode length: 62.35\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.34s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 40/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7872 \n",
            "                       Steps per second: 499 \n",
            "                        Collection time: 0.111s \n",
            "                          Learning time: 0.273s \n",
            "                        Mean value loss: 24.1128\n",
            "                    Mean surrogate loss: 0.0141\n",
            "                      Mean entropy loss: 2.8474\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0119\n",
            "                  Mean extrinsic reward: 66.24\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 66.24\n",
            "                    Mean episode length: 66.24\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.38s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 41/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8064 \n",
            "                       Steps per second: 481 \n",
            "                        Collection time: 0.114s \n",
            "                          Learning time: 0.285s \n",
            "                        Mean value loss: 30.7399\n",
            "                    Mean surrogate loss: 0.0093\n",
            "                      Mean entropy loss: 2.8472\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0122\n",
            "                  Mean extrinsic reward: 68.77\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 68.77\n",
            "                    Mean episode length: 68.77\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.40s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 42/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8256 \n",
            "                       Steps per second: 468 \n",
            "                        Collection time: 0.092s \n",
            "                          Learning time: 0.318s \n",
            "                        Mean value loss: 18.1727\n",
            "                    Mean surrogate loss: -0.0071\n",
            "                      Mean entropy loss: 2.8472\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: 0.0031\n",
            "                  Mean extrinsic reward: 68.77\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 68.77\n",
            "                    Mean episode length: 68.77\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.41s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 43/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8448 \n",
            "                       Steps per second: 678 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.209s \n",
            "                        Mean value loss: 15.8927\n",
            "                    Mean surrogate loss: 0.0188\n",
            "                      Mean entropy loss: 2.8475\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0061\n",
            "                  Mean extrinsic reward: 68.77\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 68.77\n",
            "                    Mean episode length: 68.77\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 44/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8640 \n",
            "                       Steps per second: 476 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.325s \n",
            "                        Mean value loss: 12.9961\n",
            "                    Mean surrogate loss: 0.0017\n",
            "                      Mean entropy loss: 2.8474\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: 0.0064\n",
            "                  Mean extrinsic reward: 68.77\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 68.77\n",
            "                    Mean episode length: 68.77\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.40s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 45/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8832 \n",
            "                       Steps per second: 691 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.200s \n",
            "                        Mean value loss: 59.9233\n",
            "                    Mean surrogate loss: -0.0068\n",
            "                      Mean entropy loss: 2.8473\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0036\n",
            "                  Mean extrinsic reward: 72.89\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 72.89\n",
            "                    Mean episode length: 72.89\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 46/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9024 \n",
            "                       Steps per second: 444 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.355s \n",
            "                        Mean value loss: 23.6424\n",
            "                    Mean surrogate loss: 0.0047\n",
            "                      Mean entropy loss: 2.8472\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0295\n",
            "                  Mean extrinsic reward: 72.89\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 72.89\n",
            "                    Mean episode length: 72.89\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.43s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 47/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9216 \n",
            "                       Steps per second: 698 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.198s \n",
            "                        Mean value loss: 18.4980\n",
            "                    Mean surrogate loss: 0.0293\n",
            "                      Mean entropy loss: 2.8465\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: 0.0251\n",
            "                  Mean extrinsic reward: 77.29\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 77.29\n",
            "                    Mean episode length: 77.29\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 48/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9408 \n",
            "                       Steps per second: 710 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 13.3326\n",
            "                    Mean surrogate loss: 0.0062\n",
            "                      Mean entropy loss: 2.8464\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0203\n",
            "                  Mean extrinsic reward: 77.29\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 77.29\n",
            "                    Mean episode length: 77.29\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 49/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9600 \n",
            "                       Steps per second: 728 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 8.1483\n",
            "                    Mean surrogate loss: 0.0008\n",
            "                      Mean entropy loss: 2.8462\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0087\n",
            "                  Mean extrinsic reward: 81.60\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 81.60\n",
            "                    Mean episode length: 81.60\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 50/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9792 \n",
            "                       Steps per second: 700 \n",
            "                        Collection time: 0.089s \n",
            "                          Learning time: 0.185s \n",
            "                        Mean value loss: 18.2500\n",
            "                    Mean surrogate loss: -0.0014\n",
            "                      Mean entropy loss: 2.8470\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0079\n",
            "                  Mean extrinsic reward: 85.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 85.07\n",
            "                    Mean episode length: 85.07\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 51/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9984 \n",
            "                       Steps per second: 734 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 14.2444\n",
            "                    Mean surrogate loss: 0.0010\n",
            "                      Mean entropy loss: 2.8475\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0061\n",
            "                  Mean extrinsic reward: 85.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 85.07\n",
            "                    Mean episode length: 85.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 52/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10176 \n",
            "                       Steps per second: 710 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.195s \n",
            "                        Mean value loss: 13.9228\n",
            "                    Mean surrogate loss: 0.0128\n",
            "                      Mean entropy loss: 2.8504\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0571\n",
            "                  Mean extrinsic reward: 85.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 85.07\n",
            "                    Mean episode length: 85.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 53/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10368 \n",
            "                       Steps per second: 691 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.204s \n",
            "                        Mean value loss: 7.8149\n",
            "                    Mean surrogate loss: 0.0069\n",
            "                      Mean entropy loss: 2.8513\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0200\n",
            "                  Mean extrinsic reward: 85.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 85.07\n",
            "                    Mean episode length: 85.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 54/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10560 \n",
            "                       Steps per second: 706 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.195s \n",
            "                        Mean value loss: 8.9954\n",
            "                    Mean surrogate loss: 0.0061\n",
            "                      Mean entropy loss: 2.8511\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0468\n",
            "                  Mean extrinsic reward: 85.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 85.07\n",
            "                    Mean episode length: 85.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 55/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10752 \n",
            "                       Steps per second: 644 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.220s \n",
            "                        Mean value loss: 10.5328\n",
            "                    Mean surrogate loss: 0.0321\n",
            "                      Mean entropy loss: 2.8513\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: 0.0246\n",
            "                  Mean extrinsic reward: 85.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 85.07\n",
            "                    Mean episode length: 85.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 56/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10944 \n",
            "                       Steps per second: 701 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.195s \n",
            "                        Mean value loss: 9.0298\n",
            "                    Mean surrogate loss: 0.0131\n",
            "                      Mean entropy loss: 2.8513\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: 0.0424\n",
            "                  Mean extrinsic reward: 89.22\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 89.22\n",
            "                    Mean episode length: 89.22\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 57/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11136 \n",
            "                       Steps per second: 667 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.209s \n",
            "                        Mean value loss: 9.8540\n",
            "                    Mean surrogate loss: 0.0217\n",
            "                      Mean entropy loss: 2.8512\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0243\n",
            "                  Mean extrinsic reward: 89.22\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 89.22\n",
            "                    Mean episode length: 89.22\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 58/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11328 \n",
            "                       Steps per second: 705 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.194s \n",
            "                        Mean value loss: 121.6926\n",
            "                    Mean surrogate loss: -0.0013\n",
            "                      Mean entropy loss: 2.8514\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0067\n",
            "                  Mean extrinsic reward: 93.09\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 93.09\n",
            "                    Mean episode length: 93.09\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 59/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11520 \n",
            "                       Steps per second: 708 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.193s \n",
            "                        Mean value loss: 8.5141\n",
            "                    Mean surrogate loss: 0.0198\n",
            "                      Mean entropy loss: 2.8521\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0226\n",
            "                  Mean extrinsic reward: 97.95\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 97.95\n",
            "                    Mean episode length: 97.95\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 60/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11712 \n",
            "                       Steps per second: 620 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.208s \n",
            "                        Mean value loss: 10.6310\n",
            "                    Mean surrogate loss: 0.0086\n",
            "                      Mean entropy loss: 2.8522\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0195\n",
            "                  Mean extrinsic reward: 97.95\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 97.95\n",
            "                    Mean episode length: 97.95\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 61/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11904 \n",
            "                       Steps per second: 680 \n",
            "                        Collection time: 0.089s \n",
            "                          Learning time: 0.193s \n",
            "                        Mean value loss: 13.6825\n",
            "                    Mean surrogate loss: 0.0138\n",
            "                      Mean entropy loss: 2.8516\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0171\n",
            "                  Mean extrinsic reward: 102.80\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 102.80\n",
            "                    Mean episode length: 102.80\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 62/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12096 \n",
            "                       Steps per second: 714 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.193s \n",
            "                        Mean value loss: 212.4773\n",
            "                    Mean surrogate loss: 0.0011\n",
            "                      Mean entropy loss: 2.8511\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0188\n",
            "                  Mean extrinsic reward: 106.69\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 106.69\n",
            "                    Mean episode length: 106.69\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 63/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12288 \n",
            "                       Steps per second: 705 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.193s \n",
            "                        Mean value loss: 17.6345\n",
            "                    Mean surrogate loss: 0.0055\n",
            "                      Mean entropy loss: 2.8505\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0224\n",
            "                  Mean extrinsic reward: 106.69\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 106.69\n",
            "                    Mean episode length: 106.69\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 64/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12480 \n",
            "                       Steps per second: 705 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.197s \n",
            "                        Mean value loss: 33.5410\n",
            "                    Mean surrogate loss: 0.0212\n",
            "                      Mean entropy loss: 2.8518\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0423\n",
            "                  Mean extrinsic reward: 106.69\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 106.69\n",
            "                    Mean episode length: 106.69\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 65/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12672 \n",
            "                       Steps per second: 723 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.189s \n",
            "                        Mean value loss: 2.9970\n",
            "                    Mean surrogate loss: 0.0406\n",
            "                      Mean entropy loss: 2.8523\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0768\n",
            "                  Mean extrinsic reward: 106.69\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 106.69\n",
            "                    Mean episode length: 106.69\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 66/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12864 \n",
            "                       Steps per second: 684 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.206s \n",
            "                        Mean value loss: 2.0220\n",
            "                    Mean surrogate loss: 0.0252\n",
            "                      Mean entropy loss: 2.8524\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0499\n",
            "                  Mean extrinsic reward: 106.69\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 106.69\n",
            "                    Mean episode length: 106.69\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 67/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13056 \n",
            "                       Steps per second: 752 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.179s \n",
            "                        Mean value loss: 3.8392\n",
            "                    Mean surrogate loss: 0.0003\n",
            "                      Mean entropy loss: 2.8529\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0027\n",
            "                  Mean extrinsic reward: 106.69\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 106.69\n",
            "                    Mean episode length: 106.69\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 68/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13248 \n",
            "                       Steps per second: 711 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.193s \n",
            "                        Mean value loss: 3.2389\n",
            "                    Mean surrogate loss: -0.0025\n",
            "                      Mean entropy loss: 2.8537\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0098\n",
            "                  Mean extrinsic reward: 111.51\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 111.51\n",
            "                    Mean episode length: 111.51\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 69/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13440 \n",
            "                       Steps per second: 679 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.200s \n",
            "                        Mean value loss: 2.1702\n",
            "                    Mean surrogate loss: 0.0074\n",
            "                      Mean entropy loss: 2.8536\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0043\n",
            "                  Mean extrinsic reward: 111.51\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 111.51\n",
            "                    Mean episode length: 111.51\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 70/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13632 \n",
            "                       Steps per second: 766 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.178s \n",
            "                        Mean value loss: 2.1496\n",
            "                    Mean surrogate loss: 0.0047\n",
            "                      Mean entropy loss: 2.8532\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0056\n",
            "                  Mean extrinsic reward: 116.31\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 116.31\n",
            "                    Mean episode length: 116.31\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 71/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13824 \n",
            "                       Steps per second: 736 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 2.8738\n",
            "                    Mean surrogate loss: -0.0068\n",
            "                      Mean entropy loss: 2.8529\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0012\n",
            "                  Mean extrinsic reward: 121.04\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 121.04\n",
            "                    Mean episode length: 121.04\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 72/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14016 \n",
            "                       Steps per second: 635 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.225s \n",
            "                        Mean value loss: 2.4664\n",
            "                    Mean surrogate loss: -0.0027\n",
            "                      Mean entropy loss: 2.8532\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0029\n",
            "                  Mean extrinsic reward: 121.04\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 121.04\n",
            "                    Mean episode length: 121.04\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 73/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14208 \n",
            "                       Steps per second: 707 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.197s \n",
            "                        Mean value loss: 2.1715\n",
            "                    Mean surrogate loss: -0.0067\n",
            "                      Mean entropy loss: 2.8551\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0164\n",
            "                  Mean extrinsic reward: 121.04\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 121.04\n",
            "                    Mean episode length: 121.04\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 74/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14400 \n",
            "                       Steps per second: 650 \n",
            "                        Collection time: 0.091s \n",
            "                          Learning time: 0.204s \n",
            "                        Mean value loss: 17.2932\n",
            "                    Mean surrogate loss: 0.0002\n",
            "                      Mean entropy loss: 2.8558\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0060\n",
            "                  Mean extrinsic reward: 124.80\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 124.80\n",
            "                    Mean episode length: 124.80\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 75/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14592 \n",
            "                       Steps per second: 722 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.184s \n",
            "                        Mean value loss: 2.7937\n",
            "                    Mean surrogate loss: -0.0009\n",
            "                      Mean entropy loss: 2.8556\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0024\n",
            "                  Mean extrinsic reward: 124.80\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 124.80\n",
            "                    Mean episode length: 124.80\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 76/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14784 \n",
            "                       Steps per second: 555 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.264s \n",
            "                        Mean value loss: 3.2049\n",
            "                    Mean surrogate loss: 0.0015\n",
            "                      Mean entropy loss: 2.8547\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0511\n",
            "                  Mean extrinsic reward: 124.80\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 124.80\n",
            "                    Mean episode length: 124.80\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.35s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 77/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14976 \n",
            "                       Steps per second: 552 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.251s \n",
            "                        Mean value loss: 2.9734\n",
            "                    Mean surrogate loss: -0.0061\n",
            "                      Mean entropy loss: 2.8546\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0224\n",
            "                  Mean extrinsic reward: 129.57\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 129.57\n",
            "                    Mean episode length: 129.57\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.35s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 78/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15168 \n",
            "                       Steps per second: 560 \n",
            "                        Collection time: 0.092s \n",
            "                          Learning time: 0.250s \n",
            "                        Mean value loss: 8.2497\n",
            "                    Mean surrogate loss: 0.0049\n",
            "                      Mean entropy loss: 2.8546\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0183\n",
            "                  Mean extrinsic reward: 129.57\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 129.57\n",
            "                    Mean episode length: 129.57\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.34s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 79/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15360 \n",
            "                       Steps per second: 538 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.257s \n",
            "                        Mean value loss: 272.8763\n",
            "                    Mean surrogate loss: 0.0037\n",
            "                      Mean entropy loss: 2.8546\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0133\n",
            "                  Mean extrinsic reward: 142.26\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 142.26\n",
            "                    Mean episode length: 142.26\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.36s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 80/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15552 \n",
            "                       Steps per second: 594 \n",
            "                        Collection time: 0.089s \n",
            "                          Learning time: 0.234s \n",
            "                        Mean value loss: 1.3065\n",
            "                    Mean surrogate loss: 0.0084\n",
            "                      Mean entropy loss: 2.8550\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0043\n",
            "                  Mean extrinsic reward: 142.26\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 142.26\n",
            "                    Mean episode length: 142.26\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.32s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 81/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15744 \n",
            "                       Steps per second: 527 \n",
            "                        Collection time: 0.107s \n",
            "                          Learning time: 0.257s \n",
            "                        Mean value loss: 337.1351\n",
            "                    Mean surrogate loss: -0.0221\n",
            "                      Mean entropy loss: 2.8579\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0112\n",
            "                  Mean extrinsic reward: 144.93\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 144.93\n",
            "                    Mean episode length: 144.93\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.36s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 82/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15936 \n",
            "                       Steps per second: 455 \n",
            "                        Collection time: 0.116s \n",
            "                          Learning time: 0.305s \n",
            "                        Mean value loss: 1.6846\n",
            "                    Mean surrogate loss: 0.0105\n",
            "                      Mean entropy loss: 2.8592\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0125\n",
            "                  Mean extrinsic reward: 144.93\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 144.93\n",
            "                    Mean episode length: 144.93\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.42s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 83/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16128 \n",
            "                       Steps per second: 668 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.181s \n",
            "                        Mean value loss: 2.0196\n",
            "                    Mean surrogate loss: -0.0100\n",
            "                      Mean entropy loss: 2.8600\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0106\n",
            "                  Mean extrinsic reward: 144.93\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 144.93\n",
            "                    Mean episode length: 144.93\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 84/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16320 \n",
            "                       Steps per second: 759 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.177s \n",
            "                        Mean value loss: 10.1642\n",
            "                    Mean surrogate loss: -0.0082\n",
            "                      Mean entropy loss: 2.8583\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0070\n",
            "                  Mean extrinsic reward: 144.93\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 144.93\n",
            "                    Mean episode length: 144.93\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 85/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16512 \n",
            "                       Steps per second: 702 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.199s \n",
            "                        Mean value loss: 5.9729\n",
            "                    Mean surrogate loss: -0.0025\n",
            "                      Mean entropy loss: 2.8582\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0076\n",
            "                  Mean extrinsic reward: 144.93\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 144.93\n",
            "                    Mean episode length: 144.93\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 86/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16704 \n",
            "                       Steps per second: 750 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.180s \n",
            "                        Mean value loss: 2.3633\n",
            "                    Mean surrogate loss: 0.0018\n",
            "                      Mean entropy loss: 2.8581\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0100\n",
            "                  Mean extrinsic reward: 144.93\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 144.93\n",
            "                    Mean episode length: 144.93\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 87/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16896 \n",
            "                       Steps per second: 774 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.176s \n",
            "                        Mean value loss: 2.4659\n",
            "                    Mean surrogate loss: 0.0065\n",
            "                      Mean entropy loss: 2.8578\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0169\n",
            "                  Mean extrinsic reward: 144.93\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 144.93\n",
            "                    Mean episode length: 144.93\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 88/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17088 \n",
            "                       Steps per second: 755 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.181s \n",
            "                        Mean value loss: 10.2656\n",
            "                    Mean surrogate loss: 0.0087\n",
            "                      Mean entropy loss: 2.8572\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.1675\n",
            "                  Mean extrinsic reward: 147.33\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 147.33\n",
            "                    Mean episode length: 147.33\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 89/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17280 \n",
            "                       Steps per second: 712 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.195s \n",
            "                        Mean value loss: 0.6870\n",
            "                    Mean surrogate loss: 0.0309\n",
            "                      Mean entropy loss: 2.8572\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0804\n",
            "                  Mean extrinsic reward: 152.03\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 152.03\n",
            "                    Mean episode length: 152.03\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 90/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17472 \n",
            "                       Steps per second: 726 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.191s \n",
            "                        Mean value loss: 0.6569\n",
            "                    Mean surrogate loss: -0.0143\n",
            "                      Mean entropy loss: 2.8576\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0053\n",
            "                  Mean extrinsic reward: 152.03\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 152.03\n",
            "                    Mean episode length: 152.03\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 91/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17664 \n",
            "                       Steps per second: 710 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.195s \n",
            "                        Mean value loss: 0.7096\n",
            "                    Mean surrogate loss: 0.0069\n",
            "                      Mean entropy loss: 2.8580\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0111\n",
            "                  Mean extrinsic reward: 152.03\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 152.03\n",
            "                    Mean episode length: 152.03\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 92/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17856 \n",
            "                       Steps per second: 731 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 0.9352\n",
            "                    Mean surrogate loss: 0.0017\n",
            "                      Mean entropy loss: 2.8574\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0057\n",
            "                  Mean extrinsic reward: 156.57\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 156.57\n",
            "                    Mean episode length: 156.57\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 93/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18048 \n",
            "                       Steps per second: 712 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.196s \n",
            "                        Mean value loss: 2.2332\n",
            "                    Mean surrogate loss: -0.0066\n",
            "                      Mean entropy loss: 2.8563\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0049\n",
            "                  Mean extrinsic reward: 156.57\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 156.57\n",
            "                    Mean episode length: 156.57\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 94/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18240 \n",
            "                       Steps per second: 719 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.186s \n",
            "                        Mean value loss: 87.6498\n",
            "                    Mean surrogate loss: -0.0130\n",
            "                      Mean entropy loss: 2.8564\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0253\n",
            "                  Mean extrinsic reward: 161.18\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 161.18\n",
            "                    Mean episode length: 161.18\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 95/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18432 \n",
            "                       Steps per second: 763 \n",
            "                        Collection time: 0.072s \n",
            "                          Learning time: 0.179s \n",
            "                        Mean value loss: 1.2274\n",
            "                    Mean surrogate loss: 0.0157\n",
            "                      Mean entropy loss: 2.8564\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0147\n",
            "                  Mean extrinsic reward: 161.18\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 161.18\n",
            "                    Mean episode length: 161.18\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 96/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18624 \n",
            "                       Steps per second: 749 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.181s \n",
            "                        Mean value loss: 150.2399\n",
            "                    Mean surrogate loss: -0.0095\n",
            "                      Mean entropy loss: 2.8565\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0283\n",
            "                  Mean extrinsic reward: 165.03\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 165.03\n",
            "                    Mean episode length: 165.03\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 97/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18816 \n",
            "                       Steps per second: 711 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.196s \n",
            "                        Mean value loss: 1.2925\n",
            "                    Mean surrogate loss: 0.0162\n",
            "                      Mean entropy loss: 2.8567\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0304\n",
            "                  Mean extrinsic reward: 165.03\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 165.03\n",
            "                    Mean episode length: 165.03\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 98/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 19008 \n",
            "                       Steps per second: 756 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.180s \n",
            "                        Mean value loss: 0.8126\n",
            "                    Mean surrogate loss: -0.0122\n",
            "                      Mean entropy loss: 2.8570\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0196\n",
            "                  Mean extrinsic reward: 169.29\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 169.29\n",
            "                    Mean episode length: 169.29\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 99/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 19200 \n",
            "                       Steps per second: 719 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.192s \n",
            "                        Mean value loss: 0.4993\n",
            "                    Mean surrogate loss: 0.0085\n",
            "                      Mean entropy loss: 2.8573\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0078\n",
            "                  Mean extrinsic reward: 174.19\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 174.19\n",
            "                    Mean episode length: 174.19\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:30\n",
            "                                    ETA: 00:00:00\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5LlVSVlNy9v"
      },
      "source": [
        "# Symmetry Generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "p1UNAqTxN5Uf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as distributions\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import gc\n",
        "\n",
        "import math\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "e1T2fda6N6Z1"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def frobenius_normalize(A):\n",
        "    return A / A.norm(p='fro')\n",
        "\n",
        "def exp_approx(x, order=10):\n",
        "    \"\"\"Tensor exponential using Taylor series approximation\"\"\"\n",
        "    result = 0\n",
        "    term = torch.eye(n=x.shape[0], device=x.device)\n",
        "    result = result + term\n",
        "    for i in range(1, order+1):\n",
        "        term = torch.mm(term, x)/i\n",
        "        result = result + term\n",
        "    return result\n",
        "\n",
        "class GeneratorLatent(nn.Module):\n",
        "    \"\"\"Single symmetry generator\"\"\"\n",
        "\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.algebra = nn.Parameter(torch.empty((num_features, num_features)))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        nn.init.kaiming_uniform_(self.algebra, a=math.sqrt(5))\n",
        "\n",
        "class GroupLatent(nn.Module):\n",
        "    def __init__(self,num_features, num_generators, LOSS_MODE = \"MAE\"): ## MAE works better than MSE but takes longer to converge and gives sparser generators\n",
        "        super().__init__()\n",
        "        self.num_generators = num_generators\n",
        "        self.LOSS_MODE = LOSS_MODE\n",
        "        self.num_features = num_features\n",
        "        self.group = nn.ModuleList([GeneratorLatent(self.num_features) for i in range(self.num_generators)])\n",
        "        self.reset_parameters()\n",
        "        self.criterion_cos = nn.CosineSimilarity(dim=0)\n",
        "        self.eps = 1e-5\n",
        "\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        for generator in self.group:\n",
        "            generator.reset_parameters()\n",
        "\n",
        "    def forward(self, theta, x, order=10):\n",
        "        \"\"\"Apply group transformations\"\"\"\n",
        "        transformed = x.clone()\n",
        "        for i, generator in enumerate(self.group):\n",
        "            inter = 0\n",
        "            term = torch.eye(x.shape[-1], device=x.device)\n",
        "            inter = inter + term.expand(x.shape[0], -1, -1)\n",
        "            for k in range(1, order+1, 1):\n",
        "                THETA = ((theta[i])**k)[:, None, None].expand(x.shape[0], x.shape[-1], x.shape[-1])\n",
        "                term = (term@generator.algebra)/k\n",
        "                inter = inter + THETA*term.expand(x.shape[0], -1, -1)\n",
        "\n",
        "            transformed = torch.bmm(inter, transformed[:, :, None]).squeeze()\n",
        "        return transformed\n",
        "\n",
        "    def orthogonal_loss_kon(self):\n",
        "        \"\"\"Orthogonality loss between generators\"\"\"\n",
        "        loss = 0\n",
        "        for i, generator1 in enumerate(self.group):\n",
        "            for j, generator2 in enumerate(self.group):\n",
        "                if i < j:\n",
        "                    mat1_norm = frobenius_normalize(generator1.algebra)\n",
        "                    mat2_norm = frobenius_normalize(generator2.algebra)\n",
        "                    loss = loss + torch.sum((mat1_norm * mat2_norm)**2)\n",
        "        return loss\n",
        "\n",
        "def generate_latent_data_from_trained_agent(trained_agent, env_name='CartPole-v1',\n",
        "                                           num_samples=100000, device='cpu'):\n",
        "    \"\"\"\n",
        "    Generate latent representations from trained PPO agent\n",
        "    This is the bridge between RL and symmetry detection\n",
        "    \"\"\"\n",
        "    print(\"=== Generating Latent Data from Trained Agent ===\")\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    latent_data = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(num_samples), desc=\"Collecting latent data\"):\n",
        "            # Reset environment periodically\n",
        "            if len(latent_data) % 1000 == 0:\n",
        "                state, _ = env.reset()\n",
        "\n",
        "            # Convert state to tensor\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "            # Get latent representation from trained agent\n",
        "            latent = trained_agent.get_latent(state_tensor)\n",
        "            latent_data.append(latent.squeeze().cpu())\n",
        "\n",
        "            # Take action to get new state\n",
        "            action, _, _, _ = trained_agent.get_action(state_tensor)\n",
        "            state, _, done, _, _ = env.step(action)\n",
        "\n",
        "            if done:\n",
        "                state, _ = env.reset()\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # Convert to tensor\n",
        "    latent_tensor = torch.stack(latent_data)\n",
        "    print(f\"Generated {latent_tensor.shape[0]} latent samples of dimension {latent_tensor.shape[1]}\")\n",
        "\n",
        "    return latent_tensor\n",
        "\n",
        "def psi_4d(x):\n",
        "    \"\"\"Invariant function for 4D latent space\"\"\"\n",
        "    M = torch.eye(4, device=x.device)\n",
        "    return torch.sum((x @ M) * x, dim=1)\n",
        "\n",
        "\n",
        "def train_symmetry_generators(model_symmetry, latent_data, device='cpu',\n",
        "                              epochs=50, batch_size=1024, M=None):\n",
        "    \"\"\"\n",
        "    PHASE 2: Train symmetry generators on fixed latent data\n",
        "    No RL training here - pure symmetry learning\n",
        "    \"\"\"\n",
        "    print(\"=== PHASE 2: Training Symmetry Generators ===\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(model_symmetry.parameters(), lr=1e-2)\n",
        "    criterion_mse = nn.MSELoss()\n",
        "    orth_coeff = 0.5\n",
        "    skew_sym_coeff = 0.1 # Coefficient for skew-symmetry regularization\n",
        "    magnitude_coeff = 0.01 # Coefficient for magnitude regularization\n",
        "\n",
        "    # Convert latent_data to a PyTorch tensor if it's a NumPy array\n",
        "    if isinstance(latent_data, np.ndarray):\n",
        "        latent_data = torch.from_numpy(latent_data).float().to(device)\n",
        "    else:\n",
        "        latent_data = latent_data.to(device)\n",
        "\n",
        "    # Use the attribute present in the provided object\n",
        "    num_generators = getattr(model_symmetry, \"num_generators\", None)\n",
        "    if num_generators is None:\n",
        "        num_generators = getattr(model_symmetry, \"numgenerators\")  # fallback to notebook's attribute name\n",
        "\n",
        "    loss_closure_history = []\n",
        "    loss_orth_history = []\n",
        "    loss_skew_sym_history = []\n",
        "    loss_magnitude_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        loss_closure_total = 0\n",
        "        loss_orth_total = 0\n",
        "        loss_skew_sym_total = 0\n",
        "        loss_magnitude_total = 0\n",
        "\n",
        "        # Shuffle data\n",
        "        indices = torch.randperm(latent_data.shape[0])\n",
        "        num_batches = (latent_data.shape[0] // batch_size) + 1\n",
        "\n",
        "        for i in tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            batch_indices = indices[i*batch_size:(i+1)*batch_size]\n",
        "            if len(batch_indices) == 0:\n",
        "                continue\n",
        "\n",
        "            z = latent_data[batch_indices]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Sample random transformation parameters\n",
        "            theta = [(2*torch.rand(z.shape[0], device=device) - 1) for _ in range(num_generators)]\n",
        "\n",
        "            # Apply symmetry transformations\n",
        "            z_prime = model_symmetry(theta=theta, x=z)\n",
        "\n",
        "            # Compute losses\n",
        "            closure_loss = criterion_mse(psi_4d(z), psi_4d(z_prime))\n",
        "            orthogonal_loss = model_symmetry.orthogonal_loss_kon()\n",
        "\n",
        "            # Add skew-symmetry and magnitude regularization\n",
        "            skew_symmetry_loss = 0\n",
        "            magnitude_loss = 0\n",
        "            for generator in model_symmetry.group:\n",
        "                G = generator.algebra\n",
        "                skew_symmetry_loss += torch.sum((G + G.T)**2) # Penalize non-skew-symmetry\n",
        "                magnitude_loss += torch.norm(G, p='fro')**2 # Penalize small generators\n",
        "\n",
        "            # Total loss\n",
        "            loss_S = closure_loss + orth_coeff * orthogonal_loss + \\\n",
        "                     skew_sym_coeff * skew_symmetry_loss + magnitude_coeff * magnitude_loss\n",
        "            loss_S.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_closure_total += closure_loss.item()\n",
        "            loss_orth_total += orthogonal_loss.item()\n",
        "            loss_skew_sym_total += skew_symmetry_loss.item()\n",
        "            loss_magnitude_total += magnitude_loss.item()\n",
        "\n",
        "        # Average losses\n",
        "        avg_closure = loss_closure_total / num_batches\n",
        "        avg_orth = loss_orth_total / num_batches\n",
        "        avg_skew_sym = loss_skew_sym_total / num_batches\n",
        "        avg_magnitude = loss_magnitude_total / num_batches\n",
        "\n",
        "        loss_closure_history.append(avg_closure)\n",
        "        loss_orth_history.append(avg_orth)\n",
        "        loss_skew_sym_history.append(avg_skew_sym)\n",
        "        loss_magnitude_history.append(avg_magnitude)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} - Closure: {avg_closure:.6f}, Orthogonality: {avg_orth:.6f}, Skew-Sym: {avg_skew_sym:.6f}, Magnitude: {avg_magnitude:.6f}\")\n",
        "\n",
        "        # Memory cleanup\n",
        "        if device.startswith('cuda'):\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # ===== Example usage after training (uses Taylor-series expapprox) =====\n",
        "    # expapprox(x, order=10) must be defined in the notebook as the Taylor-series matrix exponential\n",
        "    # z′ = (Π_i exp(θ_i G_i)) z, computed per-sample\n",
        "    print(\"\\n=== EXAMPLE USAGE AFTER TRAINING ===\")\n",
        "    with torch.no_grad():\n",
        "        # Small evaluation batch\n",
        "        B = min(4, latent_data.shape[0])\n",
        "        z_eval = latent_data[:B]  # (B, D)\n",
        "        D = z_eval.shape[1]\n",
        "\n",
        "        # Sample θ per generator per sample (same distribution as training)\n",
        "        theta_eval = [2*torch.rand(B, device=device) - 1 for _ in range(num_generators)]\n",
        "\n",
        "        # Build per-sample product Π_i exp(θ_i G_i) using expapprox\n",
        "        product = torch.eye(D, device=device).unsqueeze(0).repeat(B, 1, 1)  # (B, D, D)\n",
        "        for i, gen in enumerate(model_symmetry.group):\n",
        "            G = gen.algebra.detach().to(device)  # (D, D)\n",
        "            Mi_batch = torch.stack([exp_approx(theta_eval[i][j] * G, order=10) for j in range(B)], dim=0)  # (B, D, D)\n",
        "            product = torch.bmm(Mi_batch, product)  # left-multiply to match forward order\n",
        "\n",
        "        # Apply the product to z to get z′\n",
        "        z_prime_eval_from_product = torch.bmm(product, z_eval.unsqueeze(-1)).squeeze(-1)  # (B, D)\n",
        "\n",
        "        # Also compute z′ via the model's forward to cross-check\n",
        "        z_prime_eval_model = model_symmetry(theta=theta_eval, x=z_eval)\n",
        "\n",
        "        # Print results\n",
        "        print(\"Original z (eval batch):\")\n",
        "        print(z_eval)\n",
        "        print(\"\\nPer-sample product Π exp(θ_i G_i) with Taylor approximation (B, D, D):\")\n",
        "        print(product)\n",
        "        print(\"\\nz′ from product = (Π exp(θ_i G_i)) @ z:\")\n",
        "        print(z_prime_eval_from_product)\n",
        "        print(\"\\nz′ from model forward (for cross-check):\")\n",
        "        print(z_prime_eval_model)\n",
        "\n",
        "        # Optional: quick consistency check (mean absolute diff)\n",
        "        mad = (z_prime_eval_from_product - z_prime_eval_model).abs().mean().item()\n",
        "        print(f\"\\nMean |z′(product) - z′(model)| = {mad:.6e}\")\n",
        "\n",
        "    return loss_closure_history, loss_orth_history\n",
        "\n",
        "def plot_generators(model_symmetry, title=\"SO(4) CartPole Symmetry Generators\"):\n",
        "    \"\"\"Visualize learned generators\"\"\"\n",
        "    generators = [g.algebra.detach().cpu().numpy() for g in model_symmetry.group]\n",
        "    G = len(generators)\n",
        "\n",
        "    n_cols = 9\n",
        "    n_rows = (G + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(3.5 * n_cols, 3.5 * n_rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i in range(G):\n",
        "        vmax = np.max(np.abs(generators[i]))\n",
        "        im = axes[i].imshow(generators[i], cmap='bwr', interpolation='nearest',\n",
        "                           vmin=-vmax, vmax=vmax)\n",
        "        axes[i].set_title(f'Generator {i}', fontsize=10)\n",
        "        axes[i].axis('off')\n",
        "        fig.colorbar(im, ax=axes[i], shrink=0.7)\n",
        "\n",
        "    for j in range(G, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    fig.suptitle(title, fontsize=20)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "def analyze_generators(model_symmetry):\n",
        "    \"\"\"Analyze learned generators\"\"\"\n",
        "    print(\"=== Generator Analysis ===\")\n",
        "\n",
        "    generators = [g.algebra.detach().cpu().numpy() for g in model_symmetry.group]\n",
        "\n",
        "    for i, gen in enumerate(generators):\n",
        "        print(f\"\\nGenerator {i}:\")\n",
        "        print(f\"  Shape: {gen.shape}\")\n",
        "        print(f\"  Min: {gen.min():.4f}, Max: {gen.max():.4f}\")\n",
        "        print(f\"  Mean: {gen.mean():.6f}, Std: {gen.std():.4f}\")\n",
        "        print(f\"  Frobenius norm: {np.linalg.norm(gen, 'fro'):.4f}\")\n",
        "\n",
        "        # Check if skew-symmetric (Lie algebra property)\n",
        "        skew_symmetric = np.allclose(gen, -gen.T, atol=1e-3)\n",
        "        print(f\"  Skew-symmetric: {skew_symmetric}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "av4WHz2POHY4"
      },
      "outputs": [],
      "source": [
        "device=\"cuda:0\"\n",
        "M = torch.eye(4, device=\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tmhDm8WlOK9A",
        "outputId": "952c8dd9-a57d-4b80-daa3-da8a3593fe2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Successfully uninstalled gym-0.25.2\n",
            "Collecting numpy<2.0\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Collecting stable_baselines3[extra]\n",
            "  Downloading stable_baselines3-2.7.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (2.37.2)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (25.0)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.12/dist-packages (from stable_baselines3[extra]) (2.9.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable_baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable_baselines3[extra]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from stable_baselines3[extra]) (4.12.0.88)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.12/dist-packages (from stable_baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.12/dist-packages (from stable_baselines3[extra]) (2.19.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from stable_baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stable_baselines3[extra]) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from stable_baselines3[extra]) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from stable_baselines3[extra]) (0.11.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from stable_baselines3[extra]) (11.3.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.12/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.13.0)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[mujoco])\n",
            "  Downloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.10)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3[extra]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3[extra]) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3[extra]) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3[extra]) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3[extra]) (2.9.0.post0)\n",
            "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-python (from stable_baselines3[extra])\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable_baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable_baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->stable_baselines3[extra]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->stable_baselines3[extra]) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->stable_baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable_baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable_baselines3[extra]) (3.0.3)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.23.0)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mujoco-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stable_baselines3-2.7.1-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.0/188.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, numpy, opencv-python, mujoco, stable_baselines3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.12.0.88\n",
            "    Uninstalling opencv-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-4.12.0.88\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, which is not installed.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed glfw-2.10.0 mujoco-3.4.0 numpy-1.26.4 opencv-python-4.11.0.86 stable_baselines3-2.7.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "183e44e6e59744c2ae316e650446a3da",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#%pip uninstall -y gym\n",
        "#%pip install \"gymnasium[mujoco]\" \"stable_baselines3[extra]\" \"numpy<2.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nLY74fMOSqP",
        "outputId": "9c12c752-6941-43ca-96f8-ad6a864882ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_improved_cartpole_dataset(num_episodes=5000, max_steps=300, device='cpu'):\n",
        "    \"\"\"\n",
        "    Generate larger, more diverse symmetry-aware CartPole dataset.\n",
        "    Returns ONLY the final normalized dataset tensor.\n",
        "    \"\"\"\n",
        "    print(\"Generating Improved Symmetry-Aware CartPole Dataset...\")\n",
        "\n",
        "    # Reproducibility\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    env = gym.make('CartPole-v1', render_mode=None)\n",
        "    all_states = []\n",
        "\n",
        "    for _ in tqdm(range(num_episodes), desc=\"Episodes\"):\n",
        "        reset_result = env.reset()\n",
        "        state = reset_result[0] if isinstance(reset_result, tuple) else reset_result\n",
        "        episode_states = []\n",
        "\n",
        "        for _ in range(max_steps):\n",
        "            # --- Diverse actions: mix of policy and random ---\n",
        "            # If angle (index 2) is significant (> 0.1 rad), use policy to stabilize\n",
        "            if np.abs(state[2]) > 0.1:\n",
        "                action = 1 if state[2] > 0 else 0\n",
        "            else:\n",
        "                # Otherwise random exploration\n",
        "                action = env.action_space.sample()\n",
        "\n",
        "            step_result = env.step(action)\n",
        "            next_state = step_result[0]\n",
        "            terminated = step_result[2]\n",
        "            truncated = step_result[3] if len(step_result) > 4 else False\n",
        "\n",
        "            episode_states.append(state.copy())\n",
        "            state = next_state\n",
        "\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        all_states.extend(episode_states)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # --- Create symmetric dataset (original + reflected) ---\n",
        "    states = np.array(all_states)\n",
        "    reflected = states.copy()\n",
        "    reflected[:, [0, 1]] *= -1  # Reflect position and velocity\n",
        "\n",
        "    full_dataset = np.concatenate([states, reflected], axis=0)\n",
        "\n",
        "\n",
        "\n",
        "    # Save full details to disk (so you don't lose mean/std for un-normalization later)\n",
        "    torch.save({\n",
        "        'raw_data': full_dataset\n",
        "    }, 'improved_cartpole_symmetry.pt')\n",
        "\n",
        "    print(f\"Saved dataset to disk. Total samples: {len(full_dataset)}\")\n",
        "\n",
        "    # Return ONLY the final dataset as requested\n",
        "    return full_dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "BNFaIbJxUCVu"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pv5LPCbhOi8h",
        "outputId": "4d9cfe18-2db0-46d7-acdc-60ae705afd3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Improved Symmetry-Aware CartPole Dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episodes: 100%|██████████| 5000/5000 [00:04<00:00, 1201.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved dataset to disk. Total samples: 426704\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "latent_data = generate_improved_cartpole_dataset(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UH6d17opOoIx",
        "outputId": "73c632dc-357a-4887-8b5e-13567cb61c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Starting complete workflow...\n",
            "=== PHASE 2: Training Symmetry Generators ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50: 100%|██████████| 417/417 [00:12<00:00, 33.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Closure: 2.727080, Orthogonality: 0.818082, Skew-Sym: 2.782773, Magnitude: 5.231217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50: 100%|██████████| 417/417 [00:11<00:00, 35.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Closure: 0.020644, Orthogonality: 0.314845, Skew-Sym: 0.285639, Magnitude: 4.514511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50: 100%|██████████| 417/417 [00:11<00:00, 35.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Closure: 0.002503, Orthogonality: 0.032862, Skew-Sym: 0.051148, Magnitude: 3.649291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50: 100%|██████████| 417/417 [00:11<00:00, 35.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Closure: 0.000450, Orthogonality: 0.008253, Skew-Sym: 0.010290, Magnitude: 2.544594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50: 100%|██████████| 417/417 [00:11<00:00, 35.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Closure: 0.000086, Orthogonality: 0.001732, Skew-Sym: 0.001874, Magnitude: 1.657955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50: 100%|██████████| 417/417 [00:11<00:00, 35.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Closure: 0.000018, Orthogonality: 0.000070, Skew-Sym: 0.000432, Magnitude: 1.016771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50: 100%|██████████| 417/417 [00:11<00:00, 35.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Closure: 0.000008, Orthogonality: 0.000077, Skew-Sym: 0.000310, Magnitude: 0.633195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50: 100%|██████████| 417/417 [00:11<00:00, 35.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Closure: 0.000007, Orthogonality: 0.000187, Skew-Sym: 0.000455, Magnitude: 0.411793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50: 100%|██████████| 417/417 [00:12<00:00, 32.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - Closure: 0.000007, Orthogonality: 0.000501, Skew-Sym: 0.000854, Magnitude: 0.340232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50: 100%|██████████| 417/417 [00:11<00:00, 35.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - Closure: 0.000003, Orthogonality: 0.000287, Skew-Sym: 0.000442, Magnitude: 0.193797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50: 100%|██████████| 417/417 [00:11<00:00, 35.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 - Closure: 0.000033, Orthogonality: 0.000765, Skew-Sym: 0.002552, Magnitude: 0.159068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/50: 100%|██████████| 417/417 [00:11<00:00, 35.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 - Closure: 0.000068, Orthogonality: 0.001204, Skew-Sym: 0.008279, Magnitude: 0.161893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/50: 100%|██████████| 417/417 [00:11<00:00, 35.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 - Closure: 0.000005, Orthogonality: 0.000471, Skew-Sym: 0.000471, Magnitude: 0.085119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/50: 100%|██████████| 417/417 [00:11<00:00, 35.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 - Closure: 0.000094, Orthogonality: 0.001291, Skew-Sym: 0.004677, Magnitude: 0.142703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/50: 100%|██████████| 417/417 [00:11<00:00, 35.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 - Closure: 0.000014, Orthogonality: 0.000469, Skew-Sym: 0.000786, Magnitude: 0.079343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/50: 100%|██████████| 417/417 [00:11<00:00, 35.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 - Closure: 0.000025, Orthogonality: 0.000787, Skew-Sym: 0.001168, Magnitude: 0.073224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/50: 100%|██████████| 417/417 [00:11<00:00, 35.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 - Closure: 0.000046, Orthogonality: 0.001512, Skew-Sym: 0.003298, Magnitude: 0.153427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/50: 100%|██████████| 417/417 [00:11<00:00, 35.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 - Closure: 0.000017, Orthogonality: 0.001300, Skew-Sym: 0.001471, Magnitude: 0.072069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/50: 100%|██████████| 417/417 [00:11<00:00, 34.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 - Closure: 0.000123, Orthogonality: 0.001359, Skew-Sym: 0.006756, Magnitude: 0.175720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/50: 100%|██████████| 417/417 [00:11<00:00, 35.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 - Closure: 0.000028, Orthogonality: 0.001260, Skew-Sym: 0.002036, Magnitude: 0.070480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/50: 100%|██████████| 417/417 [00:11<00:00, 35.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 - Closure: 0.000116, Orthogonality: 0.001622, Skew-Sym: 0.006715, Magnitude: 0.124380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/50: 100%|██████████| 417/417 [00:11<00:00, 35.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 - Closure: 0.000037, Orthogonality: 0.000962, Skew-Sym: 0.000796, Magnitude: 0.144422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/50: 100%|██████████| 417/417 [00:11<00:00, 35.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 - Closure: 0.000012, Orthogonality: 0.001127, Skew-Sym: 0.001244, Magnitude: 0.095665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/50: 100%|██████████| 417/417 [00:11<00:00, 35.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 - Closure: 0.000177, Orthogonality: 0.002416, Skew-Sym: 0.014109, Magnitude: 0.208702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50: 100%|██████████| 417/417 [00:11<00:00, 35.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 - Closure: 0.000075, Orthogonality: 0.001513, Skew-Sym: 0.002788, Magnitude: 0.151361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50: 100%|██████████| 417/417 [00:11<00:00, 35.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 - Closure: 0.000103, Orthogonality: 0.001640, Skew-Sym: 0.003177, Magnitude: 0.193855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/50: 100%|██████████| 417/417 [00:11<00:00, 35.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 - Closure: 0.000028, Orthogonality: 0.001239, Skew-Sym: 0.001010, Magnitude: 0.101319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/50: 100%|██████████| 417/417 [00:11<00:00, 35.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 - Closure: 0.000103, Orthogonality: 0.001334, Skew-Sym: 0.002670, Magnitude: 0.189933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/50: 100%|██████████| 417/417 [00:11<00:00, 35.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 - Closure: 0.000089, Orthogonality: 0.001511, Skew-Sym: 0.004683, Magnitude: 0.161383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/50: 100%|██████████| 417/417 [00:11<00:00, 35.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 - Closure: 0.000037, Orthogonality: 0.001406, Skew-Sym: 0.002340, Magnitude: 0.139574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/50: 100%|██████████| 417/417 [00:11<00:00, 35.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31 - Closure: 0.000059, Orthogonality: 0.001415, Skew-Sym: 0.001925, Magnitude: 0.128954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/50: 100%|██████████| 417/417 [00:11<00:00, 35.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32 - Closure: 0.000183, Orthogonality: 0.001603, Skew-Sym: 0.001957, Magnitude: 0.182596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/50: 100%|██████████| 417/417 [00:11<00:00, 35.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33 - Closure: 0.000038, Orthogonality: 0.001515, Skew-Sym: 0.002558, Magnitude: 0.177982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/50: 100%|██████████| 417/417 [00:11<00:00, 35.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34 - Closure: 0.000035, Orthogonality: 0.001374, Skew-Sym: 0.002990, Magnitude: 0.164853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/50: 100%|██████████| 417/417 [00:11<00:00, 36.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35 - Closure: 0.000306, Orthogonality: 0.002100, Skew-Sym: 0.006974, Magnitude: 0.185376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/50: 100%|██████████| 417/417 [00:11<00:00, 35.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36 - Closure: 0.000071, Orthogonality: 0.001483, Skew-Sym: 0.001414, Magnitude: 0.169172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/50: 100%|██████████| 417/417 [00:11<00:00, 35.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37 - Closure: 0.000063, Orthogonality: 0.001378, Skew-Sym: 0.001796, Magnitude: 0.173309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/50: 100%|██████████| 417/417 [00:11<00:00, 35.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38 - Closure: 0.000178, Orthogonality: 0.001547, Skew-Sym: 0.004309, Magnitude: 0.184114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/50: 100%|██████████| 417/417 [00:11<00:00, 35.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39 - Closure: 0.000046, Orthogonality: 0.001307, Skew-Sym: 0.002246, Magnitude: 0.121887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/50: 100%|██████████| 417/417 [00:11<00:00, 35.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40 - Closure: 0.000140, Orthogonality: 0.001792, Skew-Sym: 0.003109, Magnitude: 0.199530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/50: 100%|██████████| 417/417 [00:11<00:00, 35.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41 - Closure: 0.000561, Orthogonality: 0.002130, Skew-Sym: 0.003935, Magnitude: 0.206813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/50: 100%|██████████| 417/417 [00:11<00:00, 35.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42 - Closure: 0.000064, Orthogonality: 0.001326, Skew-Sym: 0.003756, Magnitude: 0.187784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/50: 100%|██████████| 417/417 [00:11<00:00, 35.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43 - Closure: 0.000035, Orthogonality: 0.001206, Skew-Sym: 0.001140, Magnitude: 0.108768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/50: 100%|██████████| 417/417 [00:11<00:00, 35.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44 - Closure: 0.000076, Orthogonality: 0.001305, Skew-Sym: 0.001886, Magnitude: 0.134201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/50: 100%|██████████| 417/417 [00:11<00:00, 34.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45 - Closure: 0.000030, Orthogonality: 0.001599, Skew-Sym: 0.003092, Magnitude: 0.161488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/50: 100%|██████████| 417/417 [00:11<00:00, 35.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46 - Closure: 0.000198, Orthogonality: 0.001813, Skew-Sym: 0.006549, Magnitude: 0.159416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/50: 100%|██████████| 417/417 [00:11<00:00, 34.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47 - Closure: 0.000315, Orthogonality: 0.002555, Skew-Sym: 0.008896, Magnitude: 0.251332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/50: 100%|██████████| 417/417 [00:11<00:00, 35.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48 - Closure: 0.000027, Orthogonality: 0.000981, Skew-Sym: 0.001346, Magnitude: 0.123121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/50: 100%|██████████| 417/417 [00:11<00:00, 35.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49 - Closure: 0.000106, Orthogonality: 0.001489, Skew-Sym: 0.004840, Magnitude: 0.127619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/50: 100%|██████████| 417/417 [00:11<00:00, 35.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50 - Closure: 0.000029, Orthogonality: 0.001538, Skew-Sym: 0.002602, Magnitude: 0.152005\n",
            "\n",
            "=== EXAMPLE USAGE AFTER TRAINING ===\n",
            "Original z (eval batch):\n",
            "tensor([[-0.0224, -0.0307,  0.0248, -0.0026],\n",
            "        [-0.0230,  0.1641,  0.0248, -0.2873],\n",
            "        [-0.0197,  0.3589,  0.0190, -0.5721],\n",
            "        [-0.0126,  0.5537,  0.0076, -0.8587]], device='cuda:0')\n",
            "\n",
            "Per-sample product Π exp(θ_i G_i) with Taylor approximation (B, D, D):\n",
            "tensor([[[ 0.9977,  0.0270, -0.0465, -0.0387],\n",
            "         [-0.0251,  0.9993,  0.0304,  0.0095],\n",
            "         [ 0.0477, -0.0287,  0.9962, -0.0678],\n",
            "         [ 0.0432, -0.0104,  0.0663,  0.9970]],\n",
            "\n",
            "        [[ 0.9850,  0.1649, -0.0400,  0.0327],\n",
            "         [-0.1654,  0.9852, -0.0389, -0.0261],\n",
            "         [ 0.0366,  0.0447,  0.9978, -0.0335],\n",
            "         [-0.0355,  0.0218,  0.0342,  0.9987]],\n",
            "\n",
            "        [[ 0.9899, -0.1195,  0.0653, -0.0363],\n",
            "         [ 0.1228,  0.9907, -0.0221,  0.0471],\n",
            "         [-0.0658,  0.0346,  0.9928, -0.0937],\n",
            "         [ 0.0250, -0.0482,  0.0981,  0.9936]],\n",
            "\n",
            "        [[ 0.9994, -0.0200, -0.0157,  0.0231],\n",
            "         [ 0.0182,  0.9976, -0.0151,  0.0655],\n",
            "         [ 0.0166,  0.0147,  0.9998,  0.0029],\n",
            "         [-0.0247, -0.0650, -0.0016,  0.9976]]], device='cuda:0')\n",
            "\n",
            "z′ from product = (Π exp(θ_i G_i)) @ z:\n",
            "tensor([[-0.0242, -0.0293,  0.0247, -0.0016],\n",
            "        [-0.0060,  0.1720,  0.0408, -0.2817],\n",
            "        [-0.0404,  0.3257,  0.0862, -0.5843],\n",
            "        [-0.0436,  0.4958,  0.0130, -0.8923]], device='cuda:0')\n",
            "\n",
            "z′ from model forward (for cross-check):\n",
            "tensor([[-0.0242, -0.0293,  0.0247, -0.0016],\n",
            "        [-0.0060,  0.1720,  0.0408, -0.2817],\n",
            "        [-0.0404,  0.3257,  0.0862, -0.5843],\n",
            "        [-0.0436,  0.4958,  0.0130, -0.8923]], device='cuda:0')\n",
            "\n",
            "Mean |z′(product) - z′(model)| = 1.215813e-08\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETE - ANALYZING RESULTS\n",
            "============================================================\n",
            "=== Generator Analysis ===\n",
            "\n",
            "Generator 0:\n",
            "  Shape: (4, 4)\n",
            "  Min: -0.0870, Max: 0.0836\n",
            "  Mean: -0.000245, Std: 0.0302\n",
            "  Frobenius norm: 0.1207\n",
            "  Skew-symmetric: False\n",
            "\n",
            "Generator 1:\n",
            "  Shape: (4, 4)\n",
            "  Min: -0.1679, Max: 0.1681\n",
            "  Mean: 0.000029, Std: 0.0594\n",
            "  Frobenius norm: 0.2376\n",
            "  Skew-symmetric: True\n",
            "\n",
            "Generator 2:\n",
            "  Shape: (4, 4)\n",
            "  Min: -0.0586, Max: 0.0583\n",
            "  Mean: -0.000014, Std: 0.0207\n",
            "  Frobenius norm: 0.0827\n",
            "  Skew-symmetric: True\n",
            "\n",
            "Generator 3:\n",
            "  Shape: (4, 4)\n",
            "  Min: -0.0899, Max: 0.0888\n",
            "  Mean: -0.000069, Std: 0.0316\n",
            "  Frobenius norm: 0.1264\n",
            "  Skew-symmetric: False\n",
            "\n",
            "Generator 4:\n",
            "  Shape: (4, 4)\n",
            "  Min: -0.0758, Max: 0.0759\n",
            "  Mean: 0.000004, Std: 0.0268\n",
            "  Frobenius norm: 0.1072\n",
            "  Skew-symmetric: True\n",
            "\n",
            "Generator 5:\n",
            "  Shape: (4, 4)\n",
            "  Min: -0.0958, Max: 0.0948\n",
            "  Mean: -0.000213, Std: 0.0337\n",
            "  Frobenius norm: 0.1348\n",
            "  Skew-symmetric: False\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 3150x350 with 15 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing Symmetry Preservation ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'to'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1707553720.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mmodel_symmetry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1707553720.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Test symmetry preservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== Testing Symmetry Preservation ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mtest_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     theta_test = [(2*torch.rand(test_batch.shape[0], device=device) - 1)\n\u001b[1;32m     72\u001b[0m                   for _ in range(num_generators)]\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to'"
          ]
        }
      ],
      "source": [
        "model_symmetry = None\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Complete workflow: RL training → Symmetry detection\n",
        "    \"\"\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Parameters\n",
        "    env_name = 'CartPole-v1'\n",
        "    latent_dim = 4\n",
        "    num_generators = 6  # SO(4) has 6 generators\n",
        "\n",
        "    print(\"Starting complete workflow...\")\n",
        "\n",
        "    # PHASE 1: Train PPO agent (RL training)\n",
        "    # trained_agent = \"/content/trained_ppo_cartpole.pt\" # This was the issue, it was a string\n",
        "    # We need to train the agent first if it's not already saved and loaded\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # PHASE 3: Initialize and train symmetry generators\n",
        "    model_symmetry = GroupLatent(num_features=latent_dim, num_generators=num_generators)\n",
        "    model_symmetry.to(device)\n",
        "\n",
        "    # Define M for psi_4d\n",
        "    M = torch.eye(latent_dim, device=device)\n",
        "   # M = get_metric(latent_dim, device=device)\n",
        "\n",
        "    # Train symmetry generators on fixed latent data\n",
        "    closure_losses, orth_losses = train_symmetry_generators(\n",
        "        model_symmetry, latent_data, device=device, epochs=50,M=M)\n",
        "    torch.save(model_symmetry.state_dict(), \"model_symmetry_cartpole.pt\")\n",
        "\n",
        "    # PHASE 4: Analysis and visualization\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING COMPLETE - ANALYZING RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Analyze generators\n",
        "    analyze_generators(model_symmetry)\n",
        "\n",
        "    # Plot generators\n",
        "    plot_generators(model_symmetry)\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(closure_losses)\n",
        "    plt.title('Symmetry Closure Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.yscale('log')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(orth_losses)\n",
        "    plt.title('Symmetry Orthogonality Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.yscale('log')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Test symmetry preservation\n",
        "    print(\"\\n=== Testing Symmetry Preservation ===\")\n",
        "    test_batch = latent_data[:100].to(device)\n",
        "    theta_test = [(2*torch.rand(test_batch.shape[0], device=device) - 1)\n",
        "                  for _ in range(num_generators)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        transformed_batch = model_symmetry(theta=theta_test, x=test_batch)\n",
        "\n",
        "        original_invariant = psi_4d(test_batch)\n",
        "        transformed_invariant = psi_4d(transformed_batch)\n",
        "\n",
        "        invariant_error = torch.abs(original_invariant - transformed_invariant).mean()\n",
        "\n",
        "    print(f\"Average invariant preservation error: {invariant_error:.6f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"✓ Phase 1: PPO agent trained on CartPole (RL)\")\n",
        "    print(\"✓ Phase 2: Latent data generated from trained agent\")\n",
        "    print(\"✓ Phase 3: Symmetry generators trained on fixed latent data\")\n",
        "    print(\"✓ Phase 4: 6 SO(4) generators learned and visualized\")\n",
        "    print(\"✓ No augmentation - pure symmetry discovery\")\n",
        "\n",
        "\n",
        "    return  model_symmetry, latent_data\n",
        "\n",
        "\n",
        "model_symmetry, latent_data = main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''%%writefile /content/rsl_rl/rsl_rl/modules/cartpole_symmetry_utils.py\n",
        "# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION\n",
        "# All rights reserved.\n",
        "#\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "from tensordict import TensorDict\n",
        "from typing import Optional\n",
        "\n",
        "from rsl_rl.env import VecEnv\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def cartpole_symmetry(obs=None, actions=None, env=None, obs_type: str = \"policy\"):\n",
        "    \"\"\"Augment observations and actions for CartPole using learned symmetry generators.\n",
        "\n",
        "    Trained on raw CartPole states from generate_improved_cartpole_dataset:\n",
        "    - No normalization applied\n",
        "    - Reflection augmentation: position/velocity flipped (indices 0,1)\n",
        "    - Generators expect raw states as latents\n",
        "    \"\"\"\n",
        "\n",
        "    # observations - ONLY policy group\n",
        "    if obs is not None:\n",
        "        batch_size = obs.batch_size[0]\n",
        "        obs_aug = obs.repeat(2)  # original + symmetric\n",
        "\n",
        "        if env is None or not hasattr(env, \"model_symmetry\"):\n",
        "            # fallback: simple reflection (matching your dataset generation)\n",
        "            obs_aug[\"policy\"][:batch_size] = obs[\"policy\"]\n",
        "            obs_aug[\"policy\"][batch_size : 2 * batch_size, [0, 1]] = -obs[\"policy\"][:, [0, 1]]\n",
        "        else:\n",
        "            model_symmetry = env.model_symmetry\n",
        "            dev = next(model_symmetry.parameters()).device\n",
        "\n",
        "            # -- original\n",
        "            obs_aug[\"policy\"][:batch_size] = obs[\"policy\"]\n",
        "\n",
        "            # -- generator-based symmetric version (trained on raw states)\n",
        "            s = obs[\"policy\"].to(dev)  # (B, 4) raw CartPole states\n",
        "\n",
        "            # NO normalization - your generators saw raw states directly\n",
        "            z = s  # raw state = latent\n",
        "\n",
        "            # sample theta per generator per sample (matching train_symmetry_generators)\n",
        "            num_generators = getattr(model_symmetry, \"num_generators\", None)\n",
        "            if num_generators is None:\n",
        "                num_generators = getattr(model_symmetry, \"numgenerators\")\n",
        "\n",
        "            theta = [(2 * torch.rand(batch_size, device=dev) - 1) for _ in range(num_generators)]\n",
        "\n",
        "            # apply symmetry: z' = model_symmetry(theta=theta, x=z)\n",
        "            z_prime = model_symmetry(theta=theta, x=z)  # (B, 4)\n",
        "\n",
        "            obs_aug[\"policy\"][batch_size : 2 * batch_size] = z_prime.to(obs[\"policy\"].device)\n",
        "    else:\n",
        "        obs_aug = None\n",
        "\n",
        "    # actions\n",
        "    actions_aug = None\n",
        "    if actions is not None:\n",
        "        batch_size = actions.shape[0]\n",
        "        actions_aug = torch.zeros(batch_size * 2, actions.shape[1], device=actions.device)\n",
        "        # -- original\n",
        "        actions_aug[:batch_size] = actions\n",
        "        # -- left-right flip (consistent with position/velocity reflection)\n",
        "        actions_aug[batch_size : 2 * batch_size] = 1 - actions\n",
        "\n",
        "    return obs_aug, actions_aug\n",
        "''''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49kOX-iEJV0G",
        "outputId": "ecae89de-814f-4896-9afc-f43477078449"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/rsl_rl/rsl_rl/modules/cartpole_symmetry_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/rsl_rl/rsl_rl/modules/cartpole_symmetry_utils.py\n",
        "# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION\n",
        "# All rights reserved.\n",
        "#\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "from tensordict import TensorDict\n",
        "from typing import Optional\n",
        "\n",
        "from rsl_rl.env import VecEnv\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def cartpole_symmetry(obs=None, actions=None, env=None, obs_type: str = \"policy\"):\n",
        "    \"\"\"Augment observations AND actions for CartPole using learned symmetry generators.\n",
        "\n",
        "    Both obs and actions are transformed by the SAME symmetry flow:\n",
        "    - obs[\"policy\"] → model_symmetry(theta, obs[\"policy\"])\n",
        "    - actions → model_symmetry(theta, actions_expanded)\n",
        "    \"\"\"\n",
        "\n",
        "    # observations - ONLY policy group\n",
        "    if obs is not None:\n",
        "        batch_size = obs.batch_size[0]\n",
        "        obs_aug = obs.repeat(2)  # original + symmetric\n",
        "\n",
        "        if env is None or not hasattr(env, \"model_symmetry\"):\n",
        "            # fallback: simple reflection\n",
        "            obs_aug[\"policy\"][:batch_size] = obs[\"policy\"]\n",
        "            obs_aug[\"policy\"][batch_size : 2 * batch_size, [0, 1]] = -obs[\"policy\"][:, [0, 1]]\n",
        "        else:\n",
        "            model_symmetry = env.model_symmetry\n",
        "            dev = next(model_symmetry.parameters()).device\n",
        "\n",
        "            # -- original\n",
        "            obs_aug[\"policy\"][:batch_size] = obs[\"policy\"]\n",
        "\n",
        "            # -- generator-based symmetric version\n",
        "            s = obs[\"policy\"].to(dev)  # (B, 4)\n",
        "            z = s  # raw state = latent\n",
        "\n",
        "            num_generators = getattr(model_symmetry, \"num_generators\", None)\n",
        "            if num_generators is None:\n",
        "                num_generators = getattr(model_symmetry, \"numgenerators\")\n",
        "\n",
        "            theta = [(2 * torch.rand(batch_size, device=dev) - 1) for _ in range(num_generators)]\n",
        "            z_prime = model_symmetry(theta=theta, x=z)  # (B, 4)\n",
        "\n",
        "            obs_aug[\"policy\"][batch_size : 2 * batch_size] = z_prime.to(obs[\"policy\"].device)\n",
        "    else:\n",
        "        obs_aug = None\n",
        "\n",
        "    # actions - APPLY SAME SYMMETRY TRANSFORMATION\n",
        "    actions_aug = None\n",
        "    if actions is not None:\n",
        "        batch_size = actions.shape[0]\n",
        "        actions_aug = torch.zeros(batch_size * 2, actions.shape[1], device=actions.device)\n",
        "\n",
        "        # -- original\n",
        "        actions_aug[:batch_size] = actions\n",
        "\n",
        "        if env is not None and hasattr(env, \"model_symmetry\"):\n",
        "            model_symmetry = env.model_symmetry\n",
        "            dev = next(model_symmetry.parameters()).device\n",
        "\n",
        "            # Expand discrete actions to match state dim (4) for symmetry flow\n",
        "            # One-hot encode: actions (B,1) → actions_expanded (B,4)\n",
        "            actions_onehot = torch.zeros(batch_size, 4, device=dev)\n",
        "            actions_onehot.scatter_(1, actions.unsqueeze(1).to(dev), 1.0)  # (B,4)\n",
        "\n",
        "            # Apply SAME symmetry flow as states\n",
        "            num_generators = getattr(model_symmetry, \"num_generators\", None)\n",
        "            if num_generators is None:\n",
        "                num_generators = getattr(model_symmetry, \"numgenerators\")\n",
        "\n",
        "            theta = [(2 * torch.rand(batch_size, device=dev) - 1) for _ in range(num_generators)]\n",
        "            actions_sym = model_symmetry(theta=theta, x=actions_onehot)  # (B,4)\n",
        "\n",
        "            # Convert back to discrete actions: argmax over transformed distribution\n",
        "            actions_sym_discrete = torch.argmax(actions_sym, dim=1)  # (B,)\n",
        "\n",
        "            actions_aug[batch_size : 2 * batch_size] = actions_sym_discrete.to(actions.device)\n",
        "        else:\n",
        "            # fallback: simple flip\n",
        "            actions_aug[batch_size : 2 * batch_size] = 1 - actions\n",
        "\n",
        "    return obs_aug, actions_aug\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfRZQMxGSVYz",
        "outputId": "114ee598-0eca-4194-c47b-d83588292f3c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/rsl_rl/rsl_rl/modules/cartpole_symmetry_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "from types import SimpleNamespace\n",
        "from rsl_rl.runners import OnPolicyRunner\n",
        "#import cartpole_symmetry\n",
        "\n",
        "yaml_config_string = \"\"\"\n",
        "runner:\n",
        "  class_name: OnPolicyRunner\n",
        "  # General\n",
        "  num_steps_per_env: 24  # Number of steps per environment per iteration\n",
        "  max_iterations: 100    # Number of policy updates\n",
        "  seed: 1\n",
        "  # Observations\n",
        "  obs_groups: {\"policy\": [\"policy\"], \"critic\": [\"policy\", \"privileged\"]}  # Maps observation groups to sets. See `vec_env.py` for more information\n",
        "  # Logging parameters\n",
        "  save_interval: 50  # Check for potential saves every `save_interval` iterations\n",
        "  experiment_name: walking_experiment\n",
        "  run_name: \"\"\n",
        "  # Logging writer\n",
        "  logger: tensorboard  # tensorboard, neptune, wandb\n",
        "  neptune_project: legged_gym\n",
        "  wandb_project: legged_gym\n",
        "\n",
        "  # Policy\n",
        "  policy:\n",
        "    class_name: ActorCritic\n",
        "    activation: elu\n",
        "    actor_obs_normalization: false\n",
        "    critic_obs_normalization: false\n",
        "    actor_hidden_dims: [256, 256, 256]\n",
        "    critic_hidden_dims: [256, 256, 256]\n",
        "    init_noise_std: 1.0\n",
        "    noise_std_type: \"scalar\"  # 'scalar' or 'log'\n",
        "    state_dependent_std: false\n",
        "\n",
        "  # Algorithm\n",
        "  algorithm:\n",
        "    class_name: PPO\n",
        "    # Training\n",
        "    learning_rate: 0.001\n",
        "    num_learning_epochs: 5\n",
        "    num_mini_batches: 4  # mini batch size = num_envs * num_steps / num_mini_batches\n",
        "    schedule: adaptive  # adaptive, fixed\n",
        "    # Value function\n",
        "    value_loss_coef: 1.0\n",
        "    clip_param: 0.2\n",
        "    use_clipped_value_loss: true\n",
        "    # Surrogate loss\n",
        "    desired_kl: 0.01\n",
        "    entropy_coef: 0.01\n",
        "    gamma: 0.99\n",
        "    lam: 0.95\n",
        "    max_grad_norm: 1.0\n",
        "    # Miscellaneous\n",
        "    normalize_advantage_per_mini_batch: false\n",
        "\n",
        "    # Random network distillation\n",
        "    rnd_cfg:\n",
        "      weight: 0.0  # Initial weight of the RND reward\n",
        "      weight_schedule: null  # This is a dictionary with a required key called \"mode\". Please check the RND module for more information\n",
        "      reward_normalization: false  # Whether to normalize RND reward\n",
        "      # Learning parameters\n",
        "      learning_rate: 0.001  # Learning rate for RND\n",
        "      # Network parameters\n",
        "      num_outputs: 1  # Number of outputs of RND network. Note: if -1, then the network will use dimensions of the observation\n",
        "      predictor_hidden_dims: [-1]  # Hidden dimensions of predictor network\n",
        "      target_hidden_dims: [-1]  # Hidden dimensions of target network\n",
        "\n",
        "    # Symmetry augmentation\n",
        "    symmetry_cfg:\n",
        "      use_data_augmentation: true  # This adds symmetric trajectories to the batch\n",
        "      use_mirror_loss: false  # This adds symmetry loss term to the loss function\n",
        "      data_augmentation_func: \"rsl_rl.modules.cartpole_symmetry_utils:cartpole_symmetry\"  # String containing the module and function name to import\n",
        "      # Example: \"legged_gym.envs.locomotion.anymal_c.symmetry:get_symmetric_states\"\n",
        "      mirror_loss_coeff: 0.0  # Coefficient for symmetry loss term. If 0, no symmetry loss is used\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_train_cfg():\n",
        "    raw = yaml.safe_load(yaml_config_string)\n",
        "    # Optional: turn top-level dict into an object-like cfg\n",
        "    return SimpleNamespace(**raw)\n",
        "import os\n",
        "\n",
        "log_dir = \"./logs_cartpole\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "# Usage:\n",
        "train_cfg = get_train_cfg()\n",
        "print(train_cfg.runner[\"class_name\"])   # 'OnPolicyRunner'\n",
        "# later:\n",
        "env = CartPoleVecEnv(num_envs=8, device=\"cuda:0\")\n",
        "model_symmetry = GroupLatent(num_features=4, num_generators=6).to(device)\n",
        "model_symmetry.load_state_dict(torch.load(\"model_symmetry_cartpole.pt\"))\n",
        "env.model_symmetry = model_symmetry\n",
        "runner = OnPolicyRunner(env, train_cfg.runner, log_dir=log_dir)\n",
        "runner.learn(num_learning_iterations=train_cfg.runner[\"max_iterations\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX9C7kO8aj8N",
        "outputId": "7783afea-7bf4-4234-e780-8265a4e942ee"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OnPolicyRunner\n",
            "--------------------------------------------------------------------------------\n",
            "Resolved observation sets: \n",
            "\t policy :  ['policy']\n",
            "\t critic :  ['policy', 'privileged']\n",
            "\t rnd_state :  ['policy']\n",
            "--------------------------------------------------------------------------------\n",
            "Actor MLP: MLP(\n",
            "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n",
            "Critic MLP: MLP(\n",
            "  (0): Linear(in_features=8, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/rsl_rl/rsl_rl/utils/utils.py:273: UserWarning: The observation configuration dictionary 'obs_groups' must contain the 'rnd_state' key. As the configuration for 'rnd_state' is missing, the observations from the 'policy' set are used. Consider adding the 'rnd_state' key to the 'obs_groups' dictionary for clarity. This behavior will be removed in a future version.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 0/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 192 \n",
            "                       Steps per second: 677 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.204s \n",
            "                        Mean value loss: 67.0136\n",
            "                    Mean surrogate loss: 0.2042\n",
            "                      Mean entropy loss: 2.8449\n",
            "                          Mean rnd loss: 0.0947\n",
            "                     Mean symmetry loss: 0.5954\n",
            "                  Mean extrinsic reward: 18.33\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 18.33\n",
            "                    Mean episode length: 18.33\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:00\n",
            "                                    ETA: 00:00:28\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 1/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 384 \n",
            "                       Steps per second: 729 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.181s \n",
            "                        Mean value loss: 35.7377\n",
            "                    Mean surrogate loss: 0.1315\n",
            "                      Mean entropy loss: 2.8462\n",
            "                          Mean rnd loss: 0.0628\n",
            "                     Mean symmetry loss: 0.2956\n",
            "                  Mean extrinsic reward: 23.27\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 23.27\n",
            "                    Mean episode length: 23.27\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:00\n",
            "                                    ETA: 00:00:26\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 2/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 576 \n",
            "                       Steps per second: 757 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.178s \n",
            "                        Mean value loss: 44.4441\n",
            "                    Mean surrogate loss: 0.0336\n",
            "                      Mean entropy loss: 2.8454\n",
            "                          Mean rnd loss: 0.0432\n",
            "                     Mean symmetry loss: 0.0331\n",
            "                  Mean extrinsic reward: 23.42\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 23.42\n",
            "                    Mean episode length: 23.42\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:00\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 3/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 768 \n",
            "                       Steps per second: 732 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.185s \n",
            "                        Mean value loss: 36.8078\n",
            "                    Mean surrogate loss: -0.0024\n",
            "                      Mean entropy loss: 2.8451\n",
            "                          Mean rnd loss: 0.0227\n",
            "                     Mean symmetry loss: 0.0086\n",
            "                  Mean extrinsic reward: 22.50\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 22.50\n",
            "                    Mean episode length: 22.50\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 4/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 960 \n",
            "                       Steps per second: 705 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.197s \n",
            "                        Mean value loss: 53.6701\n",
            "                    Mean surrogate loss: 0.0057\n",
            "                      Mean entropy loss: 2.8479\n",
            "                          Mean rnd loss: 0.0179\n",
            "                     Mean symmetry loss: 0.0058\n",
            "                  Mean extrinsic reward: 22.88\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 22.88\n",
            "                    Mean episode length: 22.88\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 5/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1152 \n",
            "                       Steps per second: 762 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.177s \n",
            "                        Mean value loss: 60.3583\n",
            "                    Mean surrogate loss: 0.0005\n",
            "                      Mean entropy loss: 2.8498\n",
            "                          Mean rnd loss: 0.0202\n",
            "                     Mean symmetry loss: 0.0026\n",
            "                  Mean extrinsic reward: 23.31\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 23.31\n",
            "                    Mean episode length: 23.31\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 6/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1344 \n",
            "                       Steps per second: 740 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.182s \n",
            "                        Mean value loss: 51.1263\n",
            "                    Mean surrogate loss: 0.0191\n",
            "                      Mean entropy loss: 2.8608\n",
            "                          Mean rnd loss: 0.0101\n",
            "                     Mean symmetry loss: 0.0121\n",
            "                  Mean extrinsic reward: 26.87\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 26.87\n",
            "                    Mean episode length: 26.87\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 7/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1536 \n",
            "                       Steps per second: 695 \n",
            "                        Collection time: 0.088s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 48.5506\n",
            "                    Mean surrogate loss: -0.0009\n",
            "                      Mean entropy loss: 2.8618\n",
            "                          Mean rnd loss: 0.0042\n",
            "                     Mean symmetry loss: 0.0134\n",
            "                  Mean extrinsic reward: 29.68\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 29.68\n",
            "                    Mean episode length: 29.68\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 8/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1728 \n",
            "                       Steps per second: 705 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.197s \n",
            "                        Mean value loss: 54.4007\n",
            "                    Mean surrogate loss: -0.0077\n",
            "                      Mean entropy loss: 2.8619\n",
            "                          Mean rnd loss: 0.0055\n",
            "                     Mean symmetry loss: 0.0198\n",
            "                  Mean extrinsic reward: 30.48\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 30.48\n",
            "                    Mean episode length: 30.48\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 9/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1920 \n",
            "                       Steps per second: 749 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.182s \n",
            "                        Mean value loss: 56.0602\n",
            "                    Mean surrogate loss: -0.0122\n",
            "                      Mean entropy loss: 2.8631\n",
            "                          Mean rnd loss: 0.0064\n",
            "                     Mean symmetry loss: 0.0136\n",
            "                  Mean extrinsic reward: 33.15\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.15\n",
            "                    Mean episode length: 33.15\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 10/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2112 \n",
            "                       Steps per second: 747 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.184s \n",
            "                        Mean value loss: 50.3940\n",
            "                    Mean surrogate loss: 0.0024\n",
            "                      Mean entropy loss: 2.8676\n",
            "                          Mean rnd loss: 0.0023\n",
            "                     Mean symmetry loss: 0.0049\n",
            "                  Mean extrinsic reward: 35.35\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 35.35\n",
            "                    Mean episode length: 35.35\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 11/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2304 \n",
            "                       Steps per second: 735 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.184s \n",
            "                        Mean value loss: 58.1093\n",
            "                    Mean surrogate loss: -0.0048\n",
            "                      Mean entropy loss: 2.8690\n",
            "                          Mean rnd loss: 0.0022\n",
            "                     Mean symmetry loss: 0.0012\n",
            "                  Mean extrinsic reward: 37.67\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 37.67\n",
            "                    Mean episode length: 37.67\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 12/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2496 \n",
            "                       Steps per second: 702 \n",
            "                        Collection time: 0.073s \n",
            "                          Learning time: 0.201s \n",
            "                        Mean value loss: 65.6641\n",
            "                    Mean surrogate loss: 0.0187\n",
            "                      Mean entropy loss: 2.8708\n",
            "                          Mean rnd loss: 0.0011\n",
            "                     Mean symmetry loss: 0.0085\n",
            "                  Mean extrinsic reward: 37.67\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 37.67\n",
            "                    Mean episode length: 37.67\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 13/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2688 \n",
            "                       Steps per second: 715 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.191s \n",
            "                        Mean value loss: 54.1455\n",
            "                    Mean surrogate loss: 0.0020\n",
            "                      Mean entropy loss: 2.8718\n",
            "                          Mean rnd loss: 0.0033\n",
            "                     Mean symmetry loss: 0.0048\n",
            "                  Mean extrinsic reward: 40.05\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 40.05\n",
            "                    Mean episode length: 40.05\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 14/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2880 \n",
            "                       Steps per second: 712 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.194s \n",
            "                        Mean value loss: 48.2174\n",
            "                    Mean surrogate loss: -0.0022\n",
            "                      Mean entropy loss: 2.8730\n",
            "                          Mean rnd loss: 0.0036\n",
            "                     Mean symmetry loss: 0.0105\n",
            "                  Mean extrinsic reward: 42.53\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 42.53\n",
            "                    Mean episode length: 42.53\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 15/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3072 \n",
            "                       Steps per second: 495 \n",
            "                        Collection time: 0.124s \n",
            "                          Learning time: 0.264s \n",
            "                        Mean value loss: 49.0671\n",
            "                    Mean surrogate loss: 0.0270\n",
            "                      Mean entropy loss: 2.8760\n",
            "                          Mean rnd loss: 0.0007\n",
            "                     Mean symmetry loss: 0.0983\n",
            "                  Mean extrinsic reward: 44.94\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 44.94\n",
            "                    Mean episode length: 44.94\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.39s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 16/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3264 \n",
            "                       Steps per second: 533 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.254s \n",
            "                        Mean value loss: 47.5874\n",
            "                    Mean surrogate loss: 0.0119\n",
            "                      Mean entropy loss: 2.8764\n",
            "                          Mean rnd loss: 0.0010\n",
            "                     Mean symmetry loss: 0.0473\n",
            "                  Mean extrinsic reward: 45.39\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 45.39\n",
            "                    Mean episode length: 45.39\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.36s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 17/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3456 \n",
            "                       Steps per second: 534 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.263s \n",
            "                        Mean value loss: 55.6133\n",
            "                    Mean surrogate loss: 0.0242\n",
            "                      Mean entropy loss: 2.8767\n",
            "                          Mean rnd loss: 0.0025\n",
            "                     Mean symmetry loss: 0.2104\n",
            "                  Mean extrinsic reward: 45.32\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 45.32\n",
            "                    Mean episode length: 45.32\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.36s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 18/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3648 \n",
            "                       Steps per second: 563 \n",
            "                        Collection time: 0.093s \n",
            "                          Learning time: 0.248s \n",
            "                        Mean value loss: 58.4137\n",
            "                    Mean surrogate loss: 0.2335\n",
            "                      Mean entropy loss: 2.8789\n",
            "                          Mean rnd loss: 0.0024\n",
            "                     Mean symmetry loss: 2.3280\n",
            "                  Mean extrinsic reward: 46.25\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 46.25\n",
            "                    Mean episode length: 46.25\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.34s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 19/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3840 \n",
            "                       Steps per second: 497 \n",
            "                        Collection time: 0.112s \n",
            "                          Learning time: 0.273s \n",
            "                        Mean value loss: 53.5438\n",
            "                    Mean surrogate loss: 0.1657\n",
            "                      Mean entropy loss: 2.8791\n",
            "                          Mean rnd loss: 0.0008\n",
            "                     Mean symmetry loss: 1.9738\n",
            "                  Mean extrinsic reward: 47.04\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 47.04\n",
            "                    Mean episode length: 47.04\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.39s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 20/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4032 \n",
            "                       Steps per second: 504 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.282s \n",
            "                        Mean value loss: 54.9364\n",
            "                    Mean surrogate loss: 0.1134\n",
            "                      Mean entropy loss: 2.8805\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.7099\n",
            "                  Mean extrinsic reward: 48.06\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 48.06\n",
            "                    Mean episode length: 48.06\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.38s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 21/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4224 \n",
            "                       Steps per second: 506 \n",
            "                        Collection time: 0.118s \n",
            "                          Learning time: 0.261s \n",
            "                        Mean value loss: 47.5436\n",
            "                    Mean surrogate loss: 0.0450\n",
            "                      Mean entropy loss: 2.8825\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0178\n",
            "                  Mean extrinsic reward: 48.06\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 48.06\n",
            "                    Mean episode length: 48.06\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.38s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 22/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4416 \n",
            "                       Steps per second: 747 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.180s \n",
            "                        Mean value loss: 46.8600\n",
            "                    Mean surrogate loss: -0.0095\n",
            "                      Mean entropy loss: 2.8829\n",
            "                          Mean rnd loss: 0.0007\n",
            "                     Mean symmetry loss: 0.0127\n",
            "                  Mean extrinsic reward: 48.54\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 48.54\n",
            "                    Mean episode length: 48.54\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 23/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4608 \n",
            "                       Steps per second: 704 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.194s \n",
            "                        Mean value loss: 63.4821\n",
            "                    Mean surrogate loss: -0.0026\n",
            "                      Mean entropy loss: 2.8840\n",
            "                          Mean rnd loss: 0.0013\n",
            "                     Mean symmetry loss: 0.0074\n",
            "                  Mean extrinsic reward: 49.38\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 49.38\n",
            "                    Mean episode length: 49.38\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 24/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4800 \n",
            "                       Steps per second: 718 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.192s \n",
            "                        Mean value loss: 51.4642\n",
            "                    Mean surrogate loss: 0.0119\n",
            "                      Mean entropy loss: 2.8845\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0094\n",
            "                  Mean extrinsic reward: 50.60\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 50.60\n",
            "                    Mean episode length: 50.60\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 25/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4992 \n",
            "                       Steps per second: 670 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.207s \n",
            "                        Mean value loss: 53.4863\n",
            "                    Mean surrogate loss: -0.0115\n",
            "                      Mean entropy loss: 2.8844\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0440\n",
            "                  Mean extrinsic reward: 50.92\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 50.92\n",
            "                    Mean episode length: 50.92\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 26/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5184 \n",
            "                       Steps per second: 707 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.195s \n",
            "                        Mean value loss: 58.8546\n",
            "                    Mean surrogate loss: 0.0124\n",
            "                      Mean entropy loss: 2.8845\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0286\n",
            "                  Mean extrinsic reward: 51.87\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 51.87\n",
            "                    Mean episode length: 51.87\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 27/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5376 \n",
            "                       Steps per second: 741 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.184s \n",
            "                        Mean value loss: 59.5637\n",
            "                    Mean surrogate loss: 0.0071\n",
            "                      Mean entropy loss: 2.8846\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0363\n",
            "                  Mean extrinsic reward: 52.48\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 52.48\n",
            "                    Mean episode length: 52.48\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 28/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5568 \n",
            "                       Steps per second: 737 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.177s \n",
            "                        Mean value loss: 55.6464\n",
            "                    Mean surrogate loss: 0.0164\n",
            "                      Mean entropy loss: 2.8846\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0270\n",
            "                  Mean extrinsic reward: 52.95\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 52.95\n",
            "                    Mean episode length: 52.95\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 29/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5760 \n",
            "                       Steps per second: 690 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.204s \n",
            "                        Mean value loss: 49.7326\n",
            "                    Mean surrogate loss: 0.0067\n",
            "                      Mean entropy loss: 2.8843\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0099\n",
            "                  Mean extrinsic reward: 53.56\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 53.56\n",
            "                    Mean episode length: 53.56\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 30/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5952 \n",
            "                       Steps per second: 716 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.192s \n",
            "                        Mean value loss: 46.0370\n",
            "                    Mean surrogate loss: -0.0077\n",
            "                      Mean entropy loss: 2.8837\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0177\n",
            "                  Mean extrinsic reward: 54.87\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 54.87\n",
            "                    Mean episode length: 54.87\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 31/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6144 \n",
            "                       Steps per second: 742 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.184s \n",
            "                        Mean value loss: 44.3335\n",
            "                    Mean surrogate loss: 0.0103\n",
            "                      Mean entropy loss: 2.8850\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0532\n",
            "                  Mean extrinsic reward: 57.36\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 57.36\n",
            "                    Mean episode length: 57.36\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 32/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6336 \n",
            "                       Steps per second: 734 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.184s \n",
            "                        Mean value loss: 38.7660\n",
            "                    Mean surrogate loss: 0.0058\n",
            "                      Mean entropy loss: 2.8859\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0434\n",
            "                  Mean extrinsic reward: 58.30\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 58.30\n",
            "                    Mean episode length: 58.30\n",
            "                  Mean action noise std: 1.02\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 33/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6528 \n",
            "                       Steps per second: 706 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.197s \n",
            "                        Mean value loss: 38.3371\n",
            "                    Mean surrogate loss: 0.0007\n",
            "                      Mean entropy loss: 2.8875\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.1551\n",
            "                  Mean extrinsic reward: 59.00\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 59.00\n",
            "                    Mean episode length: 59.00\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 34/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6720 \n",
            "                       Steps per second: 723 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.192s \n",
            "                        Mean value loss: 35.7997\n",
            "                    Mean surrogate loss: 0.0314\n",
            "                      Mean entropy loss: 2.8877\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0442\n",
            "                  Mean extrinsic reward: 60.43\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 60.43\n",
            "                    Mean episode length: 60.43\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 35/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6912 \n",
            "                       Steps per second: 712 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.193s \n",
            "                        Mean value loss: 34.3919\n",
            "                    Mean surrogate loss: 0.0035\n",
            "                      Mean entropy loss: 2.8879\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0158\n",
            "                  Mean extrinsic reward: 62.43\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 62.43\n",
            "                    Mean episode length: 62.43\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 36/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7104 \n",
            "                       Steps per second: 712 \n",
            "                        Collection time: 0.085s \n",
            "                          Learning time: 0.184s \n",
            "                        Mean value loss: 45.9546\n",
            "                    Mean surrogate loss: -0.0042\n",
            "                      Mean entropy loss: 2.8889\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0334\n",
            "                  Mean extrinsic reward: 63.20\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 63.20\n",
            "                    Mean episode length: 63.20\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 37/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7296 \n",
            "                       Steps per second: 713 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.195s \n",
            "                        Mean value loss: 19.8467\n",
            "                    Mean surrogate loss: 0.0122\n",
            "                      Mean entropy loss: 2.8900\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0250\n",
            "                  Mean extrinsic reward: 63.77\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 63.77\n",
            "                    Mean episode length: 63.77\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 38/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7488 \n",
            "                       Steps per second: 704 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.197s \n",
            "                        Mean value loss: 26.5843\n",
            "                    Mean surrogate loss: 0.0181\n",
            "                      Mean entropy loss: 2.8901\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0146\n",
            "                  Mean extrinsic reward: 66.44\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 66.44\n",
            "                    Mean episode length: 66.44\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 39/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7680 \n",
            "                       Steps per second: 735 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.186s \n",
            "                        Mean value loss: 16.8980\n",
            "                    Mean surrogate loss: 0.0076\n",
            "                      Mean entropy loss: 2.8890\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0422\n",
            "                  Mean extrinsic reward: 66.44\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 66.44\n",
            "                    Mean episode length: 66.44\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 40/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7872 \n",
            "                       Steps per second: 745 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.183s \n",
            "                        Mean value loss: 23.8045\n",
            "                    Mean surrogate loss: 0.0132\n",
            "                      Mean entropy loss: 2.8888\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0179\n",
            "                  Mean extrinsic reward: 67.50\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 67.50\n",
            "                    Mean episode length: 67.50\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 41/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8064 \n",
            "                       Steps per second: 699 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.197s \n",
            "                        Mean value loss: 41.9031\n",
            "                    Mean surrogate loss: -0.0039\n",
            "                      Mean entropy loss: 2.8887\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0168\n",
            "                  Mean extrinsic reward: 69.01\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 69.01\n",
            "                    Mean episode length: 69.01\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 42/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8256 \n",
            "                       Steps per second: 741 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.184s \n",
            "                        Mean value loss: 15.1835\n",
            "                    Mean surrogate loss: -0.0023\n",
            "                      Mean entropy loss: 2.8903\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0097\n",
            "                  Mean extrinsic reward: 69.39\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 69.39\n",
            "                    Mean episode length: 69.39\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 43/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8448 \n",
            "                       Steps per second: 691 \n",
            "                        Collection time: 0.084s \n",
            "                          Learning time: 0.194s \n",
            "                        Mean value loss: 31.5051\n",
            "                    Mean surrogate loss: -0.0058\n",
            "                      Mean entropy loss: 2.8945\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0059\n",
            "                  Mean extrinsic reward: 69.24\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 69.24\n",
            "                    Mean episode length: 69.24\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 44/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8640 \n",
            "                       Steps per second: 718 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 21.6863\n",
            "                    Mean surrogate loss: 0.0071\n",
            "                      Mean entropy loss: 2.9000\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: 0.0170\n",
            "                  Mean extrinsic reward: 71.65\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 71.65\n",
            "                    Mean episode length: 71.65\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 45/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8832 \n",
            "                       Steps per second: 662 \n",
            "                        Collection time: 0.090s \n",
            "                          Learning time: 0.199s \n",
            "                        Mean value loss: 12.8517\n",
            "                    Mean surrogate loss: 0.0193\n",
            "                      Mean entropy loss: 2.9033\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0818\n",
            "                  Mean extrinsic reward: 72.06\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 72.06\n",
            "                    Mean episode length: 72.06\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 46/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9024 \n",
            "                       Steps per second: 685 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.201s \n",
            "                        Mean value loss: 11.0710\n",
            "                    Mean surrogate loss: 0.0543\n",
            "                      Mean entropy loss: 2.9032\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.2710\n",
            "                  Mean extrinsic reward: 72.30\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 72.30\n",
            "                    Mean episode length: 72.30\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 47/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9216 \n",
            "                       Steps per second: 705 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 8.3094\n",
            "                    Mean surrogate loss: 0.0491\n",
            "                      Mean entropy loss: 2.9033\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.1390\n",
            "                  Mean extrinsic reward: 71.55\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 71.55\n",
            "                    Mean episode length: 71.55\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 48/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9408 \n",
            "                       Steps per second: 709 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.196s \n",
            "                        Mean value loss: 14.4279\n",
            "                    Mean surrogate loss: -0.0036\n",
            "                      Mean entropy loss: 2.9032\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0216\n",
            "                  Mean extrinsic reward: 71.15\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 71.15\n",
            "                    Mean episode length: 71.15\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 49/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9600 \n",
            "                       Steps per second: 726 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.189s \n",
            "                        Mean value loss: 10.8691\n",
            "                    Mean surrogate loss: 0.0132\n",
            "                      Mean entropy loss: 2.9032\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0249\n",
            "                  Mean extrinsic reward: 70.31\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 70.31\n",
            "                    Mean episode length: 70.31\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 50/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9792 \n",
            "                       Steps per second: 749 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.181s \n",
            "                        Mean value loss: 7.4634\n",
            "                    Mean surrogate loss: -0.0034\n",
            "                      Mean entropy loss: 2.9032\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0137\n",
            "                  Mean extrinsic reward: 70.31\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 70.31\n",
            "                    Mean episode length: 70.31\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 51/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9984 \n",
            "                       Steps per second: 738 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.184s \n",
            "                        Mean value loss: 20.5468\n",
            "                    Mean surrogate loss: 0.0150\n",
            "                      Mean entropy loss: 2.9022\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0718\n",
            "                  Mean extrinsic reward: 70.50\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 70.50\n",
            "                    Mean episode length: 70.50\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 52/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10176 \n",
            "                       Steps per second: 683 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.204s \n",
            "                        Mean value loss: 42.7187\n",
            "                    Mean surrogate loss: 0.0007\n",
            "                      Mean entropy loss: 2.9025\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0069\n",
            "                  Mean extrinsic reward: 70.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 70.13\n",
            "                    Mean episode length: 70.13\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 53/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10368 \n",
            "                       Steps per second: 702 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.194s \n",
            "                        Mean value loss: 19.9901\n",
            "                    Mean surrogate loss: 0.0005\n",
            "                      Mean entropy loss: 2.9036\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0026\n",
            "                  Mean extrinsic reward: 70.66\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 70.66\n",
            "                    Mean episode length: 70.66\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 54/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10560 \n",
            "                       Steps per second: 718 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.189s \n",
            "                        Mean value loss: 13.2267\n",
            "                    Mean surrogate loss: 0.0053\n",
            "                      Mean entropy loss: 2.9040\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0050\n",
            "                  Mean extrinsic reward: 71.21\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 71.21\n",
            "                    Mean episode length: 71.21\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 55/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10752 \n",
            "                       Steps per second: 729 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 15.9856\n",
            "                    Mean surrogate loss: 0.0152\n",
            "                      Mean entropy loss: 2.9050\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0096\n",
            "                  Mean extrinsic reward: 71.89\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 71.89\n",
            "                    Mean episode length: 71.89\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 56/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10944 \n",
            "                       Steps per second: 679 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.206s \n",
            "                        Mean value loss: 6.4009\n",
            "                    Mean surrogate loss: 0.0003\n",
            "                      Mean entropy loss: 2.9044\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0073\n",
            "                  Mean extrinsic reward: 71.89\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 71.89\n",
            "                    Mean episode length: 71.89\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 57/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11136 \n",
            "                       Steps per second: 709 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.194s \n",
            "                        Mean value loss: 16.9338\n",
            "                    Mean surrogate loss: -0.0024\n",
            "                      Mean entropy loss: 2.9019\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0084\n",
            "                  Mean extrinsic reward: 73.34\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 73.34\n",
            "                    Mean episode length: 73.34\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 58/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11328 \n",
            "                       Steps per second: 599 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.245s \n",
            "                        Mean value loss: 9.0992\n",
            "                    Mean surrogate loss: 0.0033\n",
            "                      Mean entropy loss: 2.9013\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: 0.0097\n",
            "                  Mean extrinsic reward: 73.38\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 73.38\n",
            "                    Mean episode length: 73.38\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.32s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 59/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11520 \n",
            "                       Steps per second: 499 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.281s \n",
            "                        Mean value loss: 4.5119\n",
            "                    Mean surrogate loss: 0.0115\n",
            "                      Mean entropy loss: 2.9015\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0147\n",
            "                  Mean extrinsic reward: 74.93\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 74.93\n",
            "                    Mean episode length: 74.93\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.38s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 60/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11712 \n",
            "                       Steps per second: 525 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.261s \n",
            "                        Mean value loss: 4.9504\n",
            "                    Mean surrogate loss: -0.0112\n",
            "                      Mean entropy loss: 2.9014\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0051\n",
            "                  Mean extrinsic reward: 74.93\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 74.93\n",
            "                    Mean episode length: 74.93\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.37s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 61/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11904 \n",
            "                       Steps per second: 553 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.251s \n",
            "                        Mean value loss: 8.4294\n",
            "                    Mean surrogate loss: -0.0022\n",
            "                      Mean entropy loss: 2.9007\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0066\n",
            "                  Mean extrinsic reward: 75.69\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 75.69\n",
            "                    Mean episode length: 75.69\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.35s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 62/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12096 \n",
            "                       Steps per second: 522 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.270s \n",
            "                        Mean value loss: 4.0807\n",
            "                    Mean surrogate loss: 0.0043\n",
            "                      Mean entropy loss: 2.9004\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: 0.0238\n",
            "                  Mean extrinsic reward: 76.81\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 76.81\n",
            "                    Mean episode length: 76.81\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.37s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 63/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12288 \n",
            "                       Steps per second: 479 \n",
            "                        Collection time: 0.116s \n",
            "                          Learning time: 0.284s \n",
            "                        Mean value loss: 4.9112\n",
            "                    Mean surrogate loss: -0.0163\n",
            "                      Mean entropy loss: 2.9024\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0170\n",
            "                  Mean extrinsic reward: 77.34\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 77.34\n",
            "                    Mean episode length: 77.34\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.40s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 64/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12480 \n",
            "                       Steps per second: 465 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.311s \n",
            "                        Mean value loss: 10.4944\n",
            "                    Mean surrogate loss: 0.0074\n",
            "                      Mean entropy loss: 2.9031\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0110\n",
            "                  Mean extrinsic reward: 77.86\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 77.86\n",
            "                    Mean episode length: 77.86\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.41s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 65/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12672 \n",
            "                       Steps per second: 469 \n",
            "                        Collection time: 0.133s \n",
            "                          Learning time: 0.276s \n",
            "                        Mean value loss: 15.0695\n",
            "                    Mean surrogate loss: 0.0070\n",
            "                      Mean entropy loss: 2.9069\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: 0.0121\n",
            "                  Mean extrinsic reward: 77.86\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 77.86\n",
            "                    Mean episode length: 77.86\n",
            "                  Mean action noise std: 1.04\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.41s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 66/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12864 \n",
            "                       Steps per second: 735 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.186s \n",
            "                        Mean value loss: 18.4955\n",
            "                    Mean surrogate loss: 0.0144\n",
            "                      Mean entropy loss: 2.9104\n",
            "                          Mean rnd loss: 0.0014\n",
            "                     Mean symmetry loss: 0.0176\n",
            "                  Mean extrinsic reward: 78.30\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 78.30\n",
            "                    Mean episode length: 78.30\n",
            "                  Mean action noise std: 1.04\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 67/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13056 \n",
            "                       Steps per second: 723 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.189s \n",
            "                        Mean value loss: 24.1443\n",
            "                    Mean surrogate loss: -0.0027\n",
            "                      Mean entropy loss: 2.9112\n",
            "                          Mean rnd loss: 0.0007\n",
            "                     Mean symmetry loss: 0.0121\n",
            "                  Mean extrinsic reward: 79.37\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 79.37\n",
            "                    Mean episode length: 79.37\n",
            "                  Mean action noise std: 1.04\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 68/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13248 \n",
            "                       Steps per second: 719 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 11.0650\n",
            "                    Mean surrogate loss: -0.0110\n",
            "                      Mean entropy loss: 2.9102\n",
            "                          Mean rnd loss: 0.0012\n",
            "                     Mean symmetry loss: 0.0085\n",
            "                  Mean extrinsic reward: 81.28\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 81.28\n",
            "                    Mean episode length: 81.28\n",
            "                  Mean action noise std: 1.04\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 69/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13440 \n",
            "                       Steps per second: 672 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.207s \n",
            "                        Mean value loss: 14.9792\n",
            "                    Mean surrogate loss: -0.0020\n",
            "                      Mean entropy loss: 2.9064\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0036\n",
            "                  Mean extrinsic reward: 81.28\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 81.28\n",
            "                    Mean episode length: 81.28\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 70/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13632 \n",
            "                       Steps per second: 715 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.193s \n",
            "                        Mean value loss: 37.0984\n",
            "                    Mean surrogate loss: 0.0254\n",
            "                      Mean entropy loss: 2.9062\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: 0.0241\n",
            "                  Mean extrinsic reward: 81.28\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 81.28\n",
            "                    Mean episode length: 81.28\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 71/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13824 \n",
            "                       Steps per second: 708 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.192s \n",
            "                        Mean value loss: 35.7898\n",
            "                    Mean surrogate loss: 0.0127\n",
            "                      Mean entropy loss: 2.9061\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0268\n",
            "                  Mean extrinsic reward: 83.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 83.07\n",
            "                    Mean episode length: 83.07\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 72/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14016 \n",
            "                       Steps per second: 728 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.189s \n",
            "                        Mean value loss: 20.1472\n",
            "                    Mean surrogate loss: 0.0171\n",
            "                      Mean entropy loss: 2.9056\n",
            "                          Mean rnd loss: 0.0009\n",
            "                     Mean symmetry loss: 0.0109\n",
            "                  Mean extrinsic reward: 83.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 83.07\n",
            "                    Mean episode length: 83.07\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 73/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14208 \n",
            "                       Steps per second: 694 \n",
            "                        Collection time: 0.091s \n",
            "                          Learning time: 0.186s \n",
            "                        Mean value loss: 31.6821\n",
            "                    Mean surrogate loss: 0.0100\n",
            "                      Mean entropy loss: 2.9043\n",
            "                          Mean rnd loss: 0.0013\n",
            "                     Mean symmetry loss: 0.0379\n",
            "                  Mean extrinsic reward: 83.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 83.07\n",
            "                    Mean episode length: 83.07\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 74/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14400 \n",
            "                       Steps per second: 729 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 64.0978\n",
            "                    Mean surrogate loss: 0.0008\n",
            "                      Mean entropy loss: 2.9045\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: 0.0209\n",
            "                  Mean extrinsic reward: 84.56\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 84.56\n",
            "                    Mean episode length: 84.56\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 75/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14592 \n",
            "                       Steps per second: 737 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.186s \n",
            "                        Mean value loss: 15.9997\n",
            "                    Mean surrogate loss: 0.0124\n",
            "                      Mean entropy loss: 2.9045\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: 0.0109\n",
            "                  Mean extrinsic reward: 85.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 85.13\n",
            "                    Mean episode length: 85.13\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 76/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14784 \n",
            "                       Steps per second: 677 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.208s \n",
            "                        Mean value loss: 12.5667\n",
            "                    Mean surrogate loss: -0.0061\n",
            "                      Mean entropy loss: 2.9033\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: 0.0078\n",
            "                  Mean extrinsic reward: 87.05\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 87.05\n",
            "                    Mean episode length: 87.05\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 77/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14976 \n",
            "                       Steps per second: 720 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 26.5314\n",
            "                    Mean surrogate loss: 0.0828\n",
            "                      Mean entropy loss: 2.8973\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0653\n",
            "                  Mean extrinsic reward: 87.05\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 87.05\n",
            "                    Mean episode length: 87.05\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 78/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15168 \n",
            "                       Steps per second: 636 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.223s \n",
            "                        Mean value loss: 46.5608\n",
            "                    Mean surrogate loss: 0.0397\n",
            "                      Mean entropy loss: 2.8962\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0235\n",
            "                  Mean extrinsic reward: 88.56\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 88.56\n",
            "                    Mean episode length: 88.56\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 79/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15360 \n",
            "                       Steps per second: 704 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.192s \n",
            "                        Mean value loss: 14.6013\n",
            "                    Mean surrogate loss: 0.0051\n",
            "                      Mean entropy loss: 2.8960\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: 0.0170\n",
            "                  Mean extrinsic reward: 88.56\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 88.56\n",
            "                    Mean episode length: 88.56\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 80/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15552 \n",
            "                       Steps per second: 676 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.206s \n",
            "                        Mean value loss: 55.5762\n",
            "                    Mean surrogate loss: 0.0049\n",
            "                      Mean entropy loss: 2.8950\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0119\n",
            "                  Mean extrinsic reward: 89.98\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 89.98\n",
            "                    Mean episode length: 89.98\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 81/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15744 \n",
            "                       Steps per second: 750 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.182s \n",
            "                        Mean value loss: 34.3406\n",
            "                    Mean surrogate loss: 0.0386\n",
            "                      Mean entropy loss: 2.8951\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: 0.0352\n",
            "                  Mean extrinsic reward: 91.67\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 91.67\n",
            "                    Mean episode length: 91.67\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 82/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15936 \n",
            "                       Steps per second: 715 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.191s \n",
            "                        Mean value loss: 158.6119\n",
            "                    Mean surrogate loss: -0.0078\n",
            "                      Mean entropy loss: 2.8950\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0267\n",
            "                  Mean extrinsic reward: 93.06\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 93.06\n",
            "                    Mean episode length: 93.06\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 83/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16128 \n",
            "                       Steps per second: 737 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.186s \n",
            "                        Mean value loss: 123.3860\n",
            "                    Mean surrogate loss: 0.0089\n",
            "                      Mean entropy loss: 2.8951\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0241\n",
            "                  Mean extrinsic reward: 96.34\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 96.34\n",
            "                    Mean episode length: 96.34\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 84/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16320 \n",
            "                       Steps per second: 695 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.199s \n",
            "                        Mean value loss: 39.6637\n",
            "                    Mean surrogate loss: -0.0092\n",
            "                      Mean entropy loss: 2.8946\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0088\n",
            "                  Mean extrinsic reward: 99.24\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 99.24\n",
            "                    Mean episode length: 99.24\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 85/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16512 \n",
            "                       Steps per second: 730 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 23.3281\n",
            "                    Mean surrogate loss: -0.0086\n",
            "                      Mean entropy loss: 2.8937\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0101\n",
            "                  Mean extrinsic reward: 99.24\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 99.24\n",
            "                    Mean episode length: 99.24\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 86/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16704 \n",
            "                       Steps per second: 743 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.183s \n",
            "                        Mean value loss: 176.5205\n",
            "                    Mean surrogate loss: 0.0213\n",
            "                      Mean entropy loss: 2.8927\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0279\n",
            "                  Mean extrinsic reward: 101.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 101.13\n",
            "                    Mean episode length: 101.13\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 87/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16896 \n",
            "                       Steps per second: 729 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 14.3774\n",
            "                    Mean surrogate loss: 0.0011\n",
            "                      Mean entropy loss: 2.8927\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0321\n",
            "                  Mean extrinsic reward: 101.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 101.13\n",
            "                    Mean episode length: 101.13\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 88/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17088 \n",
            "                       Steps per second: 670 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.204s \n",
            "                        Mean value loss: 20.6033\n",
            "                    Mean surrogate loss: -0.0156\n",
            "                      Mean entropy loss: 2.8921\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0358\n",
            "                  Mean extrinsic reward: 101.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 101.13\n",
            "                    Mean episode length: 101.13\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 89/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17280 \n",
            "                       Steps per second: 734 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.184s \n",
            "                        Mean value loss: 20.5775\n",
            "                    Mean surrogate loss: 0.0027\n",
            "                      Mean entropy loss: 2.8921\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0138\n",
            "                  Mean extrinsic reward: 101.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 101.13\n",
            "                    Mean episode length: 101.13\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 90/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17472 \n",
            "                       Steps per second: 741 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.185s \n",
            "                        Mean value loss: 113.9557\n",
            "                    Mean surrogate loss: -0.0024\n",
            "                      Mean entropy loss: 2.8924\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0521\n",
            "                  Mean extrinsic reward: 103.22\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 103.22\n",
            "                    Mean episode length: 103.22\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 91/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17664 \n",
            "                       Steps per second: 712 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.193s \n",
            "                        Mean value loss: 15.7010\n",
            "                    Mean surrogate loss: 0.0096\n",
            "                      Mean entropy loss: 2.8925\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0459\n",
            "                  Mean extrinsic reward: 106.54\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 106.54\n",
            "                    Mean episode length: 106.54\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 92/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17856 \n",
            "                       Steps per second: 699 \n",
            "                        Collection time: 0.088s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 11.3361\n",
            "                    Mean surrogate loss: -0.0025\n",
            "                      Mean entropy loss: 2.8928\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0414\n",
            "                  Mean extrinsic reward: 107.51\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 107.51\n",
            "                    Mean episode length: 107.51\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 93/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18048 \n",
            "                       Steps per second: 708 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.195s \n",
            "                        Mean value loss: 13.8874\n",
            "                    Mean surrogate loss: -0.0015\n",
            "                      Mean entropy loss: 2.8930\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0258\n",
            "                  Mean extrinsic reward: 107.51\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 107.51\n",
            "                    Mean episode length: 107.51\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 94/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18240 \n",
            "                       Steps per second: 673 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.203s \n",
            "                        Mean value loss: 9.2750\n",
            "                    Mean surrogate loss: -0.0013\n",
            "                      Mean entropy loss: 2.8927\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0105\n",
            "                  Mean extrinsic reward: 107.51\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 107.51\n",
            "                    Mean episode length: 107.51\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 95/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18432 \n",
            "                       Steps per second: 685 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.202s \n",
            "                        Mean value loss: 10.3399\n",
            "                    Mean surrogate loss: 0.0104\n",
            "                      Mean entropy loss: 2.8934\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0300\n",
            "                  Mean extrinsic reward: 107.51\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 107.51\n",
            "                    Mean episode length: 107.51\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 96/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18624 \n",
            "                       Steps per second: 712 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.194s \n",
            "                        Mean value loss: 6.1580\n",
            "                    Mean surrogate loss: -0.0099\n",
            "                      Mean entropy loss: 2.8940\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0182\n",
            "                  Mean extrinsic reward: 107.51\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 107.51\n",
            "                    Mean episode length: 107.51\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 97/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18816 \n",
            "                       Steps per second: 697 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.197s \n",
            "                        Mean value loss: 11.0349\n",
            "                    Mean surrogate loss: 0.0062\n",
            "                      Mean entropy loss: 2.8939\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0139\n",
            "                  Mean extrinsic reward: 107.51\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 107.51\n",
            "                    Mean episode length: 107.51\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 98/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 19008 \n",
            "                       Steps per second: 685 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.203s \n",
            "                        Mean value loss: 10.4056\n",
            "                    Mean surrogate loss: 0.0144\n",
            "                      Mean entropy loss: 2.8928\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0722\n",
            "                  Mean extrinsic reward: 107.51\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 107.51\n",
            "                    Mean episode length: 107.51\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 99/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 19200 \n",
            "                       Steps per second: 695 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.200s \n",
            "                        Mean value loss: 13.0662\n",
            "                    Mean surrogate loss: 0.0035\n",
            "                      Mean entropy loss: 2.8926\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0279\n",
            "                  Mean extrinsic reward: 107.51\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 107.51\n",
            "                    Mean episode length: 107.51\n",
            "                  Mean action noise std: 1.03\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:00:00\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "from types import SimpleNamespace\n",
        "from rsl_rl.runners import OnPolicyRunner\n",
        "#import cartpole_symmetry\n",
        "\n",
        "yaml_config_string = \"\"\"\n",
        "runner:\n",
        "  class_name: OnPolicyRunner\n",
        "  # General\n",
        "  num_steps_per_env: 24  # Number of steps per environment per iteration\n",
        "  max_iterations: 100    # Number of policy updates\n",
        "  seed: 1\n",
        "  # Observations\n",
        "  obs_groups: {\"policy\": [\"policy\"], \"critic\": [\"policy\", \"privileged\"]}  # Maps observation groups to sets. See `vec_env.py` for more information\n",
        "  # Logging parameters\n",
        "  save_interval: 50  # Check for potential saves every `save_interval` iterations\n",
        "  experiment_name: walking_experiment\n",
        "  run_name: \"\"\n",
        "  # Logging writer\n",
        "  logger: tensorboard  # tensorboard, neptune, wandb\n",
        "  neptune_project: legged_gym\n",
        "  wandb_project: legged_gym\n",
        "\n",
        "  # Policy\n",
        "  policy:\n",
        "    class_name: ActorCritic\n",
        "    activation: elu\n",
        "    actor_obs_normalization: false\n",
        "    critic_obs_normalization: false\n",
        "    actor_hidden_dims: [256, 256, 256]\n",
        "    critic_hidden_dims: [256, 256, 256]\n",
        "    init_noise_std: 1.0\n",
        "    noise_std_type: \"scalar\"  # 'scalar' or 'log'\n",
        "    state_dependent_std: false\n",
        "\n",
        "  # Algorithm\n",
        "  algorithm:\n",
        "    class_name: PPO\n",
        "    # Training\n",
        "    learning_rate: 0.001\n",
        "    num_learning_epochs: 5\n",
        "    num_mini_batches: 4  # mini batch size = num_envs * num_steps / num_mini_batches\n",
        "    schedule: adaptive  # adaptive, fixed\n",
        "    # Value function\n",
        "    value_loss_coef: 1.0\n",
        "    clip_param: 0.2\n",
        "    use_clipped_value_loss: true\n",
        "    # Surrogate loss\n",
        "    desired_kl: 0.01\n",
        "    entropy_coef: 0.01\n",
        "    gamma: 0.99\n",
        "    lam: 0.95\n",
        "    max_grad_norm: 1.0\n",
        "    # Miscellaneous\n",
        "    normalize_advantage_per_mini_batch: false\n",
        "\n",
        "    # Random network distillation\n",
        "    rnd_cfg:\n",
        "      weight: 0.0  # Initial weight of the RND reward\n",
        "      weight_schedule: null  # This is a dictionary with a required key called \"mode\". Please check the RND module for more information\n",
        "      reward_normalization: false  # Whether to normalize RND reward\n",
        "      # Learning parameters\n",
        "      learning_rate: 0.001  # Learning rate for RND\n",
        "      # Network parameters\n",
        "      num_outputs: 1  # Number of outputs of RND network. Note: if -1, then the network will use dimensions of the observation\n",
        "      predictor_hidden_dims: [-1]  # Hidden dimensions of predictor network\n",
        "      target_hidden_dims: [-1]  # Hidden dimensions of target network\n",
        "\n",
        "    # Symmetry augmentation\n",
        "    symmetry_cfg:\n",
        "      use_data_augmentation: true  # This adds symmetric trajectories to the batch\n",
        "      use_mirror_loss: false  # This adds symmetry loss term to the loss function\n",
        "      data_augmentation_func: \"rsl_rl.modules.cartpole_symmetry_utils:cartpole_symmetry\"  # String containing the module and function name to import\n",
        "      # Example: \"legged_gym.envs.locomotion.anymal_c.symmetry:get_symmetric_states\"\n",
        "      mirror_loss_coeff: 0.0  # Coefficient for symmetry loss term. If 0, no symmetry loss is used\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_train_cfg():\n",
        "    raw = yaml.safe_load(yaml_config_string)\n",
        "    # Optional: turn top-level dict into an object-like cfg\n",
        "    return SimpleNamespace(**raw)\n",
        "import os\n",
        "\n",
        "log_dir = \"./logs_cartpole\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "# Usage:\n",
        "train_cfg = get_train_cfg()\n",
        "print(train_cfg.runner[\"class_name\"])   # 'OnPolicyRunner'\n",
        "# later:\n",
        "env = CartPoleVecEnv(num_envs=8, device=\"cuda:0\")\n",
        "model_symmetry = GroupLatent(num_features=4, num_generators=6).to(device)\n",
        "model_symmetry.load_state_dict(torch.load(\"model_symmetry_cartpole.pt\"))\n",
        "env.model_symmetry = model_symmetry\n",
        "runner = OnPolicyRunner(env, train_cfg.runner, log_dir=log_dir)\n",
        "runner.learn(num_learning_iterations=train_cfg.runner[\"max_iterations\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iECCFrU9Ijd7",
        "outputId": "214b5797-6e63-4056-b8da-960bddf68df2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OnPolicyRunner\n",
            "--------------------------------------------------------------------------------\n",
            "Resolved observation sets: \n",
            "\t policy :  ['policy']\n",
            "\t critic :  ['policy', 'privileged']\n",
            "\t rnd_state :  ['policy']\n",
            "--------------------------------------------------------------------------------\n",
            "Actor MLP: MLP(\n",
            "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n",
            "Critic MLP: MLP(\n",
            "  (0): Linear(in_features=8, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/rsl_rl/rsl_rl/utils/utils.py:273: UserWarning: The observation configuration dictionary 'obs_groups' must contain the 'rnd_state' key. As the configuration for 'rnd_state' is missing, the observations from the 'policy' set are used. Consider adding the 'rnd_state' key to the 'obs_groups' dictionary for clarity. This behavior will be removed in a future version.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 0/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 192 \n",
            "                       Steps per second: 648 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.214s \n",
            "                        Mean value loss: 50.5214\n",
            "                    Mean surrogate loss: 0.1993\n",
            "                      Mean entropy loss: 2.8373\n",
            "                          Mean rnd loss: 1.0046\n",
            "                     Mean symmetry loss: 0.0904\n",
            "                  Mean extrinsic reward: 15.86\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 15.86\n",
            "                    Mean episode length: 15.86\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:00\n",
            "                                    ETA: 00:00:29\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 1/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 384 \n",
            "                       Steps per second: 727 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.186s \n",
            "                        Mean value loss: 41.9267\n",
            "                    Mean surrogate loss: -0.0047\n",
            "                      Mean entropy loss: 2.8375\n",
            "                          Mean rnd loss: 1.0377\n",
            "                     Mean symmetry loss: 0.0148\n",
            "                  Mean extrinsic reward: 17.12\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 17.12\n",
            "                    Mean episode length: 17.12\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:00\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 2/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 576 \n",
            "                       Steps per second: 676 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.208s \n",
            "                        Mean value loss: 36.8005\n",
            "                    Mean surrogate loss: -0.0008\n",
            "                      Mean entropy loss: 2.8378\n",
            "                          Mean rnd loss: 0.9328\n",
            "                     Mean symmetry loss: 0.0041\n",
            "                  Mean extrinsic reward: 19.04\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 19.04\n",
            "                    Mean episode length: 19.04\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:00\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 3/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 768 \n",
            "                       Steps per second: 614 \n",
            "                        Collection time: 0.089s \n",
            "                          Learning time: 0.224s \n",
            "                        Mean value loss: 46.4374\n",
            "                    Mean surrogate loss: 0.0072\n",
            "                      Mean entropy loss: 2.8344\n",
            "                          Mean rnd loss: 0.8045\n",
            "                     Mean symmetry loss: 0.0210\n",
            "                  Mean extrinsic reward: 19.43\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 19.43\n",
            "                    Mean episode length: 19.43\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 4/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 960 \n",
            "                       Steps per second: 676 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.186s \n",
            "                        Mean value loss: 48.2561\n",
            "                    Mean surrogate loss: 0.0079\n",
            "                      Mean entropy loss: 2.8328\n",
            "                          Mean rnd loss: 0.7094\n",
            "                     Mean symmetry loss: 0.0637\n",
            "                  Mean extrinsic reward: 22.16\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 22.16\n",
            "                    Mean episode length: 22.16\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 5/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1152 \n",
            "                       Steps per second: 713 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.192s \n",
            "                        Mean value loss: 51.7137\n",
            "                    Mean surrogate loss: -0.0083\n",
            "                      Mean entropy loss: 2.8328\n",
            "                          Mean rnd loss: 0.7392\n",
            "                     Mean symmetry loss: 0.0105\n",
            "                  Mean extrinsic reward: 23.24\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 23.24\n",
            "                    Mean episode length: 23.24\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:00:26\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 6/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1344 \n",
            "                       Steps per second: 725 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 48.6851\n",
            "                    Mean surrogate loss: 0.0160\n",
            "                      Mean entropy loss: 2.8305\n",
            "                          Mean rnd loss: 0.6015\n",
            "                     Mean symmetry loss: 0.0712\n",
            "                  Mean extrinsic reward: 26.02\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 26.02\n",
            "                    Mean episode length: 26.02\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:00:26\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 7/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1536 \n",
            "                       Steps per second: 688 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.204s \n",
            "                        Mean value loss: 56.0446\n",
            "                    Mean surrogate loss: -0.0084\n",
            "                      Mean entropy loss: 2.8299\n",
            "                          Mean rnd loss: 0.5541\n",
            "                     Mean symmetry loss: 0.0115\n",
            "                  Mean extrinsic reward: 25.83\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 25.83\n",
            "                    Mean episode length: 25.83\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 8/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1728 \n",
            "                       Steps per second: 709 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 46.2013\n",
            "                    Mean surrogate loss: 0.0019\n",
            "                      Mean entropy loss: 2.8296\n",
            "                          Mean rnd loss: 0.4502\n",
            "                     Mean symmetry loss: 0.0196\n",
            "                  Mean extrinsic reward: 27.12\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 27.12\n",
            "                    Mean episode length: 27.12\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 9/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1920 \n",
            "                       Steps per second: 731 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 49.4015\n",
            "                    Mean surrogate loss: 0.0028\n",
            "                      Mean entropy loss: 2.8278\n",
            "                          Mean rnd loss: 0.3236\n",
            "                     Mean symmetry loss: 0.0084\n",
            "                  Mean extrinsic reward: 29.39\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 29.39\n",
            "                    Mean episode length: 29.39\n",
            "                  Mean action noise std: 0.99\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 10/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2112 \n",
            "                       Steps per second: 725 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 60.5486\n",
            "                    Mean surrogate loss: 0.0059\n",
            "                      Mean entropy loss: 2.8267\n",
            "                          Mean rnd loss: 0.3118\n",
            "                     Mean symmetry loss: 0.0029\n",
            "                  Mean extrinsic reward: 29.74\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 29.74\n",
            "                    Mean episode length: 29.74\n",
            "                  Mean action noise std: 0.99\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 11/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2304 \n",
            "                       Steps per second: 655 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.217s \n",
            "                        Mean value loss: 59.3853\n",
            "                    Mean surrogate loss: 0.0031\n",
            "                      Mean entropy loss: 2.8275\n",
            "                          Mean rnd loss: 0.3570\n",
            "                     Mean symmetry loss: 0.0219\n",
            "                  Mean extrinsic reward: 30.12\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 30.12\n",
            "                    Mean episode length: 30.12\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 12/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2496 \n",
            "                       Steps per second: 676 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.202s \n",
            "                        Mean value loss: 43.3354\n",
            "                    Mean surrogate loss: 0.0005\n",
            "                      Mean entropy loss: 2.8281\n",
            "                          Mean rnd loss: 0.2579\n",
            "                     Mean symmetry loss: 0.0056\n",
            "                  Mean extrinsic reward: 35.38\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 35.38\n",
            "                    Mean episode length: 35.38\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 13/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2688 \n",
            "                       Steps per second: 675 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.204s \n",
            "                        Mean value loss: 52.2057\n",
            "                    Mean surrogate loss: 0.0008\n",
            "                      Mean entropy loss: 2.8270\n",
            "                          Mean rnd loss: 0.1524\n",
            "                     Mean symmetry loss: 0.0122\n",
            "                  Mean extrinsic reward: 35.76\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 35.76\n",
            "                    Mean episode length: 35.76\n",
            "                  Mean action noise std: 0.99\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 14/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2880 \n",
            "                       Steps per second: 674 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.205s \n",
            "                        Mean value loss: 47.3149\n",
            "                    Mean surrogate loss: 0.0066\n",
            "                      Mean entropy loss: 2.8284\n",
            "                          Mean rnd loss: 0.1295\n",
            "                     Mean symmetry loss: 0.0341\n",
            "                  Mean extrinsic reward: 36.97\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 36.97\n",
            "                    Mean episode length: 36.97\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 15/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3072 \n",
            "                       Steps per second: 667 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.207s \n",
            "                        Mean value loss: 37.4330\n",
            "                    Mean surrogate loss: 0.0060\n",
            "                      Mean entropy loss: 2.8295\n",
            "                          Mean rnd loss: 0.0990\n",
            "                     Mean symmetry loss: 0.0217\n",
            "                  Mean extrinsic reward: 38.04\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 38.04\n",
            "                    Mean episode length: 38.04\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 16/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3264 \n",
            "                       Steps per second: 707 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.192s \n",
            "                        Mean value loss: 43.7775\n",
            "                    Mean surrogate loss: -0.0104\n",
            "                      Mean entropy loss: 2.8311\n",
            "                          Mean rnd loss: 0.0737\n",
            "                     Mean symmetry loss: 0.0089\n",
            "                  Mean extrinsic reward: 38.55\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 38.55\n",
            "                    Mean episode length: 38.55\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 17/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3456 \n",
            "                       Steps per second: 717 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.191s \n",
            "                        Mean value loss: 41.9600\n",
            "                    Mean surrogate loss: -0.0058\n",
            "                      Mean entropy loss: 2.8314\n",
            "                          Mean rnd loss: 0.0799\n",
            "                     Mean symmetry loss: 0.0048\n",
            "                  Mean extrinsic reward: 38.79\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 38.79\n",
            "                    Mean episode length: 38.79\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 18/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3648 \n",
            "                       Steps per second: 693 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.199s \n",
            "                        Mean value loss: 55.9123\n",
            "                    Mean surrogate loss: 0.0101\n",
            "                      Mean entropy loss: 2.8304\n",
            "                          Mean rnd loss: 0.0652\n",
            "                     Mean symmetry loss: 0.0478\n",
            "                  Mean extrinsic reward: 38.68\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 38.68\n",
            "                    Mean episode length: 38.68\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 19/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3840 \n",
            "                       Steps per second: 660 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.196s \n",
            "                        Mean value loss: 58.3320\n",
            "                    Mean surrogate loss: 0.0293\n",
            "                      Mean entropy loss: 2.8298\n",
            "                          Mean rnd loss: 0.0344\n",
            "                     Mean symmetry loss: 0.0507\n",
            "                  Mean extrinsic reward: 38.68\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 38.68\n",
            "                    Mean episode length: 38.68\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 20/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4032 \n",
            "                       Steps per second: 707 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.195s \n",
            "                        Mean value loss: 61.7120\n",
            "                    Mean surrogate loss: 0.0173\n",
            "                      Mean entropy loss: 2.8301\n",
            "                          Mean rnd loss: 0.0370\n",
            "                     Mean symmetry loss: 0.0199\n",
            "                  Mean extrinsic reward: 38.68\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 38.68\n",
            "                    Mean episode length: 38.68\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 21/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4224 \n",
            "                       Steps per second: 705 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.193s \n",
            "                        Mean value loss: 43.7879\n",
            "                    Mean surrogate loss: 0.0216\n",
            "                      Mean entropy loss: 2.8338\n",
            "                          Mean rnd loss: 0.0257\n",
            "                     Mean symmetry loss: 0.0723\n",
            "                  Mean extrinsic reward: 41.26\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 41.26\n",
            "                    Mean episode length: 41.26\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 22/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4416 \n",
            "                       Steps per second: 686 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.202s \n",
            "                        Mean value loss: 37.6076\n",
            "                    Mean surrogate loss: 0.0026\n",
            "                      Mean entropy loss: 2.8344\n",
            "                          Mean rnd loss: 0.0182\n",
            "                     Mean symmetry loss: 0.0555\n",
            "                  Mean extrinsic reward: 43.39\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 43.39\n",
            "                    Mean episode length: 43.39\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 23/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4608 \n",
            "                       Steps per second: 679 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.205s \n",
            "                        Mean value loss: 43.2887\n",
            "                    Mean surrogate loss: 0.0189\n",
            "                      Mean entropy loss: 2.8339\n",
            "                          Mean rnd loss: 0.0126\n",
            "                     Mean symmetry loss: 0.0160\n",
            "                  Mean extrinsic reward: 43.60\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 43.60\n",
            "                    Mean episode length: 43.60\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 24/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4800 \n",
            "                       Steps per second: 501 \n",
            "                        Collection time: 0.115s \n",
            "                          Learning time: 0.268s \n",
            "                        Mean value loss: 48.0781\n",
            "                    Mean surrogate loss: -0.0023\n",
            "                      Mean entropy loss: 2.8341\n",
            "                          Mean rnd loss: 0.0060\n",
            "                     Mean symmetry loss: 0.0168\n",
            "                  Mean extrinsic reward: 44.63\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 44.63\n",
            "                    Mean episode length: 44.63\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.38s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 25/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4992 \n",
            "                       Steps per second: 487 \n",
            "                        Collection time: 0.110s \n",
            "                          Learning time: 0.284s \n",
            "                        Mean value loss: 51.2458\n",
            "                    Mean surrogate loss: 0.0007\n",
            "                      Mean entropy loss: 2.8380\n",
            "                          Mean rnd loss: 0.0044\n",
            "                     Mean symmetry loss: 0.0342\n",
            "                  Mean extrinsic reward: 44.63\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 44.63\n",
            "                    Mean episode length: 44.63\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.39s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 26/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5184 \n",
            "                       Steps per second: 526 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.263s \n",
            "                        Mean value loss: 46.3969\n",
            "                    Mean surrogate loss: 0.0395\n",
            "                      Mean entropy loss: 2.8454\n",
            "                          Mean rnd loss: 0.0048\n",
            "                     Mean symmetry loss: 0.1330\n",
            "                  Mean extrinsic reward: 44.63\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 44.63\n",
            "                    Mean episode length: 44.63\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.36s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 27/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5376 \n",
            "                       Steps per second: 552 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.254s \n",
            "                        Mean value loss: 58.7872\n",
            "                    Mean surrogate loss: 0.0745\n",
            "                      Mean entropy loss: 2.8488\n",
            "                          Mean rnd loss: 0.0041\n",
            "                     Mean symmetry loss: 0.5110\n",
            "                  Mean extrinsic reward: 47.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 47.07\n",
            "                    Mean episode length: 47.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.35s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 28/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5568 \n",
            "                       Steps per second: 432 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.338s \n",
            "                        Mean value loss: 54.9584\n",
            "                    Mean surrogate loss: 0.1364\n",
            "                      Mean entropy loss: 2.8487\n",
            "                          Mean rnd loss: 0.0026\n",
            "                     Mean symmetry loss: 0.3759\n",
            "                  Mean extrinsic reward: 48.00\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 48.00\n",
            "                    Mean episode length: 48.00\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.44s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 29/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5760 \n",
            "                       Steps per second: 469 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.305s \n",
            "                        Mean value loss: 54.7568\n",
            "                    Mean surrogate loss: 0.0399\n",
            "                      Mean entropy loss: 2.8479\n",
            "                          Mean rnd loss: 0.0032\n",
            "                     Mean symmetry loss: 0.0400\n",
            "                  Mean extrinsic reward: 49.82\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 49.82\n",
            "                    Mean episode length: 49.82\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.41s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 30/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5952 \n",
            "                       Steps per second: 513 \n",
            "                        Collection time: 0.125s \n",
            "                          Learning time: 0.248s \n",
            "                        Mean value loss: 68.1360\n",
            "                    Mean surrogate loss: 0.0045\n",
            "                      Mean entropy loss: 2.8473\n",
            "                          Mean rnd loss: 0.0025\n",
            "                     Mean symmetry loss: 0.0025\n",
            "                  Mean extrinsic reward: 53.29\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 53.29\n",
            "                    Mean episode length: 53.29\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.37s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 31/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6144 \n",
            "                       Steps per second: 637 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.223s \n",
            "                        Mean value loss: 64.7784\n",
            "                    Mean surrogate loss: -0.0110\n",
            "                      Mean entropy loss: 2.8472\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: 0.0028\n",
            "                  Mean extrinsic reward: 56.08\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 56.08\n",
            "                    Mean episode length: 56.08\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 32/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6336 \n",
            "                       Steps per second: 661 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.214s \n",
            "                        Mean value loss: 49.6204\n",
            "                    Mean surrogate loss: -0.0020\n",
            "                      Mean entropy loss: 2.8470\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0074\n",
            "                  Mean extrinsic reward: 56.08\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 56.08\n",
            "                    Mean episode length: 56.08\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 33/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6528 \n",
            "                       Steps per second: 669 \n",
            "                        Collection time: 0.086s \n",
            "                          Learning time: 0.201s \n",
            "                        Mean value loss: 40.5865\n",
            "                    Mean surrogate loss: 0.0034\n",
            "                      Mean entropy loss: 2.8462\n",
            "                          Mean rnd loss: 0.0017\n",
            "                     Mean symmetry loss: 0.0155\n",
            "                  Mean extrinsic reward: 56.08\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 56.08\n",
            "                    Mean episode length: 56.08\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 34/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6720 \n",
            "                       Steps per second: 690 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.196s \n",
            "                        Mean value loss: 41.1104\n",
            "                    Mean surrogate loss: 0.0047\n",
            "                      Mean entropy loss: 2.8457\n",
            "                          Mean rnd loss: 0.0027\n",
            "                     Mean symmetry loss: 0.1101\n",
            "                  Mean extrinsic reward: 57.30\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 57.30\n",
            "                    Mean episode length: 57.30\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 35/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6912 \n",
            "                       Steps per second: 642 \n",
            "                        Collection time: 0.093s \n",
            "                          Learning time: 0.205s \n",
            "                        Mean value loss: 60.0751\n",
            "                    Mean surrogate loss: 0.0151\n",
            "                      Mean entropy loss: 2.8461\n",
            "                          Mean rnd loss: 0.0029\n",
            "                     Mean symmetry loss: 0.0639\n",
            "                  Mean extrinsic reward: 58.22\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 58.22\n",
            "                    Mean episode length: 58.22\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 36/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7104 \n",
            "                       Steps per second: 661 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.212s \n",
            "                        Mean value loss: 53.8100\n",
            "                    Mean surrogate loss: -0.0140\n",
            "                      Mean entropy loss: 2.8464\n",
            "                          Mean rnd loss: 0.0009\n",
            "                     Mean symmetry loss: 0.0048\n",
            "                  Mean extrinsic reward: 61.68\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 61.68\n",
            "                    Mean episode length: 61.68\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 37/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7296 \n",
            "                       Steps per second: 633 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.223s \n",
            "                        Mean value loss: 98.3080\n",
            "                    Mean surrogate loss: -0.0082\n",
            "                      Mean entropy loss: 2.8468\n",
            "                          Mean rnd loss: 0.0032\n",
            "                     Mean symmetry loss: 0.0020\n",
            "                  Mean extrinsic reward: 68.01\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 68.01\n",
            "                    Mean episode length: 68.01\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 38/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7488 \n",
            "                       Steps per second: 601 \n",
            "                        Collection time: 0.087s \n",
            "                          Learning time: 0.232s \n",
            "                        Mean value loss: 24.8612\n",
            "                    Mean surrogate loss: 0.0030\n",
            "                      Mean entropy loss: 2.8497\n",
            "                          Mean rnd loss: 0.0010\n",
            "                     Mean symmetry loss: 0.0045\n",
            "                  Mean extrinsic reward: 68.01\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 68.01\n",
            "                    Mean episode length: 68.01\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.32s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 39/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7680 \n",
            "                       Steps per second: 658 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.212s \n",
            "                        Mean value loss: 47.7954\n",
            "                    Mean surrogate loss: -0.0059\n",
            "                      Mean entropy loss: 2.8528\n",
            "                          Mean rnd loss: 0.0013\n",
            "                     Mean symmetry loss: 0.0223\n",
            "                  Mean extrinsic reward: 68.50\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 68.50\n",
            "                    Mean episode length: 68.50\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 40/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7872 \n",
            "                       Steps per second: 660 \n",
            "                        Collection time: 0.085s \n",
            "                          Learning time: 0.206s \n",
            "                        Mean value loss: 17.5425\n",
            "                    Mean surrogate loss: 0.0258\n",
            "                      Mean entropy loss: 2.8541\n",
            "                          Mean rnd loss: 0.0007\n",
            "                     Mean symmetry loss: 0.0237\n",
            "                  Mean extrinsic reward: 68.50\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 68.50\n",
            "                    Mean episode length: 68.50\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 41/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8064 \n",
            "                       Steps per second: 649 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.219s \n",
            "                        Mean value loss: 65.2433\n",
            "                    Mean surrogate loss: 0.0069\n",
            "                      Mean entropy loss: 2.8556\n",
            "                          Mean rnd loss: 0.0022\n",
            "                     Mean symmetry loss: 0.0576\n",
            "                  Mean extrinsic reward: 70.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 70.07\n",
            "                    Mean episode length: 70.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 42/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8256 \n",
            "                       Steps per second: 651 \n",
            "                        Collection time: 0.093s \n",
            "                          Learning time: 0.202s \n",
            "                        Mean value loss: 24.6492\n",
            "                    Mean surrogate loss: 0.0159\n",
            "                      Mean entropy loss: 2.8562\n",
            "                          Mean rnd loss: 0.0027\n",
            "                     Mean symmetry loss: 0.0840\n",
            "                  Mean extrinsic reward: 71.86\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 71.86\n",
            "                    Mean episode length: 71.86\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 43/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8448 \n",
            "                       Steps per second: 682 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.204s \n",
            "                        Mean value loss: 46.6239\n",
            "                    Mean surrogate loss: 0.0433\n",
            "                      Mean entropy loss: 2.8564\n",
            "                          Mean rnd loss: 0.0017\n",
            "                     Mean symmetry loss: 0.0318\n",
            "                  Mean extrinsic reward: 74.87\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 74.87\n",
            "                    Mean episode length: 74.87\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 44/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8640 \n",
            "                       Steps per second: 678 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.202s \n",
            "                        Mean value loss: 148.2298\n",
            "                    Mean surrogate loss: 0.0129\n",
            "                      Mean entropy loss: 2.8565\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0123\n",
            "                  Mean extrinsic reward: 78.01\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 78.01\n",
            "                    Mean episode length: 78.01\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 45/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8832 \n",
            "                       Steps per second: 653 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.215s \n",
            "                        Mean value loss: 26.4905\n",
            "                    Mean surrogate loss: -0.0144\n",
            "                      Mean entropy loss: 2.8566\n",
            "                          Mean rnd loss: 0.0008\n",
            "                     Mean symmetry loss: 0.0083\n",
            "                  Mean extrinsic reward: 78.01\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 78.01\n",
            "                    Mean episode length: 78.01\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 46/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9024 \n",
            "                       Steps per second: 681 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.205s \n",
            "                        Mean value loss: 135.5377\n",
            "                    Mean surrogate loss: 0.0043\n",
            "                      Mean entropy loss: 2.8558\n",
            "                          Mean rnd loss: 0.0011\n",
            "                     Mean symmetry loss: 0.0618\n",
            "                  Mean extrinsic reward: 79.47\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 79.47\n",
            "                    Mean episode length: 79.47\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 47/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9216 \n",
            "                       Steps per second: 696 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.197s \n",
            "                        Mean value loss: 102.1239\n",
            "                    Mean surrogate loss: -0.0040\n",
            "                      Mean entropy loss: 2.8555\n",
            "                          Mean rnd loss: 0.0016\n",
            "                     Mean symmetry loss: 0.0425\n",
            "                  Mean extrinsic reward: 80.22\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 80.22\n",
            "                    Mean episode length: 80.22\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 48/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9408 \n",
            "                       Steps per second: 698 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.198s \n",
            "                        Mean value loss: 97.6102\n",
            "                    Mean surrogate loss: -0.0093\n",
            "                      Mean entropy loss: 2.8549\n",
            "                          Mean rnd loss: 0.0015\n",
            "                     Mean symmetry loss: 0.0210\n",
            "                  Mean extrinsic reward: 81.24\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 81.24\n",
            "                    Mean episode length: 81.24\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 49/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9600 \n",
            "                       Steps per second: 649 \n",
            "                        Collection time: 0.087s \n",
            "                          Learning time: 0.208s \n",
            "                        Mean value loss: 24.5683\n",
            "                    Mean surrogate loss: 0.0074\n",
            "                      Mean entropy loss: 2.8547\n",
            "                          Mean rnd loss: 0.0008\n",
            "                     Mean symmetry loss: 0.0192\n",
            "                  Mean extrinsic reward: 81.24\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 81.24\n",
            "                    Mean episode length: 81.24\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 50/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9792 \n",
            "                       Steps per second: 699 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.199s \n",
            "                        Mean value loss: 14.2116\n",
            "                    Mean surrogate loss: -0.0112\n",
            "                      Mean entropy loss: 2.8545\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0171\n",
            "                  Mean extrinsic reward: 81.24\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 81.24\n",
            "                    Mean episode length: 81.24\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 51/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9984 \n",
            "                       Steps per second: 671 \n",
            "                        Collection time: 0.087s \n",
            "                          Learning time: 0.199s \n",
            "                        Mean value loss: 18.0097\n",
            "                    Mean surrogate loss: -0.0048\n",
            "                      Mean entropy loss: 2.8543\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0163\n",
            "                  Mean extrinsic reward: 81.24\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 81.24\n",
            "                    Mean episode length: 81.24\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 52/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10176 \n",
            "                       Steps per second: 670 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.211s \n",
            "                        Mean value loss: 18.8717\n",
            "                    Mean surrogate loss: 0.0019\n",
            "                      Mean entropy loss: 2.8557\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0187\n",
            "                  Mean extrinsic reward: 81.24\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 81.24\n",
            "                    Mean episode length: 81.24\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 53/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10368 \n",
            "                       Steps per second: 645 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.216s \n",
            "                        Mean value loss: 25.1956\n",
            "                    Mean surrogate loss: 0.0064\n",
            "                      Mean entropy loss: 2.8559\n",
            "                          Mean rnd loss: 0.0010\n",
            "                     Mean symmetry loss: 0.0111\n",
            "                  Mean extrinsic reward: 83.30\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 83.30\n",
            "                    Mean episode length: 83.30\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 54/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10560 \n",
            "                       Steps per second: 713 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 153.6832\n",
            "                    Mean surrogate loss: 0.0060\n",
            "                      Mean entropy loss: 2.8556\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: 0.0067\n",
            "                  Mean extrinsic reward: 87.06\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 87.06\n",
            "                    Mean episode length: 87.06\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 55/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10752 \n",
            "                       Steps per second: 702 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.198s \n",
            "                        Mean value loss: 12.7389\n",
            "                    Mean surrogate loss: -0.0027\n",
            "                      Mean entropy loss: 2.8565\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: 0.0026\n",
            "                  Mean extrinsic reward: 87.06\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 87.06\n",
            "                    Mean episode length: 87.06\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 56/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10944 \n",
            "                       Steps per second: 680 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.207s \n",
            "                        Mean value loss: 30.4888\n",
            "                    Mean surrogate loss: -0.0116\n",
            "                      Mean entropy loss: 2.8572\n",
            "                          Mean rnd loss: 0.0010\n",
            "                     Mean symmetry loss: 0.0024\n",
            "                  Mean extrinsic reward: 90.29\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 90.29\n",
            "                    Mean episode length: 90.29\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 57/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11136 \n",
            "                       Steps per second: 712 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.194s \n",
            "                        Mean value loss: 173.0098\n",
            "                    Mean surrogate loss: 0.0028\n",
            "                      Mean entropy loss: 2.8573\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0067\n",
            "                  Mean extrinsic reward: 94.19\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 94.19\n",
            "                    Mean episode length: 94.19\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 58/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11328 \n",
            "                       Steps per second: 721 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 167.4790\n",
            "                    Mean surrogate loss: 0.0008\n",
            "                      Mean entropy loss: 2.8576\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0148\n",
            "                  Mean extrinsic reward: 96.32\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 96.32\n",
            "                    Mean episode length: 96.32\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 59/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11520 \n",
            "                       Steps per second: 718 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.189s \n",
            "                        Mean value loss: 139.6804\n",
            "                    Mean surrogate loss: -0.0150\n",
            "                      Mean entropy loss: 2.8577\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0114\n",
            "                  Mean extrinsic reward: 98.75\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 98.75\n",
            "                    Mean episode length: 98.75\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 60/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11712 \n",
            "                       Steps per second: 676 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.208s \n",
            "                        Mean value loss: 6.3226\n",
            "                    Mean surrogate loss: -0.0096\n",
            "                      Mean entropy loss: 2.8577\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0114\n",
            "                  Mean extrinsic reward: 98.75\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 98.75\n",
            "                    Mean episode length: 98.75\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 61/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11904 \n",
            "                       Steps per second: 739 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.182s \n",
            "                        Mean value loss: 8.4707\n",
            "                    Mean surrogate loss: 0.0001\n",
            "                      Mean entropy loss: 2.8576\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0053\n",
            "                  Mean extrinsic reward: 98.75\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 98.75\n",
            "                    Mean episode length: 98.75\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 62/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12096 \n",
            "                       Steps per second: 678 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.205s \n",
            "                        Mean value loss: 196.3987\n",
            "                    Mean surrogate loss: -0.0003\n",
            "                      Mean entropy loss: 2.8576\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0064\n",
            "                  Mean extrinsic reward: 99.72\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 99.72\n",
            "                    Mean episode length: 99.72\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 63/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12288 \n",
            "                       Steps per second: 721 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 9.0003\n",
            "                    Mean surrogate loss: 0.0005\n",
            "                      Mean entropy loss: 2.8596\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0048\n",
            "                  Mean extrinsic reward: 99.72\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 99.72\n",
            "                    Mean episode length: 99.72\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 64/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12480 \n",
            "                       Steps per second: 704 \n",
            "                        Collection time: 0.087s \n",
            "                          Learning time: 0.186s \n",
            "                        Mean value loss: 9.8205\n",
            "                    Mean surrogate loss: 0.0165\n",
            "                      Mean entropy loss: 2.8623\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0165\n",
            "                  Mean extrinsic reward: 99.72\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 99.72\n",
            "                    Mean episode length: 99.72\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 65/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12672 \n",
            "                       Steps per second: 607 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.239s \n",
            "                        Mean value loss: 205.7485\n",
            "                    Mean surrogate loss: -0.0152\n",
            "                      Mean entropy loss: 2.8625\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0108\n",
            "                  Mean extrinsic reward: 100.95\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 100.95\n",
            "                    Mean episode length: 100.95\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.32s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 66/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12864 \n",
            "                       Steps per second: 515 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.268s \n",
            "                        Mean value loss: 6.3969\n",
            "                    Mean surrogate loss: -0.0106\n",
            "                      Mean entropy loss: 2.8625\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0141\n",
            "                  Mean extrinsic reward: 100.95\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 100.95\n",
            "                    Mean episode length: 100.95\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.37s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 67/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13056 \n",
            "                       Steps per second: 505 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.276s \n",
            "                        Mean value loss: 9.1023\n",
            "                    Mean surrogate loss: 0.0016\n",
            "                      Mean entropy loss: 2.8625\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0123\n",
            "                  Mean extrinsic reward: 100.95\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 100.95\n",
            "                    Mean episode length: 100.95\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.38s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 68/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13248 \n",
            "                       Steps per second: 516 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.273s \n",
            "                        Mean value loss: 14.0999\n",
            "                    Mean surrogate loss: -0.0051\n",
            "                      Mean entropy loss: 2.8626\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0206\n",
            "                  Mean extrinsic reward: 100.95\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 100.95\n",
            "                    Mean episode length: 100.95\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.37s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 69/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13440 \n",
            "                       Steps per second: 568 \n",
            "                        Collection time: 0.091s \n",
            "                          Learning time: 0.246s \n",
            "                        Mean value loss: 20.1168\n",
            "                    Mean surrogate loss: 0.0013\n",
            "                      Mean entropy loss: 2.8626\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0216\n",
            "                  Mean extrinsic reward: 104.31\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 104.31\n",
            "                    Mean episode length: 104.31\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.34s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 70/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13632 \n",
            "                       Steps per second: 496 \n",
            "                        Collection time: 0.109s \n",
            "                          Learning time: 0.277s \n",
            "                        Mean value loss: 12.4688\n",
            "                    Mean surrogate loss: -0.0027\n",
            "                      Mean entropy loss: 2.8628\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0155\n",
            "                  Mean extrinsic reward: 104.31\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 104.31\n",
            "                    Mean episode length: 104.31\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.39s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 71/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13824 \n",
            "                       Steps per second: 468 \n",
            "                        Collection time: 0.117s \n",
            "                          Learning time: 0.292s \n",
            "                        Mean value loss: 14.0029\n",
            "                    Mean surrogate loss: -0.0014\n",
            "                      Mean entropy loss: 2.8629\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: 0.0189\n",
            "                  Mean extrinsic reward: 104.31\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 104.31\n",
            "                    Mean episode length: 104.31\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.41s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 72/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14016 \n",
            "                       Steps per second: 517 \n",
            "                        Collection time: 0.123s \n",
            "                          Learning time: 0.248s \n",
            "                        Mean value loss: 9.4016\n",
            "                    Mean surrogate loss: 0.0102\n",
            "                      Mean entropy loss: 2.8628\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0249\n",
            "                  Mean extrinsic reward: 104.31\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 104.31\n",
            "                    Mean episode length: 104.31\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.37s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 73/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14208 \n",
            "                       Steps per second: 701 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.192s \n",
            "                        Mean value loss: 8.4800\n",
            "                    Mean surrogate loss: 0.0151\n",
            "                      Mean entropy loss: 2.8626\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0097\n",
            "                  Mean extrinsic reward: 104.31\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 104.31\n",
            "                    Mean episode length: 104.31\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 74/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14400 \n",
            "                       Steps per second: 715 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.186s \n",
            "                        Mean value loss: 461.0250\n",
            "                    Mean surrogate loss: -0.0006\n",
            "                      Mean entropy loss: 2.8623\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0039\n",
            "                  Mean extrinsic reward: 113.61\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 113.61\n",
            "                    Mean episode length: 113.61\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 75/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14592 \n",
            "                       Steps per second: 713 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.194s \n",
            "                        Mean value loss: 7.8525\n",
            "                    Mean surrogate loss: -0.0047\n",
            "                      Mean entropy loss: 2.8616\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0058\n",
            "                  Mean extrinsic reward: 113.61\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 113.61\n",
            "                    Mean episode length: 113.61\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 76/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14784 \n",
            "                       Steps per second: 692 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.202s \n",
            "                        Mean value loss: 67.8267\n",
            "                    Mean surrogate loss: -0.0094\n",
            "                      Mean entropy loss: 2.8611\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0514\n",
            "                  Mean extrinsic reward: 117.94\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 117.94\n",
            "                    Mean episode length: 117.94\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 77/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14976 \n",
            "                       Steps per second: 744 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.183s \n",
            "                        Mean value loss: 3.1440\n",
            "                    Mean surrogate loss: 0.0100\n",
            "                      Mean entropy loss: 2.8613\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0694\n",
            "                  Mean extrinsic reward: 117.94\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 117.94\n",
            "                    Mean episode length: 117.94\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 78/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15168 \n",
            "                       Steps per second: 740 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.185s \n",
            "                        Mean value loss: 246.0011\n",
            "                    Mean surrogate loss: 0.0066\n",
            "                      Mean entropy loss: 2.8620\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0524\n",
            "                  Mean extrinsic reward: 121.96\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 121.96\n",
            "                    Mean episode length: 121.96\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 79/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15360 \n",
            "                       Steps per second: 653 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.217s \n",
            "                        Mean value loss: 118.1223\n",
            "                    Mean surrogate loss: 0.0059\n",
            "                      Mean entropy loss: 2.8630\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.1615\n",
            "                  Mean extrinsic reward: 129.51\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 129.51\n",
            "                    Mean episode length: 129.51\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 80/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15552 \n",
            "                       Steps per second: 701 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.199s \n",
            "                        Mean value loss: 3.4224\n",
            "                    Mean surrogate loss: 0.0359\n",
            "                      Mean entropy loss: 2.8644\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0586\n",
            "                  Mean extrinsic reward: 129.51\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 129.51\n",
            "                    Mean episode length: 129.51\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 81/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15744 \n",
            "                       Steps per second: 727 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.189s \n",
            "                        Mean value loss: 386.4748\n",
            "                    Mean surrogate loss: -0.0069\n",
            "                      Mean entropy loss: 2.8645\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0086\n",
            "                  Mean extrinsic reward: 133.47\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 133.47\n",
            "                    Mean episode length: 133.47\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 82/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15936 \n",
            "                       Steps per second: 678 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.202s \n",
            "                        Mean value loss: 495.1695\n",
            "                    Mean surrogate loss: -0.0043\n",
            "                      Mean entropy loss: 2.8644\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0154\n",
            "                  Mean extrinsic reward: 136.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 136.13\n",
            "                    Mean episode length: 136.13\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 83/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16128 \n",
            "                       Steps per second: 704 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.197s \n",
            "                        Mean value loss: 40.1822\n",
            "                    Mean surrogate loss: 0.0061\n",
            "                      Mean entropy loss: 2.8641\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0096\n",
            "                  Mean extrinsic reward: 135.30\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 135.30\n",
            "                    Mean episode length: 135.30\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 84/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16320 \n",
            "                       Steps per second: 697 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.196s \n",
            "                        Mean value loss: 315.8287\n",
            "                    Mean surrogate loss: -0.0102\n",
            "                      Mean entropy loss: 2.8634\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0128\n",
            "                  Mean extrinsic reward: 135.40\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 135.40\n",
            "                    Mean episode length: 135.40\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 85/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16512 \n",
            "                       Steps per second: 681 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.205s \n",
            "                        Mean value loss: 279.6430\n",
            "                    Mean surrogate loss: 0.0108\n",
            "                      Mean entropy loss: 2.8639\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0202\n",
            "                  Mean extrinsic reward: 135.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 135.13\n",
            "                    Mean episode length: 135.13\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 86/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16704 \n",
            "                       Steps per second: 704 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.197s \n",
            "                        Mean value loss: 5.2393\n",
            "                    Mean surrogate loss: -0.0000\n",
            "                      Mean entropy loss: 2.8643\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0122\n",
            "                  Mean extrinsic reward: 135.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 135.13\n",
            "                    Mean episode length: 135.13\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 87/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16896 \n",
            "                       Steps per second: 732 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 5.2770\n",
            "                    Mean surrogate loss: 0.0049\n",
            "                      Mean entropy loss: 2.8641\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0142\n",
            "                  Mean extrinsic reward: 135.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 135.13\n",
            "                    Mean episode length: 135.13\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 88/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17088 \n",
            "                       Steps per second: 708 \n",
            "                        Collection time: 0.086s \n",
            "                          Learning time: 0.184s \n",
            "                        Mean value loss: 1.9906\n",
            "                    Mean surrogate loss: 0.0066\n",
            "                      Mean entropy loss: 2.8639\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0487\n",
            "                  Mean extrinsic reward: 135.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 135.13\n",
            "                    Mean episode length: 135.13\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 89/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17280 \n",
            "                       Steps per second: 706 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.194s \n",
            "                        Mean value loss: 1.6148\n",
            "                    Mean surrogate loss: 0.0125\n",
            "                      Mean entropy loss: 2.8640\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0326\n",
            "                  Mean extrinsic reward: 135.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 135.13\n",
            "                    Mean episode length: 135.13\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 90/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17472 \n",
            "                       Steps per second: 672 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.205s \n",
            "                        Mean value loss: 2.0109\n",
            "                    Mean surrogate loss: -0.0025\n",
            "                      Mean entropy loss: 2.8641\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0170\n",
            "                  Mean extrinsic reward: 135.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 135.13\n",
            "                    Mean episode length: 135.13\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 91/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17664 \n",
            "                       Steps per second: 699 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.200s \n",
            "                        Mean value loss: 4.3584\n",
            "                    Mean surrogate loss: 0.0094\n",
            "                      Mean entropy loss: 2.8642\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0496\n",
            "                  Mean extrinsic reward: 135.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 135.13\n",
            "                    Mean episode length: 135.13\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 92/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17856 \n",
            "                       Steps per second: 737 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.186s \n",
            "                        Mean value loss: 1.6432\n",
            "                    Mean surrogate loss: 0.0022\n",
            "                      Mean entropy loss: 2.8645\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0283\n",
            "                  Mean extrinsic reward: 135.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 135.13\n",
            "                    Mean episode length: 135.13\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 93/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18048 \n",
            "                       Steps per second: 726 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 1.4558\n",
            "                    Mean surrogate loss: -0.0036\n",
            "                      Mean entropy loss: 2.8645\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0137\n",
            "                  Mean extrinsic reward: 135.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 135.13\n",
            "                    Mean episode length: 135.13\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 94/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18240 \n",
            "                       Steps per second: 747 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.183s \n",
            "                        Mean value loss: 0.8978\n",
            "                    Mean surrogate loss: -0.0059\n",
            "                      Mean entropy loss: 2.8644\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0085\n",
            "                  Mean extrinsic reward: 135.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 135.13\n",
            "                    Mean episode length: 135.13\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 95/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18432 \n",
            "                       Steps per second: 688 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.201s \n",
            "                        Mean value loss: 2.2268\n",
            "                    Mean surrogate loss: 0.0035\n",
            "                      Mean entropy loss: 2.8633\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0200\n",
            "                  Mean extrinsic reward: 139.93\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 139.93\n",
            "                    Mean episode length: 139.93\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 96/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18624 \n",
            "                       Steps per second: 712 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.193s \n",
            "                        Mean value loss: 5.7418\n",
            "                    Mean surrogate loss: 0.0173\n",
            "                      Mean entropy loss: 2.8566\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0123\n",
            "                  Mean extrinsic reward: 139.93\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 139.93\n",
            "                    Mean episode length: 139.93\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 97/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18816 \n",
            "                       Steps per second: 714 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.191s \n",
            "                        Mean value loss: 0.8927\n",
            "                    Mean surrogate loss: 0.0148\n",
            "                      Mean entropy loss: 2.8573\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0279\n",
            "                  Mean extrinsic reward: 139.93\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 139.93\n",
            "                    Mean episode length: 139.93\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 98/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 19008 \n",
            "                       Steps per second: 727 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 0.8090\n",
            "                    Mean surrogate loss: 0.0075\n",
            "                      Mean entropy loss: 2.8565\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0568\n",
            "                  Mean extrinsic reward: 139.93\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 139.93\n",
            "                    Mean episode length: 139.93\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 99/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 19200 \n",
            "                       Steps per second: 682 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.204s \n",
            "                        Mean value loss: 0.8025\n",
            "                    Mean surrogate loss: 0.0057\n",
            "                      Mean entropy loss: 2.8563\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0213\n",
            "                  Mean extrinsic reward: 139.93\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 139.93\n",
            "                    Mean episode length: 139.93\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:00:00\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5MBIyaorIbnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NOrmalised"
      ],
      "metadata": {
        "id": "FgAzIhuRUJQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_improved_cartpole_dataset(num_episodes=5000, max_steps=300, device='cpu'):\n",
        "    \"\"\"\n",
        "    Generate larger, more diverse symmetry-aware CartPole dataset.\n",
        "    Returns ONLY the final normalized dataset tensor.\n",
        "    \"\"\"\n",
        "    print(\"Generating Improved Symmetry-Aware CartPole Dataset...\")\n",
        "\n",
        "    # Reproducibility\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    env = gym.make('CartPole-v1', render_mode=None)\n",
        "    all_states = []\n",
        "\n",
        "    for _ in tqdm(range(num_episodes), desc=\"Episodes\"):\n",
        "        reset_result = env.reset()\n",
        "        state = reset_result[0] if isinstance(reset_result, tuple) else reset_result\n",
        "        episode_states = []\n",
        "\n",
        "        for _ in range(max_steps):\n",
        "            # --- Diverse actions: mix of policy and random ---\n",
        "            # If angle (index 2) is significant (> 0.1 rad), use policy to stabilize\n",
        "            if np.abs(state[2]) > 0.1:\n",
        "                action = 1 if state[2] > 0 else 0\n",
        "            else:\n",
        "                # Otherwise random exploration\n",
        "                action = env.action_space.sample()\n",
        "\n",
        "            step_result = env.step(action)\n",
        "            next_state = step_result[0]\n",
        "            terminated = step_result[2]\n",
        "            truncated = step_result[3] if len(step_result) > 4 else False\n",
        "\n",
        "            episode_states.append(state.copy())\n",
        "            state = next_state\n",
        "\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        all_states.extend(episode_states)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # --- Create symmetric dataset (original + reflected) ---\n",
        "    states = np.array(all_states)\n",
        "    reflected = states.copy()\n",
        "    reflected[:, [0, 1]] *= -1  # Reflect position and velocity\n",
        "\n",
        "    full_dataset = np.concatenate([states, reflected], axis=0)\n",
        "\n",
        "    # --- Normalization ---\n",
        "    data = torch.tensor(full_dataset, dtype=torch.float32).to(device)\n",
        "    mean = data.mean(dim=0)\n",
        "    std = data.std(dim=0) + 1e-8\n",
        "    normalized_data = (data - mean) / std\n",
        "\n",
        "    # Save full details to disk (so you don't lose mean/std for un-normalization later)\n",
        "    torch.save({\n",
        "        'raw_data': full_dataset,\n",
        "        'mean': mean.cpu(),\n",
        "        'std': std.cpu()\n",
        "    }, 'improved_cartpole_symmetry.pt')\n",
        "\n",
        "    print(f\"Saved dataset to disk. Total samples: {len(normalized_data)}\")\n",
        "\n",
        "    # Return ONLY the final dataset as requested\n",
        "    return normalized_data"
      ],
      "metadata": {
        "id": "Uc2mzDvaUPGH"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "latent_data = generate_improved_cartpole_dataset(device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eMhBafzUQG3",
        "outputId": "2174541f-e364-4fe3-da6a-822ee37d24b8"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Improved Symmetry-Aware CartPole Dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episodes: 100%|██████████| 5000/5000 [00:04<00:00, 1158.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved dataset to disk. Total samples: 428834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_symmetry = None\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Complete workflow: RL training → Symmetry detection\n",
        "    \"\"\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Parameters\n",
        "    env_name = 'CartPole-v1'\n",
        "    latent_dim = 4\n",
        "    num_generators = 6  # SO(4) has 6 generators\n",
        "\n",
        "    print(\"Starting complete workflow...\")\n",
        "\n",
        "    # PHASE 1: Train PPO agent (RL training)\n",
        "    # trained_agent = \"/content/trained_ppo_cartpole.pt\" # This was the issue, it was a string\n",
        "    # We need to train the agent first if it's not already saved and loaded\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # PHASE 3: Initialize and train symmetry generators\n",
        "    model_symmetry = GroupLatent(num_features=latent_dim, num_generators=num_generators)\n",
        "    model_symmetry.to(device)\n",
        "\n",
        "    # Define M for psi_4d\n",
        "    M = torch.eye(latent_dim, device=device)\n",
        "   # M = get_metric(latent_dim, device=device)\n",
        "\n",
        "    # Train symmetry generators on fixed latent data\n",
        "    closure_losses, orth_losses = train_symmetry_generators(\n",
        "        model_symmetry, latent_data, device=device, epochs=15,M=M)\n",
        "    torch.save(model_symmetry.state_dict(), \"model_symmetry_cartpole.pt\")\n",
        "\n",
        "    # PHASE 4: Analysis and visualization\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING COMPLETE - ANALYZING RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Analyze generators\n",
        "    analyze_generators(model_symmetry)\n",
        "\n",
        "    # Plot generators\n",
        "    plot_generators(model_symmetry)\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(closure_losses)\n",
        "    plt.title('Symmetry Closure Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.yscale('log')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(orth_losses)\n",
        "    plt.title('Symmetry Orthogonality Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.yscale('log')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Test symmetry preservation\n",
        "    print(\"\\n=== Testing Symmetry Preservation ===\")\n",
        "    test_batch = latent_data[:100].to(device)\n",
        "    theta_test = [(2*torch.rand(test_batch.shape[0], device=device) - 1)\n",
        "                  for _ in range(num_generators)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        transformed_batch = model_symmetry(theta=theta_test, x=test_batch)\n",
        "\n",
        "        original_invariant = psi_4d(test_batch)\n",
        "        transformed_invariant = psi_4d(transformed_batch)\n",
        "\n",
        "        invariant_error = torch.abs(original_invariant - transformed_invariant).mean()\n",
        "\n",
        "    print(f\"Average invariant preservation error: {invariant_error:.6f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"✓ Phase 1: PPO agent trained on CartPole (RL)\")\n",
        "    print(\"✓ Phase 2: Latent data generated from trained agent\")\n",
        "    print(\"✓ Phase 3: Symmetry generators trained on fixed latent data\")\n",
        "    print(\"✓ Phase 4: 6 SO(4) generators learned and visualized\")\n",
        "    print(\"✓ No augmentation - pure symmetry discovery\")\n",
        "\n",
        "\n",
        "    return  model_symmetry, latent_data\n",
        "\n",
        "\n",
        "model_symmetry, latent_data = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KyDlChp9UUkV",
        "outputId": "1e821ba5-e7d2-4238-ea6a-f9e0bc3668df"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Starting complete workflow...\n",
            "=== PHASE 2: Training Symmetry Generators ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/15: 100%|██████████| 419/419 [00:11<00:00, 35.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Closure: 29.985263, Orthogonality: 0.950211, Skew-Sym: 4.656353, Magnitude: 3.783275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/15: 100%|██████████| 419/419 [00:11<00:00, 35.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Closure: 2.101325, Orthogonality: 0.909378, Skew-Sym: 1.176985, Magnitude: 3.431738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/15: 100%|██████████| 419/419 [00:11<00:00, 35.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Closure: 0.795557, Orthogonality: 0.859489, Skew-Sym: 0.567338, Magnitude: 3.495089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/15: 100%|██████████| 419/419 [00:11<00:00, 36.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Closure: 0.278184, Orthogonality: 0.761560, Skew-Sym: 0.284101, Magnitude: 3.537786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/15: 100%|██████████| 419/419 [00:11<00:00, 35.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Closure: 0.095881, Orthogonality: 0.602177, Skew-Sym: 0.135910, Magnitude: 3.591057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/15: 100%|██████████| 419/419 [00:11<00:00, 35.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Closure: 0.028660, Orthogonality: 0.352263, Skew-Sym: 0.056451, Magnitude: 3.690992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/15: 100%|██████████| 419/419 [00:11<00:00, 35.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Closure: 0.007431, Orthogonality: 0.145901, Skew-Sym: 0.017979, Magnitude: 3.798573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/15: 100%|██████████| 419/419 [00:11<00:00, 35.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Closure: 0.001442, Orthogonality: 0.067042, Skew-Sym: 0.004251, Magnitude: 3.780911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/15: 100%|██████████| 419/419 [00:11<00:00, 35.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - Closure: 0.000348, Orthogonality: 0.035539, Skew-Sym: 0.000970, Magnitude: 3.624142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/15: 100%|██████████| 419/419 [00:11<00:00, 35.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - Closure: 0.000135, Orthogonality: 0.019485, Skew-Sym: 0.000313, Magnitude: 3.350366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/15: 100%|██████████| 419/419 [00:11<00:00, 35.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 - Closure: 0.000073, Orthogonality: 0.009831, Skew-Sym: 0.000163, Magnitude: 2.982837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/15: 100%|██████████| 419/419 [00:11<00:00, 36.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 - Closure: 0.000042, Orthogonality: 0.004218, Skew-Sym: 0.000101, Magnitude: 2.553541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/15: 100%|██████████| 419/419 [00:11<00:00, 35.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 - Closure: 0.000023, Orthogonality: 0.001417, Skew-Sym: 0.000062, Magnitude: 2.100639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/15: 100%|██████████| 419/419 [00:11<00:00, 35.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 - Closure: 0.000012, Orthogonality: 0.000324, Skew-Sym: 0.000032, Magnitude: 1.661656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/15: 100%|██████████| 419/419 [00:11<00:00, 35.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 - Closure: 0.000007, Orthogonality: 0.000037, Skew-Sym: 0.000016, Magnitude: 1.266204\n",
            "\n",
            "=== EXAMPLE USAGE AFTER TRAINING ===\n",
            "Original z (eval batch):\n",
            "tensor([[-0.2216, -0.0264,  0.4105,  0.0071],\n",
            "        [-0.2237, -0.2708,  0.4124,  0.3270],\n",
            "        [-0.2461, -0.0278,  0.4762,  0.0339],\n",
            "        [-0.2484,  0.2150,  0.4832, -0.2569]], device='cuda:0')\n",
            "\n",
            "Per-sample product Π exp(θ_i G_i) with Taylor approximation (B, D, D):\n",
            "tensor([[[ 0.9287, -0.2536, -0.1327, -0.2351],\n",
            "         [ 0.2886,  0.9464,  0.1375,  0.0382],\n",
            "         [ 0.0681, -0.1586,  0.9786, -0.1123],\n",
            "         [ 0.2231, -0.1178,  0.0762,  0.9647]],\n",
            "\n",
            "        [[ 0.9520, -0.1124, -0.1569, -0.2367],\n",
            "         [ 0.0820,  0.9875, -0.1156, -0.0645],\n",
            "         [ 0.2107,  0.1054,  0.9590,  0.1589],\n",
            "         [ 0.2064,  0.0211, -0.2069,  0.9566]],\n",
            "\n",
            "        [[ 0.9099,  0.2824, -0.2592, -0.1579],\n",
            "         [-0.2413,  0.9543,  0.1071,  0.1449],\n",
            "         [ 0.2569, -0.0227,  0.9568, -0.1339],\n",
            "         [ 0.2205, -0.1000,  0.0746,  0.9669]],\n",
            "\n",
            "        [[ 0.9046,  0.1814, -0.3829, -0.0418],\n",
            "         [-0.2435,  0.9630, -0.1137, -0.0330],\n",
            "         [ 0.3505,  0.1995,  0.9100,  0.0977],\n",
            "         [-0.0045,  0.0199, -0.1096,  0.9937]]], device='cuda:0')\n",
            "\n",
            "z′ from product = (Π exp(θ_i G_i)) @ z:\n",
            "tensor([[-0.2552, -0.0322,  0.3900, -0.0081],\n",
            "        [-0.3247, -0.3545,  0.3717,  0.1756],\n",
            "        [-0.3606,  0.0887,  0.3885,  0.0168],\n",
            "        [-0.3600,  0.2211,  0.3705, -0.3029]], device='cuda:0')\n",
            "\n",
            "z′ from model forward (for cross-check):\n",
            "tensor([[-0.2552, -0.0322,  0.3900, -0.0081],\n",
            "        [-0.3247, -0.3545,  0.3717,  0.1756],\n",
            "        [-0.3606,  0.0887,  0.3885,  0.0168],\n",
            "        [-0.3600,  0.2211,  0.3705, -0.3029]], device='cuda:0')\n",
            "\n",
            "Mean |z′(product) - z′(model)| = 1.635635e-08\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETE - ANALYZING RESULTS\n",
            "============================================================\n",
            "=== Generator Analysis ===\n",
            "\n",
            "Generator 0:\n",
            "  Shape: (4, 4)\n",
            "  Min: -0.2526, Max: 0.2537\n",
            "  Mean: 0.000032, Std: 0.0895\n",
            "  Frobenius norm: 0.3580\n",
            "  Skew-symmetric: False\n",
            "\n",
            "Generator 1:\n",
            "  Shape: (4, 4)\n",
            "  Min: -0.3104, Max: 0.3115\n",
            "  Mean: 0.000028, Std: 0.1099\n",
            "  Frobenius norm: 0.4398\n",
            "  Skew-symmetric: False\n",
            "\n",
            "Generator 2:\n",
            "  Shape: (4, 4)\n",
            "  Min: -0.4920, Max: 0.4909\n",
            "  Mean: -0.000035, Std: 0.1737\n",
            "  Frobenius norm: 0.6950\n",
            "  Skew-symmetric: False\n",
            "\n",
            "Generator 3:\n",
            "  Shape: (4, 4)\n",
            "  Min: -0.1995, Max: 0.1995\n",
            "  Mean: -0.000002, Std: 0.0705\n",
            "  Frobenius norm: 0.2821\n",
            "  Skew-symmetric: True\n",
            "\n",
            "Generator 4:\n",
            "  Shape: (4, 4)\n",
            "  Min: -0.2636, Max: 0.2641\n",
            "  Mean: 0.000021, Std: 0.0933\n",
            "  Frobenius norm: 0.3731\n",
            "  Skew-symmetric: True\n",
            "\n",
            "Generator 5:\n",
            "  Shape: (4, 4)\n",
            "  Min: -0.1795, Max: 0.1800\n",
            "  Mean: 0.000030, Std: 0.0636\n",
            "  Frobenius norm: 0.2542\n",
            "  Skew-symmetric: True\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 3150x350 with 15 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing Symmetry Preservation ===\n",
            "Average invariant preservation error: 0.001673\n",
            "\n",
            "============================================================\n",
            "SUMMARY\n",
            "============================================================\n",
            "✓ Phase 1: PPO agent trained on CartPole (RL)\n",
            "✓ Phase 2: Latent data generated from trained agent\n",
            "✓ Phase 3: Symmetry generators trained on fixed latent data\n",
            "✓ Phase 4: 6 SO(4) generators learned and visualized\n",
            "✓ No augmentation - pure symmetry discovery\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/rsl_rl/rsl_rl/modules/cartpole_symmetry_utils.py\n",
        "# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION\n",
        "# All rights reserved.\n",
        "#\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "from tensordict import TensorDict\n",
        "from typing import Optional\n",
        "\n",
        "from rsl_rl.env import VecEnv\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def cartpole_symmetry(obs=None, actions=None, env=None, obs_type: str = \"policy\"):\n",
        "    \"\"\"Augment observations AND actions for CartPole using learned symmetry generators.\n",
        "\n",
        "    Handles normalization from previous symmetry training:\n",
        "    - env.symm_mean, env.symm_std: normalization stats from symmetry dataset\n",
        "    - model_symmetry trained on normalized states\n",
        "    \"\"\"\n",
        "\n",
        "    # observations - ONLY policy group\n",
        "    if obs is not None:\n",
        "        batch_size = obs.batch_size[0]\n",
        "        obs_aug = obs.repeat(2)  # original + symmetric\n",
        "\n",
        "        if env is None or not hasattr(env, \"model_symmetry\"):\n",
        "            # fallback: simple reflection\n",
        "            obs_aug[\"policy\"][:batch_size] = obs[\"policy\"]\n",
        "            obs_aug[\"policy\"][batch_size : 2 * batch_size, [0, 1]] = -obs[\"policy\"][:, [0, 1]]\n",
        "        else:\n",
        "            model_symmetry = env.model_symmetry\n",
        "            dev = next(model_symmetry.parameters()).device\n",
        "\n",
        "            # -- original\n",
        "            obs_aug[\"policy\"][:batch_size] = obs[\"policy\"]\n",
        "\n",
        "            # -- generator-based symmetric version\n",
        "            s = obs[\"policy\"].to(dev)  # (B, 4) raw RL states\n",
        "\n",
        "            # Normalize to match symmetry training data\n",
        "            if hasattr(env, \"symm_mean\") and hasattr(env, \"symm_std\"):\n",
        "                mean = env.symm_mean.to(dev)\n",
        "                std = env.symm_std.to(dev)\n",
        "                z = (s - mean) / (std + 1e-8)  # normalize\n",
        "            else:\n",
        "                z = s  # no normalization available\n",
        "\n",
        "            num_generators = getattr(model_symmetry, \"num_generators\", None)\n",
        "            if num_generators is None:\n",
        "                num_generators = getattr(model_symmetry, \"numgenerators\")\n",
        "\n",
        "            theta = [(2 * torch.rand(batch_size, device=dev) - 1) for _ in range(num_generators)]\n",
        "            z_prime_norm = model_symmetry(theta=theta, x=z)  # (B, 4) normalized symmetric\n",
        "\n",
        "            # De-normalize back to raw state space\n",
        "            if hasattr(env, \"symm_mean\") and hasattr(env, \"symm_std\"):\n",
        "                z_prime = z_prime_norm * std + mean\n",
        "            else:\n",
        "                z_prime = z_prime_norm\n",
        "\n",
        "            obs_aug[\"policy\"][batch_size : 2 * batch_size] = z_prime.to(obs[\"policy\"].device)\n",
        "    else:\n",
        "        obs_aug = None\n",
        "\n",
        "    # actions - APPLY SAME SYMMETRY TRANSFORMATION\n",
        "    actions_aug = None\n",
        "    if actions is not None:\n",
        "        batch_size = actions.shape[0]\n",
        "        actions_aug = torch.zeros(batch_size * 2, actions.shape[1], device=actions.device)\n",
        "\n",
        "        # -- original\n",
        "        actions_aug[:batch_size] = actions\n",
        "\n",
        "        if env is not None and hasattr(env, \"model_symmetry\"):\n",
        "            model_symmetry = env.model_symmetry\n",
        "            dev = next(model_symmetry.parameters()).device\n",
        "\n",
        "            # One-hot encode discrete actions: (B,1) → (B,4)\n",
        "            actions_onehot_raw = torch.zeros(batch_size, 4, device=dev)\n",
        "            actions_onehot_raw.scatter_(1, actions.unsqueeze(1).to(dev), 1.0)\n",
        "\n",
        "            # Normalize actions if stats available (treat as 4D vector)\n",
        "            if hasattr(env, \"symm_mean\") and hasattr(env, \"symm_std\"):\n",
        "                mean = env.symm_mean.to(dev)\n",
        "                std = env.symm_std.to(dev)\n",
        "                actions_onehot_norm = (actions_onehot_raw - mean) / (std + 1e-8)\n",
        "            else:\n",
        "                actions_onehot_norm = actions_onehot_raw\n",
        "\n",
        "            # Apply SAME symmetry flow\n",
        "            num_generators = getattr(model_symmetry, \"num_generators\", None)\n",
        "            if num_generators is None:\n",
        "                num_generators = getattr(model_symmetry, \"numgenerators\")\n",
        "\n",
        "            theta = [(2 * torch.rand(batch_size, device=dev) - 1) for _ in range(num_generators)]\n",
        "            actions_sym_norm = model_symmetry(theta=theta, x=actions_onehot_norm)\n",
        "\n",
        "            # De-normalize back\n",
        "            if hasattr(env, \"symm_mean\") and hasattr(env, \"symm_std\"):\n",
        "                actions_sym_raw = actions_sym_norm * std + mean\n",
        "            else:\n",
        "                actions_sym_raw = actions_sym_norm\n",
        "\n",
        "            # Convert back to discrete: argmax\n",
        "            actions_sym_discrete = torch.argmax(actions_sym_raw, dim=1)\n",
        "\n",
        "            actions_aug[batch_size : 2 * batch_size] = actions_sym_discrete.to(actions.device)\n",
        "        else:\n",
        "            # fallback: simple flip\n",
        "            actions_aug[batch_size : 2 * batch_size] = 1 - actions\n",
        "\n",
        "    return obs_aug, actions_aug\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSn7vdcpUdzR",
        "outputId": "0febf8d4-e81c-470b-cb70-e5ca2802fda6"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/rsl_rl/rsl_rl/modules/cartpole_symmetry_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "from types import SimpleNamespace\n",
        "from rsl_rl.runners import OnPolicyRunner\n",
        "#import cartpole_symmetry\n",
        "\n",
        "yaml_config_string = \"\"\"\n",
        "runner:\n",
        "  class_name: OnPolicyRunner\n",
        "  # General\n",
        "  num_steps_per_env: 24  # Number of steps per environment per iteration\n",
        "  max_iterations: 100    # Number of policy updates\n",
        "  seed: 1\n",
        "  # Observations\n",
        "  obs_groups: {\"policy\": [\"policy\"], \"critic\": [\"policy\", \"privileged\"]}  # Maps observation groups to sets. See `vec_env.py` for more information\n",
        "  # Logging parameters\n",
        "  save_interval: 50  # Check for potential saves every `save_interval` iterations\n",
        "  experiment_name: walking_experiment\n",
        "  run_name: \"\"\n",
        "  # Logging writer\n",
        "  logger: tensorboard  # tensorboard, neptune, wandb\n",
        "  neptune_project: legged_gym\n",
        "  wandb_project: legged_gym\n",
        "\n",
        "  # Policy\n",
        "  policy:\n",
        "    class_name: ActorCritic\n",
        "    activation: elu\n",
        "    actor_obs_normalization: false\n",
        "    critic_obs_normalization: false\n",
        "    actor_hidden_dims: [256, 256, 256]\n",
        "    critic_hidden_dims: [256, 256, 256]\n",
        "    init_noise_std: 1.0\n",
        "    noise_std_type: \"scalar\"  # 'scalar' or 'log'\n",
        "    state_dependent_std: false\n",
        "\n",
        "  # Algorithm\n",
        "  algorithm:\n",
        "    class_name: PPO\n",
        "    # Training\n",
        "    learning_rate: 0.001\n",
        "    num_learning_epochs: 5\n",
        "    num_mini_batches: 4  # mini batch size = num_envs * num_steps / num_mini_batches\n",
        "    schedule: adaptive  # adaptive, fixed\n",
        "    # Value function\n",
        "    value_loss_coef: 1.0\n",
        "    clip_param: 0.2\n",
        "    use_clipped_value_loss: true\n",
        "    # Surrogate loss\n",
        "    desired_kl: 0.01\n",
        "    entropy_coef: 0.01\n",
        "    gamma: 0.99\n",
        "    lam: 0.95\n",
        "    max_grad_norm: 1.0\n",
        "    # Miscellaneous\n",
        "    normalize_advantage_per_mini_batch: false\n",
        "\n",
        "    # Random network distillation\n",
        "    rnd_cfg:\n",
        "      weight: 0.0  # Initial weight of the RND reward\n",
        "      weight_schedule: null  # This is a dictionary with a required key called \"mode\". Please check the RND module for more information\n",
        "      reward_normalization: false  # Whether to normalize RND reward\n",
        "      # Learning parameters\n",
        "      learning_rate: 0.001  # Learning rate for RND\n",
        "      # Network parameters\n",
        "      num_outputs: 1  # Number of outputs of RND network. Note: if -1, then the network will use dimensions of the observation\n",
        "      predictor_hidden_dims: [-1]  # Hidden dimensions of predictor network\n",
        "      target_hidden_dims: [-1]  # Hidden dimensions of target network\n",
        "\n",
        "    # Symmetry augmentation\n",
        "    symmetry_cfg:\n",
        "      use_data_augmentation: true  # This adds symmetric trajectories to the batch\n",
        "      use_mirror_loss: false  # This adds symmetry loss term to the loss function\n",
        "      data_augmentation_func: \"rsl_rl.modules.cartpole_symmetry_utils:cartpole_symmetry\"  # String containing the module and function name to import\n",
        "      # Example: \"legged_gym.envs.locomotion.anymal_c.symmetry:get_symmetric_states\"\n",
        "      mirror_loss_coeff: 0.0  # Coefficient for symmetry loss term. If 0, no symmetry loss is used\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_train_cfg():\n",
        "    raw = yaml.safe_load(yaml_config_string)\n",
        "    # Optional: turn top-level dict into an object-like cfg\n",
        "    return SimpleNamespace(**raw)\n",
        "import os\n",
        "\n",
        "log_dir = \"./logs_cartpole\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "# Usage:\n",
        "train_cfg = get_train_cfg()\n",
        "print(train_cfg.runner[\"class_name\"])   # 'OnPolicyRunner'\n",
        "# later:\n",
        "env = CartPoleVecEnv(num_envs=8, device=\"cuda:0\")\n",
        "# Load normalization stats from your symmetry training dataset\n",
        "symm_stats = torch.load(\"improved_cartpole_symmetry.pt\", map_location=device, weights_only=False)\n",
        "env.symm_mean = symm_stats[\"mean\"].to(device)   # (4,)\n",
        "env.symm_std = symm_stats[\"std\"].to(device)     # (4,)\n",
        "\n",
        "# Load trained generators\n",
        "model_symmetry = GroupLatent(num_features=4, num_generators=6).to(device)\n",
        "model_symmetry.load_state_dict(torch.load(\"model_symmetry_cartpole.pt\", map_location=device, weights_only=False))\n",
        "env.model_symmetry = model_symmetry\n",
        "\n",
        "runner = OnPolicyRunner(env, train_cfg.runner, log_dir=log_dir, device=device)\n",
        "runner.learn(num_learning_iterations=train_cfg.runner[\"max_iterations\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzqJZ0hwVgri",
        "outputId": "cd23baf5-fd0a-48a4-be94-c33fff00006b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OnPolicyRunner\n",
            "--------------------------------------------------------------------------------\n",
            "Resolved observation sets: \n",
            "\t policy :  ['policy']\n",
            "\t critic :  ['policy', 'privileged']\n",
            "\t rnd_state :  ['policy']\n",
            "--------------------------------------------------------------------------------\n",
            "Actor MLP: MLP(\n",
            "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n",
            "Critic MLP: MLP(\n",
            "  (0): Linear(in_features=8, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/rsl_rl/rsl_rl/utils/utils.py:273: UserWarning: The observation configuration dictionary 'obs_groups' must contain the 'rnd_state' key. As the configuration for 'rnd_state' is missing, the observations from the 'policy' set are used. Consider adding the 'rnd_state' key to the 'obs_groups' dictionary for clarity. This behavior will be removed in a future version.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 0/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 192 \n",
            "                       Steps per second: 776 \n",
            "                        Collection time: 0.087s \n",
            "                          Learning time: 0.161s \n",
            "                        Mean value loss: 61.2401\n",
            "                    Mean surrogate loss: 0.0768\n",
            "                      Mean entropy loss: 2.8413\n",
            "                          Mean rnd loss: 0.2405\n",
            "                     Mean symmetry loss: 0.1362\n",
            "                  Mean extrinsic reward: 17.75\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 17.75\n",
            "                    Mean episode length: 17.75\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:00\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 1/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 384 \n",
            "                       Steps per second: 741 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.164s \n",
            "                        Mean value loss: 45.4648\n",
            "                    Mean surrogate loss: 0.0081\n",
            "                      Mean entropy loss: 2.8422\n",
            "                          Mean rnd loss: 0.1550\n",
            "                     Mean symmetry loss: 0.0127\n",
            "                  Mean extrinsic reward: 25.50\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 25.50\n",
            "                    Mean episode length: 25.50\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:00\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 2/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 576 \n",
            "                       Steps per second: 789 \n",
            "                        Collection time: 0.088s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 42.1943\n",
            "                    Mean surrogate loss: -0.0010\n",
            "                      Mean entropy loss: 2.8422\n",
            "                          Mean rnd loss: 0.1209\n",
            "                     Mean symmetry loss: 0.0053\n",
            "                  Mean extrinsic reward: 23.65\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 23.65\n",
            "                    Mean episode length: 23.65\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:00\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 3/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 768 \n",
            "                       Steps per second: 784 \n",
            "                        Collection time: 0.088s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 42.0240\n",
            "                    Mean surrogate loss: 0.0182\n",
            "                      Mean entropy loss: 2.8408\n",
            "                          Mean rnd loss: 0.0922\n",
            "                     Mean symmetry loss: 0.0022\n",
            "                  Mean extrinsic reward: 23.93\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 23.93\n",
            "                    Mean episode length: 23.93\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:00\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 4/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 960 \n",
            "                       Steps per second: 786 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.164s \n",
            "                        Mean value loss: 46.0818\n",
            "                    Mean surrogate loss: -0.0098\n",
            "                      Mean entropy loss: 2.8397\n",
            "                          Mean rnd loss: 0.0835\n",
            "                     Mean symmetry loss: 0.0021\n",
            "                  Mean extrinsic reward: 23.61\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 23.61\n",
            "                    Mean episode length: 23.61\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 5/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1152 \n",
            "                       Steps per second: 760 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.159s \n",
            "                        Mean value loss: 51.3314\n",
            "                    Mean surrogate loss: -0.0011\n",
            "                      Mean entropy loss: 2.8335\n",
            "                          Mean rnd loss: 0.0569\n",
            "                     Mean symmetry loss: 0.0008\n",
            "                  Mean extrinsic reward: 25.08\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 25.08\n",
            "                    Mean episode length: 25.08\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 6/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1344 \n",
            "                       Steps per second: 805 \n",
            "                        Collection time: 0.084s \n",
            "                          Learning time: 0.154s \n",
            "                        Mean value loss: 39.7420\n",
            "                    Mean surrogate loss: -0.0039\n",
            "                      Mean entropy loss: 2.8298\n",
            "                          Mean rnd loss: 0.0352\n",
            "                     Mean symmetry loss: 0.0022\n",
            "                  Mean extrinsic reward: 28.65\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 28.65\n",
            "                    Mean episode length: 28.65\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 7/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1536 \n",
            "                       Steps per second: 804 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.156s \n",
            "                        Mean value loss: 53.2687\n",
            "                    Mean surrogate loss: -0.0131\n",
            "                      Mean entropy loss: 2.8287\n",
            "                          Mean rnd loss: 0.0250\n",
            "                     Mean symmetry loss: 0.0053\n",
            "                  Mean extrinsic reward: 28.72\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 28.72\n",
            "                    Mean episode length: 28.72\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 8/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1728 \n",
            "                       Steps per second: 814 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.154s \n",
            "                        Mean value loss: 63.6305\n",
            "                    Mean surrogate loss: -0.0017\n",
            "                      Mean entropy loss: 2.8291\n",
            "                          Mean rnd loss: 0.0204\n",
            "                     Mean symmetry loss: 0.0089\n",
            "                  Mean extrinsic reward: 29.06\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 29.06\n",
            "                    Mean episode length: 29.06\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 9/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1920 \n",
            "                       Steps per second: 781 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.165s \n",
            "                        Mean value loss: 55.4263\n",
            "                    Mean surrogate loss: 0.0024\n",
            "                      Mean entropy loss: 2.8310\n",
            "                          Mean rnd loss: 0.0186\n",
            "                     Mean symmetry loss: 0.0251\n",
            "                  Mean extrinsic reward: 30.57\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 30.57\n",
            "                    Mean episode length: 30.57\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 10/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2112 \n",
            "                       Steps per second: 802 \n",
            "                        Collection time: 0.086s \n",
            "                          Learning time: 0.153s \n",
            "                        Mean value loss: 54.9266\n",
            "                    Mean surrogate loss: 0.0069\n",
            "                      Mean entropy loss: 2.8314\n",
            "                          Mean rnd loss: 0.0130\n",
            "                     Mean symmetry loss: 0.0040\n",
            "                  Mean extrinsic reward: 31.94\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 31.94\n",
            "                    Mean episode length: 31.94\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 11/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2304 \n",
            "                       Steps per second: 806 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.158s \n",
            "                        Mean value loss: 62.1692\n",
            "                    Mean surrogate loss: -0.0013\n",
            "                      Mean entropy loss: 2.8323\n",
            "                          Mean rnd loss: 0.0046\n",
            "                     Mean symmetry loss: 0.0040\n",
            "                  Mean extrinsic reward: 33.65\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.65\n",
            "                    Mean episode length: 33.65\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 12/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2496 \n",
            "                       Steps per second: 808 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 66.4446\n",
            "                    Mean surrogate loss: 0.0035\n",
            "                      Mean entropy loss: 2.8320\n",
            "                          Mean rnd loss: 0.0020\n",
            "                     Mean symmetry loss: 0.0113\n",
            "                  Mean extrinsic reward: 33.65\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 33.65\n",
            "                    Mean episode length: 33.65\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 13/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2688 \n",
            "                       Steps per second: 799 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.161s \n",
            "                        Mean value loss: 58.8541\n",
            "                    Mean surrogate loss: 0.0913\n",
            "                      Mean entropy loss: 2.8343\n",
            "                          Mean rnd loss: 0.0014\n",
            "                     Mean symmetry loss: 0.2171\n",
            "                  Mean extrinsic reward: 37.18\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 37.18\n",
            "                    Mean episode length: 37.18\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 14/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2880 \n",
            "                       Steps per second: 817 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.152s \n",
            "                        Mean value loss: 50.2451\n",
            "                    Mean surrogate loss: 0.0468\n",
            "                      Mean entropy loss: 2.8347\n",
            "                          Mean rnd loss: 0.0012\n",
            "                     Mean symmetry loss: 0.1364\n",
            "                  Mean extrinsic reward: 39.43\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 39.43\n",
            "                    Mean episode length: 39.43\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 15/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3072 \n",
            "                       Steps per second: 800 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 56.8666\n",
            "                    Mean surrogate loss: -0.0011\n",
            "                      Mean entropy loss: 2.8354\n",
            "                          Mean rnd loss: 0.0007\n",
            "                     Mean symmetry loss: 0.0089\n",
            "                  Mean extrinsic reward: 40.35\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 40.35\n",
            "                    Mean episode length: 40.35\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 16/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3264 \n",
            "                       Steps per second: 827 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.151s \n",
            "                        Mean value loss: 49.2063\n",
            "                    Mean surrogate loss: 0.0002\n",
            "                      Mean entropy loss: 2.8352\n",
            "                          Mean rnd loss: 0.0007\n",
            "                     Mean symmetry loss: 0.0343\n",
            "                  Mean extrinsic reward: 40.45\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 40.45\n",
            "                    Mean episode length: 40.45\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 17/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3456 \n",
            "                       Steps per second: 817 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.154s \n",
            "                        Mean value loss: 56.0045\n",
            "                    Mean surrogate loss: 0.0013\n",
            "                      Mean entropy loss: 2.8345\n",
            "                          Mean rnd loss: 0.0008\n",
            "                     Mean symmetry loss: 0.0152\n",
            "                  Mean extrinsic reward: 44.79\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 44.79\n",
            "                    Mean episode length: 44.79\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 18/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3648 \n",
            "                       Steps per second: 713 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.174s \n",
            "                        Mean value loss: 54.8917\n",
            "                    Mean surrogate loss: -0.0002\n",
            "                      Mean entropy loss: 2.8344\n",
            "                          Mean rnd loss: 0.0007\n",
            "                     Mean symmetry loss: 0.0064\n",
            "                  Mean extrinsic reward: 47.46\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 47.46\n",
            "                    Mean episode length: 47.46\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 19/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3840 \n",
            "                       Steps per second: 804 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.158s \n",
            "                        Mean value loss: 57.0381\n",
            "                    Mean surrogate loss: -0.0115\n",
            "                      Mean entropy loss: 2.8341\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0026\n",
            "                  Mean extrinsic reward: 47.67\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 47.67\n",
            "                    Mean episode length: 47.67\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 20/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4032 \n",
            "                       Steps per second: 812 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.156s \n",
            "                        Mean value loss: 51.8432\n",
            "                    Mean surrogate loss: -0.0074\n",
            "                      Mean entropy loss: 2.8336\n",
            "                          Mean rnd loss: 0.0007\n",
            "                     Mean symmetry loss: 0.0047\n",
            "                  Mean extrinsic reward: 49.76\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 49.76\n",
            "                    Mean episode length: 49.76\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 21/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4224 \n",
            "                       Steps per second: 808 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 55.2727\n",
            "                    Mean surrogate loss: 0.0002\n",
            "                      Mean entropy loss: 2.8326\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0046\n",
            "                  Mean extrinsic reward: 50.00\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 50.00\n",
            "                    Mean episode length: 50.00\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 22/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4416 \n",
            "                       Steps per second: 774 \n",
            "                        Collection time: 0.091s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 52.8819\n",
            "                    Mean surrogate loss: 0.0029\n",
            "                      Mean entropy loss: 2.8323\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0018\n",
            "                  Mean extrinsic reward: 51.35\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 51.35\n",
            "                    Mean episode length: 51.35\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 23/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4608 \n",
            "                       Steps per second: 797 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.160s \n",
            "                        Mean value loss: 50.2813\n",
            "                    Mean surrogate loss: -0.0055\n",
            "                      Mean entropy loss: 2.8323\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0021\n",
            "                  Mean extrinsic reward: 53.60\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 53.60\n",
            "                    Mean episode length: 53.60\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 24/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4800 \n",
            "                       Steps per second: 809 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 52.2736\n",
            "                    Mean surrogate loss: -0.0060\n",
            "                      Mean entropy loss: 2.8316\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0030\n",
            "                  Mean extrinsic reward: 54.95\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 54.95\n",
            "                    Mean episode length: 54.95\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 25/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4992 \n",
            "                       Steps per second: 800 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 46.9338\n",
            "                    Mean surrogate loss: 0.0053\n",
            "                      Mean entropy loss: 2.8312\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0049\n",
            "                  Mean extrinsic reward: 54.95\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 54.95\n",
            "                    Mean episode length: 54.95\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 26/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5184 \n",
            "                       Steps per second: 767 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.167s \n",
            "                        Mean value loss: 48.2455\n",
            "                    Mean surrogate loss: 0.1160\n",
            "                      Mean entropy loss: 2.8308\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: 0.2244\n",
            "                  Mean extrinsic reward: 56.54\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 56.54\n",
            "                    Mean episode length: 56.54\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 27/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5376 \n",
            "                       Steps per second: 798 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.160s \n",
            "                        Mean value loss: 73.3878\n",
            "                    Mean surrogate loss: 0.0777\n",
            "                      Mean entropy loss: 2.8309\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.1662\n",
            "                  Mean extrinsic reward: 58.66\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 58.66\n",
            "                    Mean episode length: 58.66\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 28/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5568 \n",
            "                       Steps per second: 815 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 45.1720\n",
            "                    Mean surrogate loss: 0.0091\n",
            "                      Mean entropy loss: 2.8312\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0369\n",
            "                  Mean extrinsic reward: 58.97\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 58.97\n",
            "                    Mean episode length: 58.97\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 29/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5760 \n",
            "                       Steps per second: 818 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.154s \n",
            "                        Mean value loss: 63.2796\n",
            "                    Mean surrogate loss: 0.0427\n",
            "                      Mean entropy loss: 2.8322\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.2309\n",
            "                  Mean extrinsic reward: 59.29\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 59.29\n",
            "                    Mean episode length: 59.29\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 30/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5952 \n",
            "                       Steps per second: 753 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.173s \n",
            "                        Mean value loss: 63.0937\n",
            "                    Mean surrogate loss: 0.0217\n",
            "                      Mean entropy loss: 2.8332\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0805\n",
            "                  Mean extrinsic reward: 59.87\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 59.87\n",
            "                    Mean episode length: 59.87\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 31/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6144 \n",
            "                       Steps per second: 556 \n",
            "                        Collection time: 0.135s \n",
            "                          Learning time: 0.210s \n",
            "                        Mean value loss: 91.9946\n",
            "                    Mean surrogate loss: 0.0183\n",
            "                      Mean entropy loss: 2.8333\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0419\n",
            "                  Mean extrinsic reward: 59.21\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 59.21\n",
            "                    Mean episode length: 59.21\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.35s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 32/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6336 \n",
            "                       Steps per second: 634 \n",
            "                        Collection time: 0.107s \n",
            "                          Learning time: 0.196s \n",
            "                        Mean value loss: 76.5259\n",
            "                    Mean surrogate loss: -0.0089\n",
            "                      Mean entropy loss: 2.8332\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0087\n",
            "                  Mean extrinsic reward: 59.57\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 59.57\n",
            "                    Mean episode length: 59.57\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 33/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6528 \n",
            "                       Steps per second: 626 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.204s \n",
            "                        Mean value loss: 55.7440\n",
            "                    Mean surrogate loss: -0.0120\n",
            "                      Mean entropy loss: 2.8327\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0040\n",
            "                  Mean extrinsic reward: 59.86\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 59.86\n",
            "                    Mean episode length: 59.86\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 34/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6720 \n",
            "                       Steps per second: 629 \n",
            "                        Collection time: 0.117s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 61.1042\n",
            "                    Mean surrogate loss: -0.0065\n",
            "                      Mean entropy loss: 2.8324\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0030\n",
            "                  Mean extrinsic reward: 62.19\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 62.19\n",
            "                    Mean episode length: 62.19\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 35/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6912 \n",
            "                       Steps per second: 602 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.213s \n",
            "                        Mean value loss: 56.1023\n",
            "                    Mean surrogate loss: -0.0023\n",
            "                      Mean entropy loss: 2.8319\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0047\n",
            "                  Mean extrinsic reward: 61.97\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 61.97\n",
            "                    Mean episode length: 61.97\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.32s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 36/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7104 \n",
            "                       Steps per second: 551 \n",
            "                        Collection time: 0.118s \n",
            "                          Learning time: 0.230s \n",
            "                        Mean value loss: 35.3983\n",
            "                    Mean surrogate loss: -0.0052\n",
            "                      Mean entropy loss: 2.8313\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0032\n",
            "                  Mean extrinsic reward: 61.97\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 61.97\n",
            "                    Mean episode length: 61.97\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.35s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 37/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7296 \n",
            "                       Steps per second: 607 \n",
            "                        Collection time: 0.132s \n",
            "                          Learning time: 0.184s \n",
            "                        Mean value loss: 48.6356\n",
            "                    Mean surrogate loss: -0.0029\n",
            "                      Mean entropy loss: 2.8286\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0311\n",
            "                  Mean extrinsic reward: 62.78\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 62.78\n",
            "                    Mean episode length: 62.78\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.32s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 38/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7488 \n",
            "                       Steps per second: 814 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 45.1699\n",
            "                    Mean surrogate loss: -0.0171\n",
            "                      Mean entropy loss: 2.8281\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0174\n",
            "                  Mean extrinsic reward: 63.99\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 63.99\n",
            "                    Mean episode length: 63.99\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 39/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7680 \n",
            "                       Steps per second: 818 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.153s \n",
            "                        Mean value loss: 44.7227\n",
            "                    Mean surrogate loss: 0.0071\n",
            "                      Mean entropy loss: 2.8282\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0083\n",
            "                  Mean extrinsic reward: 65.42\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 65.42\n",
            "                    Mean episode length: 65.42\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 40/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7872 \n",
            "                       Steps per second: 775 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.168s \n",
            "                        Mean value loss: 23.9351\n",
            "                    Mean surrogate loss: 0.0111\n",
            "                      Mean entropy loss: 2.8281\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0104\n",
            "                  Mean extrinsic reward: 65.42\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 65.42\n",
            "                    Mean episode length: 65.42\n",
            "                  Mean action noise std: 0.99\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 41/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8064 \n",
            "                       Steps per second: 715 \n",
            "                        Collection time: 0.088s \n",
            "                          Learning time: 0.180s \n",
            "                        Mean value loss: 98.4878\n",
            "                    Mean surrogate loss: 0.0100\n",
            "                      Mean entropy loss: 2.8259\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0774\n",
            "                  Mean extrinsic reward: 71.29\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 71.29\n",
            "                    Mean episode length: 71.29\n",
            "                  Mean action noise std: 0.99\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 42/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8256 \n",
            "                       Steps per second: 796 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.158s \n",
            "                        Mean value loss: 22.8208\n",
            "                    Mean surrogate loss: 0.0115\n",
            "                      Mean entropy loss: 2.8263\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0069\n",
            "                  Mean extrinsic reward: 71.29\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 71.29\n",
            "                    Mean episode length: 71.29\n",
            "                  Mean action noise std: 0.99\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 43/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8448 \n",
            "                       Steps per second: 781 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.165s \n",
            "                        Mean value loss: 23.6737\n",
            "                    Mean surrogate loss: 0.0005\n",
            "                      Mean entropy loss: 2.8297\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0045\n",
            "                  Mean extrinsic reward: 71.29\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 71.29\n",
            "                    Mean episode length: 71.29\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 44/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8640 \n",
            "                       Steps per second: 782 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.163s \n",
            "                        Mean value loss: 23.0441\n",
            "                    Mean surrogate loss: 0.0223\n",
            "                      Mean entropy loss: 2.8337\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0038\n",
            "                  Mean extrinsic reward: 71.29\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 71.29\n",
            "                    Mean episode length: 71.29\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 45/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8832 \n",
            "                       Steps per second: 729 \n",
            "                        Collection time: 0.084s \n",
            "                          Learning time: 0.180s \n",
            "                        Mean value loss: 33.1544\n",
            "                    Mean surrogate loss: 0.0117\n",
            "                      Mean entropy loss: 2.8367\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0370\n",
            "                  Mean extrinsic reward: 73.85\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 73.85\n",
            "                    Mean episode length: 73.85\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 46/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9024 \n",
            "                       Steps per second: 802 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.158s \n",
            "                        Mean value loss: 76.0353\n",
            "                    Mean surrogate loss: 0.0075\n",
            "                      Mean entropy loss: 2.8370\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0199\n",
            "                  Mean extrinsic reward: 75.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 75.07\n",
            "                    Mean episode length: 75.07\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 47/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9216 \n",
            "                       Steps per second: 822 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.153s \n",
            "                        Mean value loss: 25.5947\n",
            "                    Mean surrogate loss: -0.0055\n",
            "                      Mean entropy loss: 2.8371\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0059\n",
            "                  Mean extrinsic reward: 77.01\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 77.01\n",
            "                    Mean episode length: 77.01\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 48/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9408 \n",
            "                       Steps per second: 794 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.161s \n",
            "                        Mean value loss: 33.5035\n",
            "                    Mean surrogate loss: 0.0022\n",
            "                      Mean entropy loss: 2.8367\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0266\n",
            "                  Mean extrinsic reward: 79.38\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 79.38\n",
            "                    Mean episode length: 79.38\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 49/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9600 \n",
            "                       Steps per second: 757 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.170s \n",
            "                        Mean value loss: 69.6348\n",
            "                    Mean surrogate loss: 0.0086\n",
            "                      Mean entropy loss: 2.8362\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0271\n",
            "                  Mean extrinsic reward: 81.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 81.07\n",
            "                    Mean episode length: 81.07\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 50/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9792 \n",
            "                       Steps per second: 786 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.162s \n",
            "                        Mean value loss: 69.3063\n",
            "                    Mean surrogate loss: 0.0011\n",
            "                      Mean entropy loss: 2.8353\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0101\n",
            "                  Mean extrinsic reward: 83.30\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 83.30\n",
            "                    Mean episode length: 83.30\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 51/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9984 \n",
            "                       Steps per second: 807 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.156s \n",
            "                        Mean value loss: 100.1194\n",
            "                    Mean surrogate loss: -0.0099\n",
            "                      Mean entropy loss: 2.8352\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0064\n",
            "                  Mean extrinsic reward: 84.17\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 84.17\n",
            "                    Mean episode length: 84.17\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 52/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10176 \n",
            "                       Steps per second: 804 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.159s \n",
            "                        Mean value loss: 14.0283\n",
            "                    Mean surrogate loss: -0.0032\n",
            "                      Mean entropy loss: 2.8355\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0122\n",
            "                  Mean extrinsic reward: 84.17\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 84.17\n",
            "                    Mean episode length: 84.17\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 53/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10368 \n",
            "                       Steps per second: 772 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.169s \n",
            "                        Mean value loss: 10.1054\n",
            "                    Mean surrogate loss: -0.0018\n",
            "                      Mean entropy loss: 2.8372\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0059\n",
            "                  Mean extrinsic reward: 84.17\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 84.17\n",
            "                    Mean episode length: 84.17\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 54/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10560 \n",
            "                       Steps per second: 798 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.158s \n",
            "                        Mean value loss: 12.7267\n",
            "                    Mean surrogate loss: -0.0010\n",
            "                      Mean entropy loss: 2.8416\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0102\n",
            "                  Mean extrinsic reward: 84.17\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 84.17\n",
            "                    Mean episode length: 84.17\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 55/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10752 \n",
            "                       Steps per second: 802 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.159s \n",
            "                        Mean value loss: 7.0505\n",
            "                    Mean surrogate loss: 0.0125\n",
            "                      Mean entropy loss: 2.8423\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0037\n",
            "                  Mean extrinsic reward: 84.17\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 84.17\n",
            "                    Mean episode length: 84.17\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 56/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10944 \n",
            "                       Steps per second: 774 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.165s \n",
            "                        Mean value loss: 7.7650\n",
            "                    Mean surrogate loss: -0.0136\n",
            "                      Mean entropy loss: 2.8425\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0034\n",
            "                  Mean extrinsic reward: 84.17\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 84.17\n",
            "                    Mean episode length: 84.17\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 57/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11136 \n",
            "                       Steps per second: 800 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.160s \n",
            "                        Mean value loss: 8.0150\n",
            "                    Mean surrogate loss: 0.0048\n",
            "                      Mean entropy loss: 2.8430\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0058\n",
            "                  Mean extrinsic reward: 84.17\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 84.17\n",
            "                    Mean episode length: 84.17\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 58/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11328 \n",
            "                       Steps per second: 734 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.167s \n",
            "                        Mean value loss: 38.2208\n",
            "                    Mean surrogate loss: 0.0098\n",
            "                      Mean entropy loss: 2.8438\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0032\n",
            "                  Mean extrinsic reward: 87.80\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 87.80\n",
            "                    Mean episode length: 87.80\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 59/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11520 \n",
            "                       Steps per second: 798 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.158s \n",
            "                        Mean value loss: 180.1385\n",
            "                    Mean surrogate loss: 0.0025\n",
            "                      Mean entropy loss: 2.8443\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0026\n",
            "                  Mean extrinsic reward: 96.06\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 96.06\n",
            "                    Mean episode length: 96.06\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 60/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11712 \n",
            "                       Steps per second: 808 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.156s \n",
            "                        Mean value loss: 10.4253\n",
            "                    Mean surrogate loss: 0.0036\n",
            "                      Mean entropy loss: 2.8440\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0054\n",
            "                  Mean extrinsic reward: 96.06\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 96.06\n",
            "                    Mean episode length: 96.06\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 61/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11904 \n",
            "                       Steps per second: 803 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 30.4307\n",
            "                    Mean surrogate loss: 0.0196\n",
            "                      Mean entropy loss: 2.8403\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0254\n",
            "                  Mean extrinsic reward: 98.20\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 98.20\n",
            "                    Mean episode length: 98.20\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 62/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12096 \n",
            "                       Steps per second: 757 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.159s \n",
            "                        Mean value loss: 32.0033\n",
            "                    Mean surrogate loss: 0.0165\n",
            "                      Mean entropy loss: 2.8400\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0122\n",
            "                  Mean extrinsic reward: 105.78\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 105.78\n",
            "                    Mean episode length: 105.78\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 63/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12288 \n",
            "                       Steps per second: 797 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.158s \n",
            "                        Mean value loss: 250.0446\n",
            "                    Mean surrogate loss: 0.0113\n",
            "                      Mean entropy loss: 2.8404\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0080\n",
            "                  Mean extrinsic reward: 111.89\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 111.89\n",
            "                    Mean episode length: 111.89\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 64/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12480 \n",
            "                       Steps per second: 805 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.158s \n",
            "                        Mean value loss: 6.5939\n",
            "                    Mean surrogate loss: -0.0055\n",
            "                      Mean entropy loss: 2.8404\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0145\n",
            "                  Mean extrinsic reward: 111.89\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 111.89\n",
            "                    Mean episode length: 111.89\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 65/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12672 \n",
            "                       Steps per second: 785 \n",
            "                        Collection time: 0.085s \n",
            "                          Learning time: 0.160s \n",
            "                        Mean value loss: 262.5742\n",
            "                    Mean surrogate loss: 0.0022\n",
            "                      Mean entropy loss: 2.8400\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0126\n",
            "                  Mean extrinsic reward: 112.30\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 112.30\n",
            "                    Mean episode length: 112.30\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 66/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12864 \n",
            "                       Steps per second: 772 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.166s \n",
            "                        Mean value loss: 7.3436\n",
            "                    Mean surrogate loss: -0.0109\n",
            "                      Mean entropy loss: 2.8398\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0396\n",
            "                  Mean extrinsic reward: 112.30\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 112.30\n",
            "                    Mean episode length: 112.30\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 67/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13056 \n",
            "                       Steps per second: 805 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 23.9621\n",
            "                    Mean surrogate loss: 0.0050\n",
            "                      Mean entropy loss: 2.8399\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0206\n",
            "                  Mean extrinsic reward: 112.30\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 112.30\n",
            "                    Mean episode length: 112.30\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 68/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13248 \n",
            "                       Steps per second: 802 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.158s \n",
            "                        Mean value loss: 122.5284\n",
            "                    Mean surrogate loss: -0.0094\n",
            "                      Mean entropy loss: 2.8398\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0064\n",
            "                  Mean extrinsic reward: 112.30\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 112.30\n",
            "                    Mean episode length: 112.30\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 69/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13440 \n",
            "                       Steps per second: 784 \n",
            "                        Collection time: 0.086s \n",
            "                          Learning time: 0.159s \n",
            "                        Mean value loss: 206.4665\n",
            "                    Mean surrogate loss: -0.0071\n",
            "                      Mean entropy loss: 2.8397\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0051\n",
            "                  Mean extrinsic reward: 114.01\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 114.01\n",
            "                    Mean episode length: 114.01\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 70/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13632 \n",
            "                       Steps per second: 771 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.168s \n",
            "                        Mean value loss: 44.0526\n",
            "                    Mean surrogate loss: -0.0112\n",
            "                      Mean entropy loss: 2.8395\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0032\n",
            "                  Mean extrinsic reward: 114.01\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 114.01\n",
            "                    Mean episode length: 114.01\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 71/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13824 \n",
            "                       Steps per second: 815 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.152s \n",
            "                        Mean value loss: 256.2384\n",
            "                    Mean surrogate loss: 0.0040\n",
            "                      Mean entropy loss: 2.8393\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0034\n",
            "                  Mean extrinsic reward: 116.49\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 116.49\n",
            "                    Mean episode length: 116.49\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 72/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14016 \n",
            "                       Steps per second: 809 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.156s \n",
            "                        Mean value loss: 40.3012\n",
            "                    Mean surrogate loss: 0.0040\n",
            "                      Mean entropy loss: 2.8391\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0165\n",
            "                  Mean extrinsic reward: 120.54\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 120.54\n",
            "                    Mean episode length: 120.54\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 73/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14208 \n",
            "                       Steps per second: 792 \n",
            "                        Collection time: 0.085s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 12.8219\n",
            "                    Mean surrogate loss: -0.0135\n",
            "                      Mean entropy loss: 2.8389\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0087\n",
            "                  Mean extrinsic reward: 120.54\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 120.54\n",
            "                    Mean episode length: 120.54\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 74/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14400 \n",
            "                       Steps per second: 797 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.161s \n",
            "                        Mean value loss: 262.1532\n",
            "                    Mean surrogate loss: -0.0000\n",
            "                      Mean entropy loss: 2.8386\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0279\n",
            "                  Mean extrinsic reward: 125.48\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 125.48\n",
            "                    Mean episode length: 125.48\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 75/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14592 \n",
            "                       Steps per second: 771 \n",
            "                        Collection time: 0.089s \n",
            "                          Learning time: 0.160s \n",
            "                        Mean value loss: 6.9103\n",
            "                    Mean surrogate loss: 0.0027\n",
            "                      Mean entropy loss: 2.8388\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0560\n",
            "                  Mean extrinsic reward: 125.48\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 125.48\n",
            "                    Mean episode length: 125.48\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 76/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14784 \n",
            "                       Steps per second: 780 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.165s \n",
            "                        Mean value loss: 112.6312\n",
            "                    Mean surrogate loss: 0.0175\n",
            "                      Mean entropy loss: 2.8424\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0661\n",
            "                  Mean extrinsic reward: 126.76\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 126.76\n",
            "                    Mean episode length: 126.76\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 77/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14976 \n",
            "                       Steps per second: 721 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.166s \n",
            "                        Mean value loss: 279.2368\n",
            "                    Mean surrogate loss: 0.0156\n",
            "                      Mean entropy loss: 2.8438\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0586\n",
            "                  Mean extrinsic reward: 128.71\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 128.71\n",
            "                    Mean episode length: 128.71\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 78/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15168 \n",
            "                       Steps per second: 537 \n",
            "                        Collection time: 0.121s \n",
            "                          Learning time: 0.236s \n",
            "                        Mean value loss: 140.6679\n",
            "                    Mean surrogate loss: -0.0104\n",
            "                      Mean entropy loss: 2.8459\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.1005\n",
            "                  Mean extrinsic reward: 130.99\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 130.99\n",
            "                    Mean episode length: 130.99\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.36s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 79/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15360 \n",
            "                       Steps per second: 633 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.200s \n",
            "                        Mean value loss: 198.3524\n",
            "                    Mean surrogate loss: 0.0197\n",
            "                      Mean entropy loss: 2.8465\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0478\n",
            "                  Mean extrinsic reward: 132.98\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 132.98\n",
            "                    Mean episode length: 132.98\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 80/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15552 \n",
            "                       Steps per second: 626 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.206s \n",
            "                        Mean value loss: 120.6865\n",
            "                    Mean surrogate loss: 0.0034\n",
            "                      Mean entropy loss: 2.8466\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0069\n",
            "                  Mean extrinsic reward: 133.70\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 133.70\n",
            "                    Mean episode length: 133.70\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 81/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15744 \n",
            "                       Steps per second: 667 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.191s \n",
            "                        Mean value loss: 131.7150\n",
            "                    Mean surrogate loss: 0.0154\n",
            "                      Mean entropy loss: 2.8471\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0106\n",
            "                  Mean extrinsic reward: 133.71\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 133.71\n",
            "                    Mean episode length: 133.71\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 82/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15936 \n",
            "                       Steps per second: 562 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.238s \n",
            "                        Mean value loss: 30.6238\n",
            "                    Mean surrogate loss: -0.0020\n",
            "                      Mean entropy loss: 2.8473\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0197\n",
            "                  Mean extrinsic reward: 133.25\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 133.25\n",
            "                    Mean episode length: 133.25\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.34s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 83/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16128 \n",
            "                       Steps per second: 571 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.230s \n",
            "                        Mean value loss: 3.9947\n",
            "                    Mean surrogate loss: -0.0072\n",
            "                      Mean entropy loss: 2.8468\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0335\n",
            "                  Mean extrinsic reward: 133.25\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 133.25\n",
            "                    Mean episode length: 133.25\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.34s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 84/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16320 \n",
            "                       Steps per second: 561 \n",
            "                        Collection time: 0.136s \n",
            "                          Learning time: 0.206s \n",
            "                        Mean value loss: 5.5676\n",
            "                    Mean surrogate loss: -0.0051\n",
            "                      Mean entropy loss: 2.8465\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0158\n",
            "                  Mean extrinsic reward: 133.25\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 133.25\n",
            "                    Mean episode length: 133.25\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.34s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 85/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16512 \n",
            "                       Steps per second: 746 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.177s \n",
            "                        Mean value loss: 7.4871\n",
            "                    Mean surrogate loss: 0.0999\n",
            "                      Mean entropy loss: 2.8468\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.2903\n",
            "                  Mean extrinsic reward: 133.25\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 133.25\n",
            "                    Mean episode length: 133.25\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 86/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16704 \n",
            "                       Steps per second: 808 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.158s \n",
            "                        Mean value loss: 168.1515\n",
            "                    Mean surrogate loss: 0.0585\n",
            "                      Mean entropy loss: 2.8468\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.2335\n",
            "                  Mean extrinsic reward: 134.25\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 134.25\n",
            "                    Mean episode length: 134.25\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 87/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16896 \n",
            "                       Steps per second: 817 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 57.8423\n",
            "                    Mean surrogate loss: 0.0009\n",
            "                      Mean entropy loss: 2.8464\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0232\n",
            "                  Mean extrinsic reward: 135.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 135.07\n",
            "                    Mean episode length: 135.07\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 88/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17088 \n",
            "                       Steps per second: 805 \n",
            "                        Collection time: 0.083s \n",
            "                          Learning time: 0.156s \n",
            "                        Mean value loss: 272.0468\n",
            "                    Mean surrogate loss: 0.0093\n",
            "                      Mean entropy loss: 2.8461\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0089\n",
            "                  Mean extrinsic reward: 138.36\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 138.36\n",
            "                    Mean episode length: 138.36\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 89/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17280 \n",
            "                       Steps per second: 762 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.171s \n",
            "                        Mean value loss: 96.8168\n",
            "                    Mean surrogate loss: -0.0042\n",
            "                      Mean entropy loss: 2.8465\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0043\n",
            "                  Mean extrinsic reward: 141.09\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 141.09\n",
            "                    Mean episode length: 141.09\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 90/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17472 \n",
            "                       Steps per second: 814 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.153s \n",
            "                        Mean value loss: 9.0354\n",
            "                    Mean surrogate loss: -0.0016\n",
            "                      Mean entropy loss: 2.8472\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0038\n",
            "                  Mean extrinsic reward: 142.70\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 142.70\n",
            "                    Mean episode length: 142.70\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 91/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17664 \n",
            "                       Steps per second: 805 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.158s \n",
            "                        Mean value loss: 5.5461\n",
            "                    Mean surrogate loss: 0.0038\n",
            "                      Mean entropy loss: 2.8467\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0047\n",
            "                  Mean extrinsic reward: 146.43\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 146.43\n",
            "                    Mean episode length: 146.43\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 92/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17856 \n",
            "                       Steps per second: 785 \n",
            "                        Collection time: 0.087s \n",
            "                          Learning time: 0.158s \n",
            "                        Mean value loss: 35.2131\n",
            "                    Mean surrogate loss: 0.0016\n",
            "                      Mean entropy loss: 2.8471\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0019\n",
            "                  Mean extrinsic reward: 146.43\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 146.43\n",
            "                    Mean episode length: 146.43\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 93/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18048 \n",
            "                       Steps per second: 798 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.160s \n",
            "                        Mean value loss: 203.4243\n",
            "                    Mean surrogate loss: 0.0029\n",
            "                      Mean entropy loss: 2.8499\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0133\n",
            "                  Mean extrinsic reward: 144.44\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 144.44\n",
            "                    Mean episode length: 144.44\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 94/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18240 \n",
            "                       Steps per second: 755 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.159s \n",
            "                        Mean value loss: 60.3052\n",
            "                    Mean surrogate loss: -0.0077\n",
            "                      Mean entropy loss: 2.8506\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0051\n",
            "                  Mean extrinsic reward: 145.03\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 145.03\n",
            "                    Mean episode length: 145.03\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 95/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18432 \n",
            "                       Steps per second: 796 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.161s \n",
            "                        Mean value loss: 497.2203\n",
            "                    Mean surrogate loss: 0.0087\n",
            "                      Mean entropy loss: 2.8508\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0063\n",
            "                  Mean extrinsic reward: 145.99\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 145.99\n",
            "                    Mean episode length: 145.99\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 96/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18624 \n",
            "                       Steps per second: 804 \n",
            "                        Collection time: 0.082s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 572.8377\n",
            "                    Mean surrogate loss: -0.0041\n",
            "                      Mean entropy loss: 2.8511\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0076\n",
            "                  Mean extrinsic reward: 146.15\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 146.15\n",
            "                    Mean episode length: 146.15\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 97/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18816 \n",
            "                       Steps per second: 804 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 213.3520\n",
            "                    Mean surrogate loss: -0.0099\n",
            "                      Mean entropy loss: 2.8514\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0079\n",
            "                  Mean extrinsic reward: 146.02\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 146.02\n",
            "                    Mean episode length: 146.02\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 98/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 19008 \n",
            "                       Steps per second: 769 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 74.6286\n",
            "                    Mean surrogate loss: -0.0068\n",
            "                      Mean entropy loss: 2.8518\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0069\n",
            "                  Mean extrinsic reward: 146.02\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 146.02\n",
            "                    Mean episode length: 146.02\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 99/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 19200 \n",
            "                       Steps per second: 760 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.173s \n",
            "                        Mean value loss: 111.2387\n",
            "                    Mean surrogate loss: -0.0093\n",
            "                      Mean entropy loss: 2.8519\n",
            "                          Mean rnd loss: 0.0000\n",
            "                     Mean symmetry loss: 0.0089\n",
            "                  Mean extrinsic reward: 145.76\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 145.76\n",
            "                    Mean episode length: 145.76\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:00\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#normalised env"
      ],
      "metadata": {
        "id": "ExFQ72cLiLzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "from gymnasium import spaces\n",
        "from tensordict import TensorDict\n",
        "from rsl_rl.env import VecEnv\n",
        "\n",
        "class CartPoleVecEnv(VecEnv):\n",
        "    def __init__(self, num_envs: int = 8, device: str = \"cpu\", max_episode_length: int = 500):\n",
        "        self.num_envs = num_envs\n",
        "        self.device = torch.device(device)\n",
        "        self.max_episode_length = max_episode_length\n",
        "        self.episode_length_buf = torch.zeros(num_envs, dtype=torch.long, device=self.device)\n",
        "\n",
        "        # underlying envs\n",
        "        self.envs = [gym.make(\"CartPole-v1\") for _ in range(num_envs)]\n",
        "        base_env = self.envs[0]\n",
        "\n",
        "        # expose gym-like spaces so rsl_rl infers dims cleanly\n",
        "        self.observation_space = base_env.observation_space  # Box(4,)\n",
        "        self.action_space = base_env.action_space            # Discrete(2)\n",
        "\n",
        "        self.num_actions = int(self.action_space.n)\n",
        "\n",
        "        obs0, _ = base_env.reset()\n",
        "        obs_dim = obs0.shape[0]\n",
        "        self.obs_buf = torch.zeros(num_envs, obs_dim, device=self.device, dtype=torch.float32)\n",
        "\n",
        "        self.cfg = {\"env_name\": \"CartPole-v1\"}\n",
        "\n",
        "        class _DummyUnwrapped:\n",
        "            def __init__(self, step_dt: float):\n",
        "                self.step_dt = step_dt\n",
        "\n",
        "        # CartPole dt\n",
        "        self.unwrapped = _DummyUnwrapped(step_dt=0.02)\n",
        "\n",
        "        # Symmetry model placeholders (set externally)\n",
        "        self.model_symmetry = None\n",
        "        self.symm_mean = None\n",
        "        self.symm_std = None\n",
        "\n",
        "        self._reset_all()\n",
        "\n",
        "    def attach_symmetry_model(self, model_symmetry_path: str, stats_path: str = None):\n",
        "        \"\"\"Attach trained symmetry model and normalization stats.\"\"\"\n",
        "        # Load symmetry model\n",
        "        from cartpole_symmetry_model import GroupLatent  # adjust import path\n",
        "        self.model_symmetry = GroupLatent(num_features=4, num_generators=6).to(self.device)\n",
        "        self.model_symmetry.load_state_dict(torch.load(model_symmetry_path, map_location=self.device))\n",
        "\n",
        "        # Load normalization stats (optional)\n",
        "        if stats_path:\n",
        "            stats = torch.load(stats_path, map_location=self.device)\n",
        "            self.symm_mean = stats[\"mean\"].to(self.device)\n",
        "            self.symm_std = stats[\"std\"].to(self.device)\n",
        "            print(f\"Loaded symmetry stats: mean={self.symm_mean.tolist()}, std={self.symm_std.tolist()}\")\n",
        "\n",
        "        print(\"Symmetry model attached successfully!\")\n",
        "\n",
        "    def _reset_all(self):\n",
        "        for i, env in enumerate(self.envs):\n",
        "            o, _ = env.reset()\n",
        "            self.obs_buf[i] = torch.as_tensor(o, device=self.device, dtype=torch.float32)\n",
        "        self.episode_length_buf.zero_()\n",
        "\n",
        "    def get_observations(self) -> TensorDict:\n",
        "        return TensorDict(\n",
        "            {\n",
        "                \"policy\": self.obs_buf.clone(),\n",
        "                \"privileged\": self.obs_buf.clone(),\n",
        "            },\n",
        "            batch_size=[self.num_envs],\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "    def step(self, actions: torch.Tensor):\n",
        "        # Handle different action shapes from the policy:\n",
        "        # (num_envs, num_actions) → choose greedy action\n",
        "        if actions.dim() == 2 and actions.shape[1] > 1:\n",
        "            actions = torch.argmax(actions, dim=-1)\n",
        "\n",
        "        # (num_envs, 1) → squeeze to (num_envs,)\n",
        "        if actions.dim() == 2:\n",
        "            actions = actions.squeeze(-1)\n",
        "\n",
        "        rewards = torch.zeros(self.num_envs, device=self.device)\n",
        "        dones = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)\n",
        "        time_outs = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)\n",
        "\n",
        "        for i, env in enumerate(self.envs):\n",
        "            a = int(actions[i].item())\n",
        "            o, r, terminated, truncated, info = env.step(a)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            self.obs_buf[i] = torch.as_tensor(o, device=self.device, dtype=torch.float32)\n",
        "            rewards[i] = r\n",
        "            dones[i] = done\n",
        "\n",
        "            self.episode_length_buf[i] += 1\n",
        "            if self.episode_length_buf[i] >= self.max_episode_length:\n",
        "                time_outs[i] = True\n",
        "                done = True\n",
        "\n",
        "            if done:\n",
        "                o, _ = env.reset()\n",
        "                self.obs_buf[i] = torch.as_tensor(o, device=self.device, dtype=torch.float32)\n",
        "                self.episode_length_buf[i] = 0\n",
        "\n",
        "        obs_td = self.get_observations()\n",
        "        extras = {\n",
        "            \"time_outs\": time_outs,\n",
        "            \"log\": {},\n",
        "        }\n",
        "        return obs_td, rewards, dones, extras\n"
      ],
      "metadata": {
        "id": "y5MvLvcvV2JR"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/rsl_rl/rsl_rl/modules/cartpole_symmetry_utils.py\n",
        "# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION\n",
        "# All rights reserved.\n",
        "#\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "from tensordict import TensorDict\n",
        "from typing import Optional\n",
        "\n",
        "from rsl_rl.env import VecEnv\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def cartpole_symmetry(obs=None, actions=None, env=None, obs_type: str = \"policy\"):\n",
        "    \"\"\"Augment observations AND actions for CartPole using learned symmetry generators.\n",
        "\n",
        "    Both obs and actions are transformed by the SAME symmetry flow:\n",
        "    - obs[\"policy\"] → model_symmetry(theta, obs[\"policy\"])\n",
        "    - actions → model_symmetry(theta, actions_expanded)\n",
        "    \"\"\"\n",
        "\n",
        "    # observations - ONLY policy group\n",
        "    if obs is not None:\n",
        "        batch_size = obs.batch_size[0]\n",
        "        obs_aug = obs.repeat(2)  # original + symmetric\n",
        "\n",
        "        if env is None or not hasattr(env, \"model_symmetry\"):\n",
        "            # fallback: simple reflection\n",
        "            obs_aug[\"policy\"][:batch_size] = obs[\"policy\"]\n",
        "            obs_aug[\"policy\"][batch_size : 2 * batch_size, [0, 1]] = -obs[\"policy\"][:, [0, 1]]\n",
        "        else:\n",
        "            model_symmetry = env.model_symmetry\n",
        "            dev = next(model_symmetry.parameters()).device\n",
        "\n",
        "            # -- original\n",
        "            obs_aug[\"policy\"][:batch_size] = obs[\"policy\"]\n",
        "\n",
        "            # -- generator-based symmetric version\n",
        "            s = obs[\"policy\"].to(dev)  # (B, 4)\n",
        "            z = s  # raw state = latent\n",
        "\n",
        "            num_generators = getattr(model_symmetry, \"num_generators\", None)\n",
        "            if num_generators is None:\n",
        "                num_generators = getattr(model_symmetry, \"numgenerators\")\n",
        "\n",
        "            theta = [(2 * torch.rand(batch_size, device=dev) - 1) for _ in range(num_generators)]\n",
        "            z_prime = model_symmetry(theta=theta, x=z)  # (B, 4)\n",
        "\n",
        "            obs_aug[\"policy\"][batch_size : 2 * batch_size] = z_prime.to(obs[\"policy\"].device)\n",
        "    else:\n",
        "        obs_aug = None\n",
        "\n",
        "    # actions - APPLY SAME SYMMETRY TRANSFORMATION\n",
        "    actions_aug = None\n",
        "    if actions is not None:\n",
        "        batch_size = actions.shape[0]\n",
        "        actions_aug = torch.zeros(batch_size * 2, actions.shape[1], device=actions.device)\n",
        "\n",
        "        # -- original\n",
        "        actions_aug[:batch_size] = actions\n",
        "\n",
        "        if env is not None and hasattr(env, \"model_symmetry\"):\n",
        "            model_symmetry = env.model_symmetry\n",
        "            dev = next(model_symmetry.parameters()).device\n",
        "\n",
        "            # Expand discrete actions to match state dim (4) for symmetry flow\n",
        "            # One-hot encode: actions (B,1) → actions_expanded (B,4)\n",
        "            actions_onehot = torch.zeros(batch_size, 4, device=dev)\n",
        "            actions_onehot.scatter_(1, actions.unsqueeze(1).to(dev), 1.0)  # (B,4)\n",
        "\n",
        "            # Apply SAME symmetry flow as states\n",
        "            num_generators = getattr(model_symmetry, \"num_generators\", None)\n",
        "            if num_generators is None:\n",
        "                num_generators = getattr(model_symmetry, \"numgenerators\")\n",
        "\n",
        "            theta = [(2 * torch.rand(batch_size, device=dev) - 1) for _ in range(num_generators)]\n",
        "            actions_sym = model_symmetry(theta=theta, x=actions_onehot)  # (B,4)\n",
        "\n",
        "            # Convert back to discrete actions: argmax over transformed distribution\n",
        "            actions_sym_discrete = torch.argmax(actions_sym, dim=1)  # (B,)\n",
        "\n",
        "            actions_aug[batch_size : 2 * batch_size] = actions_sym_discrete.to(actions.device)\n",
        "        else:\n",
        "            # fallback: simple flip\n",
        "            actions_aug[batch_size : 2 * batch_size] = 1 - actions\n",
        "\n",
        "    return obs_aug, actions_aug\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nf5-OXy3jIX-",
        "outputId": "2f4f9d1b-4612-484f-d287-b297b1c95d45"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/rsl_rl/rsl_rl/modules/cartpole_symmetry_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final output skew generator used\n"
      ],
      "metadata": {
        "id": "L5W0AuN9tffN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "from types import SimpleNamespace\n",
        "from rsl_rl.runners import OnPolicyRunner\n",
        "#import cartpole_symmetry\n",
        "\n",
        "yaml_config_string = \"\"\"\n",
        "runner:\n",
        "  class_name: OnPolicyRunner\n",
        "  # General\n",
        "  num_steps_per_env: 24  # Number of steps per environment per iteration\n",
        "  max_iterations: 100    # Number of policy updates\n",
        "  seed: 1\n",
        "  # Observations\n",
        "  obs_groups: {\"policy\": [\"policy\"], \"critic\": [\"policy\", \"privileged\"]}  # Maps observation groups to sets. See `vec_env.py` for more information\n",
        "  # Logging parameters\n",
        "  save_interval: 50  # Check for potential saves every `save_interval` iterations\n",
        "  experiment_name: walking_experiment\n",
        "  run_name: \"\"\n",
        "  # Logging writer\n",
        "  logger: tensorboard  # tensorboard, neptune, wandb\n",
        "  neptune_project: legged_gym\n",
        "  wandb_project: legged_gym\n",
        "\n",
        "  # Policy\n",
        "  policy:\n",
        "    class_name: ActorCritic\n",
        "    activation: elu\n",
        "    actor_obs_normalization: false\n",
        "    critic_obs_normalization: false\n",
        "    actor_hidden_dims: [256, 256, 256]\n",
        "    critic_hidden_dims: [256, 256, 256]\n",
        "    init_noise_std: 1.0\n",
        "    noise_std_type: \"scalar\"  # 'scalar' or 'log'\n",
        "    state_dependent_std: false\n",
        "\n",
        "  # Algorithm\n",
        "  algorithm:\n",
        "    class_name: PPO\n",
        "    # Training\n",
        "    learning_rate: 0.001\n",
        "    num_learning_epochs: 5\n",
        "    num_mini_batches: 4  # mini batch size = num_envs * num_steps / num_mini_batches\n",
        "    schedule: adaptive  # adaptive, fixed\n",
        "    # Value function\n",
        "    value_loss_coef: 1.0\n",
        "    clip_param: 0.2\n",
        "    use_clipped_value_loss: true\n",
        "    # Surrogate loss\n",
        "    desired_kl: 0.01\n",
        "    entropy_coef: 0.01\n",
        "    gamma: 0.99\n",
        "    lam: 0.95\n",
        "    max_grad_norm: 1.0\n",
        "    # Miscellaneous\n",
        "    normalize_advantage_per_mini_batch: false\n",
        "\n",
        "    # Random network distillation\n",
        "    rnd_cfg:\n",
        "      weight: 0.0  # Initial weight of the RND reward\n",
        "      weight_schedule: null  # This is a dictionary with a required key called \"mode\". Please check the RND module for more information\n",
        "      reward_normalization: false  # Whether to normalize RND reward\n",
        "      # Learning parameters\n",
        "      learning_rate: 0.001  # Learning rate for RND\n",
        "      # Network parameters\n",
        "      num_outputs: 1  # Number of outputs of RND network. Note: if -1, then the network will use dimensions of the observation\n",
        "      predictor_hidden_dims: [-1]  # Hidden dimensions of predictor network\n",
        "      target_hidden_dims: [-1]  # Hidden dimensions of target network\n",
        "\n",
        "    # Symmetry augmentation\n",
        "    symmetry_cfg:\n",
        "      use_data_augmentation: true  # This adds symmetric trajectories to the batch\n",
        "      use_mirror_loss: false  # This adds symmetry loss term to the loss function\n",
        "      data_augmentation_func: \"rsl_rl.modules.cartpole_symmetry_utils:cartpole_symmetry\"  # String containing the module and function name to import\n",
        "      # Example: \"legged_gym.envs.locomotion.anymal_c.symmetry:get_symmetric_states\"\n",
        "      mirror_loss_coeff: 0.0  # Coefficient for symmetry loss term. If 0, no symmetry loss is used\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_train_cfg():\n",
        "    raw = yaml.safe_load(yaml_config_string)\n",
        "    # Optional: turn top-level dict into an object-like cfg\n",
        "    return SimpleNamespace(**raw)\n",
        "import os\n",
        "\n",
        "log_dir = \"./logs_cartpole\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "# Usage:\n",
        "train_cfg = get_train_cfg()\n",
        "print(train_cfg.runner[\"class_name\"])   # 'OnPolicyRunner'\n",
        "# later:\n",
        "env = CartPoleVecEnv(num_envs=8, device=\"cuda:0\")\n",
        "model_symmetry = GroupLatent(num_features=4, num_generators=6).to(device)\n",
        "model_symmetry.load_state_dict(torch.load(\"model_symmetry_cartpole.pt\"))\n",
        "env.model_symmetry = model_symmetry\n",
        "runner = OnPolicyRunner(env, train_cfg.runner, log_dir=log_dir)\n",
        "runner.learn(num_learning_iterations=train_cfg.runner[\"max_iterations\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj-tGqSqiPWl",
        "outputId": "c142ea99-6296-43ce-f2fc-9a9cdf707656"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OnPolicyRunner\n",
            "--------------------------------------------------------------------------------\n",
            "Resolved observation sets: \n",
            "\t policy :  ['policy']\n",
            "\t critic :  ['policy', 'privileged']\n",
            "\t rnd_state :  ['policy']\n",
            "--------------------------------------------------------------------------------\n",
            "Actor MLP: MLP(\n",
            "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n",
            "Critic MLP: MLP(\n",
            "  (0): Linear(in_features=8, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/rsl_rl/rsl_rl/utils/utils.py:273: UserWarning: The observation configuration dictionary 'obs_groups' must contain the 'rnd_state' key. As the configuration for 'rnd_state' is missing, the observations from the 'policy' set are used. Consider adding the 'rnd_state' key to the 'obs_groups' dictionary for clarity. This behavior will be removed in a future version.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 0/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 192 \n",
            "                       Steps per second: 500 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.306s \n",
            "                        Mean value loss: 56.3389\n",
            "                    Mean surrogate loss: 0.1559\n",
            "                      Mean entropy loss: 2.8400\n",
            "                          Mean rnd loss: 0.3823\n",
            "                     Mean symmetry loss: 0.1065\n",
            "                  Mean extrinsic reward: 16.60\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 16.60\n",
            "                    Mean episode length: 16.60\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.38s\n",
            "                           Time elapsed: 00:00:00\n",
            "                                    ETA: 00:00:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 1/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 384 \n",
            "                       Steps per second: 305 \n",
            "                        Collection time: 0.130s \n",
            "                          Learning time: 0.499s \n",
            "                        Mean value loss: 40.9279\n",
            "                    Mean surrogate loss: 0.0075\n",
            "                      Mean entropy loss: 2.8400\n",
            "                          Mean rnd loss: 0.3602\n",
            "                     Mean symmetry loss: 0.0160\n",
            "                  Mean extrinsic reward: 22.38\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 22.38\n",
            "                    Mean episode length: 22.38\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.63s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:00:49\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 2/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 576 \n",
            "                       Steps per second: 227 \n",
            "                        Collection time: 0.161s \n",
            "                          Learning time: 0.682s \n",
            "                        Mean value loss: 29.5814\n",
            "                    Mean surrogate loss: -0.0087\n",
            "                      Mean entropy loss: 2.8405\n",
            "                          Mean rnd loss: 0.2762\n",
            "                     Mean symmetry loss: 0.0074\n",
            "                  Mean extrinsic reward: 20.64\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 20.64\n",
            "                    Mean episode length: 20.64\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.84s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:00:59\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 3/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 768 \n",
            "                       Steps per second: 331 \n",
            "                        Collection time: 0.108s \n",
            "                          Learning time: 0.471s \n",
            "                        Mean value loss: 39.4487\n",
            "                    Mean surrogate loss: -0.0050\n",
            "                      Mean entropy loss: 2.8411\n",
            "                          Mean rnd loss: 0.2204\n",
            "                     Mean symmetry loss: 0.0048\n",
            "                  Mean extrinsic reward: 19.97\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 19.97\n",
            "                    Mean episode length: 19.97\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.58s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:00:58\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 4/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 960 \n",
            "                       Steps per second: 506 \n",
            "                        Collection time: 0.108s \n",
            "                          Learning time: 0.271s \n",
            "                        Mean value loss: 51.5151\n",
            "                    Mean surrogate loss: -0.0057\n",
            "                      Mean entropy loss: 2.8365\n",
            "                          Mean rnd loss: 0.1744\n",
            "                     Mean symmetry loss: 0.0279\n",
            "                  Mean extrinsic reward: 20.81\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 20.81\n",
            "                    Mean episode length: 20.81\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.38s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:00:53\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 5/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1152 \n",
            "                       Steps per second: 458 \n",
            "                        Collection time: 0.117s \n",
            "                          Learning time: 0.302s \n",
            "                        Mean value loss: 61.9112\n",
            "                    Mean surrogate loss: -0.0020\n",
            "                      Mean entropy loss: 2.8345\n",
            "                          Mean rnd loss: 0.1801\n",
            "                     Mean symmetry loss: 0.0070\n",
            "                  Mean extrinsic reward: 22.67\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 22.67\n",
            "                    Mean episode length: 22.67\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.42s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:50\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 6/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1344 \n",
            "                       Steps per second: 531 \n",
            "                        Collection time: 0.119s \n",
            "                          Learning time: 0.242s \n",
            "                        Mean value loss: 65.0562\n",
            "                    Mean surrogate loss: -0.0119\n",
            "                      Mean entropy loss: 2.8368\n",
            "                          Mean rnd loss: 0.1539\n",
            "                     Mean symmetry loss: 0.0100\n",
            "                  Mean extrinsic reward: 23.85\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 23.85\n",
            "                    Mean episode length: 23.85\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.36s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:47\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 7/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1536 \n",
            "                       Steps per second: 740 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.184s \n",
            "                        Mean value loss: 64.1460\n",
            "                    Mean surrogate loss: -0.0050\n",
            "                      Mean entropy loss: 2.8386\n",
            "                          Mean rnd loss: 0.0737\n",
            "                     Mean symmetry loss: 0.0023\n",
            "                  Mean extrinsic reward: 24.65\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 24.65\n",
            "                    Mean episode length: 24.65\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:00:44\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 8/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1728 \n",
            "                       Steps per second: 725 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 60.1790\n",
            "                    Mean surrogate loss: 0.0088\n",
            "                      Mean entropy loss: 2.8446\n",
            "                          Mean rnd loss: 0.0532\n",
            "                     Mean symmetry loss: 0.0035\n",
            "                  Mean extrinsic reward: 26.82\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 26.82\n",
            "                    Mean episode length: 26.82\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:41\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 9/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1920 \n",
            "                       Steps per second: 721 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.191s \n",
            "                        Mean value loss: 66.2893\n",
            "                    Mean surrogate loss: 0.0071\n",
            "                      Mean entropy loss: 2.8456\n",
            "                          Mean rnd loss: 0.0434\n",
            "                     Mean symmetry loss: 0.0012\n",
            "                  Mean extrinsic reward: 26.82\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 26.82\n",
            "                    Mean episode length: 26.82\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:39\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 10/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2112 \n",
            "                       Steps per second: 683 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.205s \n",
            "                        Mean value loss: 58.2148\n",
            "                    Mean surrogate loss: 0.0187\n",
            "                      Mean entropy loss: 2.8484\n",
            "                          Mean rnd loss: 0.0564\n",
            "                     Mean symmetry loss: 0.0010\n",
            "                  Mean extrinsic reward: 35.63\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 35.63\n",
            "                    Mean episode length: 35.63\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 11/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2304 \n",
            "                       Steps per second: 664 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.212s \n",
            "                        Mean value loss: 48.3753\n",
            "                    Mean surrogate loss: 0.0019\n",
            "                      Mean entropy loss: 2.8492\n",
            "                          Mean rnd loss: 0.0511\n",
            "                     Mean symmetry loss: 0.0007\n",
            "                  Mean extrinsic reward: 36.64\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 36.64\n",
            "                    Mean episode length: 36.64\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 12/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2496 \n",
            "                       Steps per second: 707 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.196s \n",
            "                        Mean value loss: 50.1919\n",
            "                    Mean surrogate loss: 0.0059\n",
            "                      Mean entropy loss: 2.8489\n",
            "                          Mean rnd loss: 0.0508\n",
            "                     Mean symmetry loss: 0.0022\n",
            "                  Mean extrinsic reward: 37.50\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 37.50\n",
            "                    Mean episode length: 37.50\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:34\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 13/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2688 \n",
            "                       Steps per second: 720 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 45.2570\n",
            "                    Mean surrogate loss: -0.0154\n",
            "                      Mean entropy loss: 2.8474\n",
            "                          Mean rnd loss: 0.0222\n",
            "                     Mean symmetry loss: 0.0026\n",
            "                  Mean extrinsic reward: 40.18\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 40.18\n",
            "                    Mean episode length: 40.18\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:33\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 14/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 2880 \n",
            "                       Steps per second: 685 \n",
            "                        Collection time: 0.091s \n",
            "                          Learning time: 0.189s \n",
            "                        Mean value loss: 47.9074\n",
            "                    Mean surrogate loss: 0.0007\n",
            "                      Mean entropy loss: 2.8480\n",
            "                          Mean rnd loss: 0.0269\n",
            "                     Mean symmetry loss: 0.0038\n",
            "                  Mean extrinsic reward: 39.94\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 39.94\n",
            "                    Mean episode length: 39.94\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:32\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 15/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3072 \n",
            "                       Steps per second: 737 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.185s \n",
            "                        Mean value loss: 49.0262\n",
            "                    Mean surrogate loss: -0.0013\n",
            "                      Mean entropy loss: 2.8516\n",
            "                          Mean rnd loss: 0.0211\n",
            "                     Mean symmetry loss: 0.0118\n",
            "                  Mean extrinsic reward: 41.16\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 41.16\n",
            "                    Mean episode length: 41.16\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:31\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 16/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3264 \n",
            "                       Steps per second: 688 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.202s \n",
            "                        Mean value loss: 49.3071\n",
            "                    Mean surrogate loss: 0.0042\n",
            "                      Mean entropy loss: 2.8548\n",
            "                          Mean rnd loss: 0.0113\n",
            "                     Mean symmetry loss: 0.0106\n",
            "                  Mean extrinsic reward: 42.39\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 42.39\n",
            "                    Mean episode length: 42.39\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:30\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 17/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3456 \n",
            "                       Steps per second: 664 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.214s \n",
            "                        Mean value loss: 47.6098\n",
            "                    Mean surrogate loss: -0.0103\n",
            "                      Mean entropy loss: 2.8559\n",
            "                          Mean rnd loss: 0.0038\n",
            "                     Mean symmetry loss: 0.0077\n",
            "                  Mean extrinsic reward: 43.42\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 43.42\n",
            "                    Mean episode length: 43.42\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:30\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 18/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3648 \n",
            "                       Steps per second: 732 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 43.5453\n",
            "                    Mean surrogate loss: -0.0114\n",
            "                      Mean entropy loss: 2.8547\n",
            "                          Mean rnd loss: 0.0033\n",
            "                     Mean symmetry loss: 0.0056\n",
            "                  Mean extrinsic reward: 44.39\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 44.39\n",
            "                    Mean episode length: 44.39\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:29\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 19/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3840 \n",
            "                       Steps per second: 700 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.198s \n",
            "                        Mean value loss: 53.2824\n",
            "                    Mean surrogate loss: -0.0147\n",
            "                      Mean entropy loss: 2.8478\n",
            "                          Mean rnd loss: 0.0030\n",
            "                     Mean symmetry loss: 0.0132\n",
            "                  Mean extrinsic reward: 46.55\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 46.55\n",
            "                    Mean episode length: 46.55\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:28\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 20/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4032 \n",
            "                       Steps per second: 722 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 56.4349\n",
            "                    Mean surrogate loss: -0.0020\n",
            "                      Mean entropy loss: 2.8476\n",
            "                          Mean rnd loss: 0.0023\n",
            "                     Mean symmetry loss: 0.0063\n",
            "                  Mean extrinsic reward: 46.55\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 46.55\n",
            "                    Mean episode length: 46.55\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 21/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4224 \n",
            "                       Steps per second: 676 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.203s \n",
            "                        Mean value loss: 49.7936\n",
            "                    Mean surrogate loss: 0.0009\n",
            "                      Mean entropy loss: 2.8491\n",
            "                          Mean rnd loss: 0.0030\n",
            "                     Mean symmetry loss: 0.0155\n",
            "                  Mean extrinsic reward: 46.55\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 46.55\n",
            "                    Mean episode length: 46.55\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 22/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4416 \n",
            "                       Steps per second: 715 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.194s \n",
            "                        Mean value loss: 43.8560\n",
            "                    Mean surrogate loss: 0.0070\n",
            "                      Mean entropy loss: 2.8472\n",
            "                          Mean rnd loss: 0.0042\n",
            "                     Mean symmetry loss: 0.0172\n",
            "                  Mean extrinsic reward: 46.55\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 46.55\n",
            "                    Mean episode length: 46.55\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:26\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 23/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4608 \n",
            "                       Steps per second: 730 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 38.7354\n",
            "                    Mean surrogate loss: 0.0089\n",
            "                      Mean entropy loss: 2.8468\n",
            "                          Mean rnd loss: 0.0026\n",
            "                     Mean symmetry loss: 0.0320\n",
            "                  Mean extrinsic reward: 49.99\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 49.99\n",
            "                    Mean episode length: 49.99\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:26\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 24/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4800 \n",
            "                       Steps per second: 725 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 42.4752\n",
            "                    Mean surrogate loss: 0.0100\n",
            "                      Mean entropy loss: 2.8469\n",
            "                          Mean rnd loss: 0.0036\n",
            "                     Mean symmetry loss: 0.0421\n",
            "                  Mean extrinsic reward: 49.99\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 49.99\n",
            "                    Mean episode length: 49.99\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 25/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4992 \n",
            "                       Steps per second: 693 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.200s \n",
            "                        Mean value loss: 38.6775\n",
            "                    Mean surrogate loss: 0.0088\n",
            "                      Mean entropy loss: 2.8472\n",
            "                          Mean rnd loss: 0.0025\n",
            "                     Mean symmetry loss: 0.0273\n",
            "                  Mean extrinsic reward: 53.12\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 53.12\n",
            "                    Mean episode length: 53.12\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 26/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5184 \n",
            "                       Steps per second: 738 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.185s \n",
            "                        Mean value loss: 52.4028\n",
            "                    Mean surrogate loss: 0.0022\n",
            "                      Mean entropy loss: 2.8467\n",
            "                          Mean rnd loss: 0.0030\n",
            "                     Mean symmetry loss: 0.0092\n",
            "                  Mean extrinsic reward: 56.56\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 56.56\n",
            "                    Mean episode length: 56.56\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 27/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5376 \n",
            "                       Steps per second: 695 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.201s \n",
            "                        Mean value loss: 31.3987\n",
            "                    Mean surrogate loss: 0.0171\n",
            "                      Mean entropy loss: 2.8437\n",
            "                          Mean rnd loss: 0.0031\n",
            "                     Mean symmetry loss: 0.0027\n",
            "                  Mean extrinsic reward: 58.19\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 58.19\n",
            "                    Mean episode length: 58.19\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 28/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5568 \n",
            "                       Steps per second: 722 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.191s \n",
            "                        Mean value loss: 37.2902\n",
            "                    Mean surrogate loss: -0.0043\n",
            "                      Mean entropy loss: 2.8432\n",
            "                          Mean rnd loss: 0.0040\n",
            "                     Mean symmetry loss: 0.0012\n",
            "                  Mean extrinsic reward: 59.19\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 59.19\n",
            "                    Mean episode length: 59.19\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 29/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5760 \n",
            "                       Steps per second: 699 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.199s \n",
            "                        Mean value loss: 88.8978\n",
            "                    Mean surrogate loss: -0.0133\n",
            "                      Mean entropy loss: 2.8429\n",
            "                          Mean rnd loss: 0.0024\n",
            "                     Mean symmetry loss: 0.0012\n",
            "                  Mean extrinsic reward: 60.41\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 60.41\n",
            "                    Mean episode length: 60.41\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 30/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 5952 \n",
            "                       Steps per second: 723 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.189s \n",
            "                        Mean value loss: 78.3849\n",
            "                    Mean surrogate loss: 0.0239\n",
            "                      Mean entropy loss: 2.8418\n",
            "                          Mean rnd loss: 0.0040\n",
            "                     Mean symmetry loss: 0.0083\n",
            "                  Mean extrinsic reward: 60.97\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 60.97\n",
            "                    Mean episode length: 60.97\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 31/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6144 \n",
            "                       Steps per second: 719 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.191s \n",
            "                        Mean value loss: 73.5042\n",
            "                    Mean surrogate loss: 0.0003\n",
            "                      Mean entropy loss: 2.8410\n",
            "                          Mean rnd loss: 0.0024\n",
            "                     Mean symmetry loss: 0.0153\n",
            "                  Mean extrinsic reward: 60.91\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 60.91\n",
            "                    Mean episode length: 60.91\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 32/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6336 \n",
            "                       Steps per second: 724 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 21.7823\n",
            "                    Mean surrogate loss: 0.0291\n",
            "                      Mean entropy loss: 2.8402\n",
            "                          Mean rnd loss: 0.0039\n",
            "                     Mean symmetry loss: 0.0143\n",
            "                  Mean extrinsic reward: 60.91\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 60.91\n",
            "                    Mean episode length: 60.91\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 33/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6528 \n",
            "                       Steps per second: 708 \n",
            "                        Collection time: 0.088s \n",
            "                          Learning time: 0.183s \n",
            "                        Mean value loss: 28.0204\n",
            "                    Mean surrogate loss: -0.0045\n",
            "                      Mean entropy loss: 2.8404\n",
            "                          Mean rnd loss: 0.0057\n",
            "                     Mean symmetry loss: 0.0279\n",
            "                  Mean extrinsic reward: 62.33\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 62.33\n",
            "                    Mean episode length: 62.33\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 34/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6720 \n",
            "                       Steps per second: 721 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 96.8948\n",
            "                    Mean surrogate loss: 0.0175\n",
            "                      Mean entropy loss: 2.8408\n",
            "                          Mean rnd loss: 0.0044\n",
            "                     Mean symmetry loss: 0.0362\n",
            "                  Mean extrinsic reward: 63.01\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 63.01\n",
            "                    Mean episode length: 63.01\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 35/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6912 \n",
            "                       Steps per second: 724 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 19.9793\n",
            "                    Mean surrogate loss: 0.0070\n",
            "                      Mean entropy loss: 2.8413\n",
            "                          Mean rnd loss: 0.0067\n",
            "                     Mean symmetry loss: 0.0171\n",
            "                  Mean extrinsic reward: 63.01\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 63.01\n",
            "                    Mean episode length: 63.01\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 36/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7104 \n",
            "                       Steps per second: 713 \n",
            "                        Collection time: 0.079s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 132.1281\n",
            "                    Mean surrogate loss: -0.0191\n",
            "                      Mean entropy loss: 2.8420\n",
            "                          Mean rnd loss: 0.0065\n",
            "                     Mean symmetry loss: 0.0123\n",
            "                  Mean extrinsic reward: 65.62\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 65.62\n",
            "                    Mean episode length: 65.62\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 37/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7296 \n",
            "                       Steps per second: 664 \n",
            "                        Collection time: 0.088s \n",
            "                          Learning time: 0.201s \n",
            "                        Mean value loss: 30.2802\n",
            "                    Mean surrogate loss: 0.0138\n",
            "                      Mean entropy loss: 2.8426\n",
            "                          Mean rnd loss: 0.0065\n",
            "                     Mean symmetry loss: 0.0336\n",
            "                  Mean extrinsic reward: 65.62\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 65.62\n",
            "                    Mean episode length: 65.62\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 38/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7488 \n",
            "                       Steps per second: 695 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.199s \n",
            "                        Mean value loss: 13.3976\n",
            "                    Mean surrogate loss: 0.0705\n",
            "                      Mean entropy loss: 2.8442\n",
            "                          Mean rnd loss: 0.0062\n",
            "                     Mean symmetry loss: 0.0231\n",
            "                  Mean extrinsic reward: 65.62\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 65.62\n",
            "                    Mean episode length: 65.62\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 39/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7680 \n",
            "                       Steps per second: 728 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.189s \n",
            "                        Mean value loss: 27.9023\n",
            "                    Mean surrogate loss: 0.0108\n",
            "                      Mean entropy loss: 2.8441\n",
            "                          Mean rnd loss: 0.0027\n",
            "                     Mean symmetry loss: 0.0009\n",
            "                  Mean extrinsic reward: 68.90\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 68.90\n",
            "                    Mean episode length: 68.90\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 40/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7872 \n",
            "                       Steps per second: 695 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.202s \n",
            "                        Mean value loss: 9.1665\n",
            "                    Mean surrogate loss: 0.0088\n",
            "                      Mean entropy loss: 2.8420\n",
            "                          Mean rnd loss: 0.0023\n",
            "                     Mean symmetry loss: 0.0071\n",
            "                  Mean extrinsic reward: 68.90\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 68.90\n",
            "                    Mean episode length: 68.90\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 41/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8064 \n",
            "                       Steps per second: 731 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 8.0568\n",
            "                    Mean surrogate loss: 0.0023\n",
            "                      Mean entropy loss: 2.8415\n",
            "                          Mean rnd loss: 0.0027\n",
            "                     Mean symmetry loss: 0.0490\n",
            "                  Mean extrinsic reward: 68.90\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 68.90\n",
            "                    Mean episode length: 68.90\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 42/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8256 \n",
            "                       Steps per second: 720 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 176.7417\n",
            "                    Mean surrogate loss: 0.0619\n",
            "                      Mean entropy loss: 2.8416\n",
            "                          Mean rnd loss: 0.0027\n",
            "                     Mean symmetry loss: 0.5213\n",
            "                  Mean extrinsic reward: 72.14\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 72.14\n",
            "                    Mean episode length: 72.14\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 43/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8448 \n",
            "                       Steps per second: 521 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.293s \n",
            "                        Mean value loss: 174.3592\n",
            "                    Mean surrogate loss: 0.0910\n",
            "                      Mean entropy loss: 2.8416\n",
            "                          Mean rnd loss: 0.0025\n",
            "                     Mean symmetry loss: 0.5141\n",
            "                  Mean extrinsic reward: 73.24\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 73.24\n",
            "                    Mean episode length: 73.24\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.37s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 44/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8640 \n",
            "                       Steps per second: 518 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.264s \n",
            "                        Mean value loss: 199.9404\n",
            "                    Mean surrogate loss: 0.0420\n",
            "                      Mean entropy loss: 2.8406\n",
            "                          Mean rnd loss: 0.0018\n",
            "                     Mean symmetry loss: 0.0831\n",
            "                  Mean extrinsic reward: 75.86\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 75.86\n",
            "                    Mean episode length: 75.86\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.37s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 45/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 8832 \n",
            "                       Steps per second: 539 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.261s \n",
            "                        Mean value loss: 5.9152\n",
            "                    Mean surrogate loss: 0.0289\n",
            "                      Mean entropy loss: 2.8396\n",
            "                          Mean rnd loss: 0.0019\n",
            "                     Mean symmetry loss: 0.0329\n",
            "                  Mean extrinsic reward: 75.86\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 75.86\n",
            "                    Mean episode length: 75.86\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.36s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 46/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9024 \n",
            "                       Steps per second: 551 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.249s \n",
            "                        Mean value loss: 145.1759\n",
            "                    Mean surrogate loss: -0.0169\n",
            "                      Mean entropy loss: 2.8396\n",
            "                          Mean rnd loss: 0.0024\n",
            "                     Mean symmetry loss: 0.0119\n",
            "                  Mean extrinsic reward: 79.44\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 79.44\n",
            "                    Mean episode length: 79.44\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.35s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 47/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9216 \n",
            "                       Steps per second: 515 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.272s \n",
            "                        Mean value loss: 220.1868\n",
            "                    Mean surrogate loss: 0.0126\n",
            "                      Mean entropy loss: 2.8395\n",
            "                          Mean rnd loss: 0.0031\n",
            "                     Mean symmetry loss: 0.0428\n",
            "                  Mean extrinsic reward: 79.95\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 79.95\n",
            "                    Mean episode length: 79.95\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.37s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 48/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9408 \n",
            "                       Steps per second: 478 \n",
            "                        Collection time: 0.108s \n",
            "                          Learning time: 0.293s \n",
            "                        Mean value loss: 158.5026\n",
            "                    Mean surrogate loss: 0.0005\n",
            "                      Mean entropy loss: 2.8394\n",
            "                          Mean rnd loss: 0.0035\n",
            "                     Mean symmetry loss: 0.0284\n",
            "                  Mean extrinsic reward: 84.65\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 84.65\n",
            "                    Mean episode length: 84.65\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.40s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 49/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9600 \n",
            "                       Steps per second: 478 \n",
            "                        Collection time: 0.109s \n",
            "                          Learning time: 0.293s \n",
            "                        Mean value loss: 147.3479\n",
            "                    Mean surrogate loss: 0.0018\n",
            "                      Mean entropy loss: 2.8395\n",
            "                          Mean rnd loss: 0.0022\n",
            "                     Mean symmetry loss: 0.0226\n",
            "                  Mean extrinsic reward: 88.61\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 88.61\n",
            "                    Mean episode length: 88.61\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.40s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 50/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9792 \n",
            "                       Steps per second: 653 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.198s \n",
            "                        Mean value loss: 54.5531\n",
            "                    Mean surrogate loss: 0.0106\n",
            "                      Mean entropy loss: 2.8405\n",
            "                          Mean rnd loss: 0.0010\n",
            "                     Mean symmetry loss: 0.0297\n",
            "                  Mean extrinsic reward: 93.20\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 93.20\n",
            "                    Mean episode length: 93.20\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 51/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9984 \n",
            "                       Steps per second: 715 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 29.4327\n",
            "                    Mean surrogate loss: 0.0039\n",
            "                      Mean entropy loss: 2.8411\n",
            "                          Mean rnd loss: 0.0008\n",
            "                     Mean symmetry loss: 0.0170\n",
            "                  Mean extrinsic reward: 93.99\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 93.99\n",
            "                    Mean episode length: 93.99\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 52/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10176 \n",
            "                       Steps per second: 728 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.189s \n",
            "                        Mean value loss: 18.7699\n",
            "                    Mean surrogate loss: 0.0102\n",
            "                      Mean entropy loss: 2.8411\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0194\n",
            "                  Mean extrinsic reward: 93.99\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 93.99\n",
            "                    Mean episode length: 93.99\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 53/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10368 \n",
            "                       Steps per second: 655 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.217s \n",
            "                        Mean value loss: 41.1776\n",
            "                    Mean surrogate loss: 0.0077\n",
            "                      Mean entropy loss: 2.8409\n",
            "                          Mean rnd loss: 0.0008\n",
            "                     Mean symmetry loss: 0.0230\n",
            "                  Mean extrinsic reward: 95.18\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 95.18\n",
            "                    Mean episode length: 95.18\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 54/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10560 \n",
            "                       Steps per second: 671 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.210s \n",
            "                        Mean value loss: 56.6310\n",
            "                    Mean surrogate loss: 0.0064\n",
            "                      Mean entropy loss: 2.8410\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: 0.0188\n",
            "                  Mean extrinsic reward: 98.40\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 98.40\n",
            "                    Mean episode length: 98.40\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 55/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10752 \n",
            "                       Steps per second: 709 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.196s \n",
            "                        Mean value loss: 57.1011\n",
            "                    Mean surrogate loss: 0.0045\n",
            "                      Mean entropy loss: 2.8409\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0183\n",
            "                  Mean extrinsic reward: 99.27\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 99.27\n",
            "                    Mean episode length: 99.27\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 56/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10944 \n",
            "                       Steps per second: 687 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.203s \n",
            "                        Mean value loss: 93.3679\n",
            "                    Mean surrogate loss: 0.0052\n",
            "                      Mean entropy loss: 2.8407\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0213\n",
            "                  Mean extrinsic reward: 101.66\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 101.66\n",
            "                    Mean episode length: 101.66\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 57/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11136 \n",
            "                       Steps per second: 665 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.213s \n",
            "                        Mean value loss: 55.3583\n",
            "                    Mean surrogate loss: 0.0070\n",
            "                      Mean entropy loss: 2.8405\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0213\n",
            "                  Mean extrinsic reward: 104.61\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 104.61\n",
            "                    Mean episode length: 104.61\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 58/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11328 \n",
            "                       Steps per second: 706 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.193s \n",
            "                        Mean value loss: 87.4173\n",
            "                    Mean surrogate loss: 0.0030\n",
            "                      Mean entropy loss: 2.8401\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0185\n",
            "                  Mean extrinsic reward: 104.61\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 104.61\n",
            "                    Mean episode length: 104.61\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 59/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11520 \n",
            "                       Steps per second: 721 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.189s \n",
            "                        Mean value loss: 190.0115\n",
            "                    Mean surrogate loss: 0.0032\n",
            "                      Mean entropy loss: 2.8400\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0093\n",
            "                  Mean extrinsic reward: 105.55\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 105.55\n",
            "                    Mean episode length: 105.55\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 60/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11712 \n",
            "                       Steps per second: 730 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 163.5612\n",
            "                    Mean surrogate loss: -0.0016\n",
            "                      Mean entropy loss: 2.8407\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0049\n",
            "                  Mean extrinsic reward: 105.55\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 105.55\n",
            "                    Mean episode length: 105.55\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 61/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 11904 \n",
            "                       Steps per second: 686 \n",
            "                        Collection time: 0.087s \n",
            "                          Learning time: 0.193s \n",
            "                        Mean value loss: 83.5674\n",
            "                    Mean surrogate loss: -0.0044\n",
            "                      Mean entropy loss: 2.8417\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: 0.0032\n",
            "                  Mean extrinsic reward: 107.79\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 107.79\n",
            "                    Mean episode length: 107.79\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 62/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12096 \n",
            "                       Steps per second: 714 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.193s \n",
            "                        Mean value loss: 20.3569\n",
            "                    Mean surrogate loss: 0.0263\n",
            "                      Mean entropy loss: 2.8436\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0038\n",
            "                  Mean extrinsic reward: 112.45\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 112.45\n",
            "                    Mean episode length: 112.45\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 63/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12288 \n",
            "                       Steps per second: 725 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.189s \n",
            "                        Mean value loss: 13.0153\n",
            "                    Mean surrogate loss: -0.0115\n",
            "                      Mean entropy loss: 2.8437\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0096\n",
            "                  Mean extrinsic reward: 112.45\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 112.45\n",
            "                    Mean episode length: 112.45\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 64/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12480 \n",
            "                       Steps per second: 681 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.205s \n",
            "                        Mean value loss: 19.6864\n",
            "                    Mean surrogate loss: 0.0028\n",
            "                      Mean entropy loss: 2.8451\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0052\n",
            "                  Mean extrinsic reward: 113.79\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 113.79\n",
            "                    Mean episode length: 113.79\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 65/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12672 \n",
            "                       Steps per second: 693 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.200s \n",
            "                        Mean value loss: 13.6951\n",
            "                    Mean surrogate loss: 0.0107\n",
            "                      Mean entropy loss: 2.8447\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0041\n",
            "                  Mean extrinsic reward: 115.62\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 115.62\n",
            "                    Mean episode length: 115.62\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 66/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12864 \n",
            "                       Steps per second: 735 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.183s \n",
            "                        Mean value loss: 9.0380\n",
            "                    Mean surrogate loss: 0.0022\n",
            "                      Mean entropy loss: 2.8441\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0069\n",
            "                  Mean extrinsic reward: 117.12\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 117.12\n",
            "                    Mean episode length: 117.12\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 67/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13056 \n",
            "                       Steps per second: 730 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.188s \n",
            "                        Mean value loss: 9.6652\n",
            "                    Mean surrogate loss: 0.0247\n",
            "                      Mean entropy loss: 2.8424\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0106\n",
            "                  Mean extrinsic reward: 118.17\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 118.17\n",
            "                    Mean episode length: 118.17\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 68/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13248 \n",
            "                       Steps per second: 681 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.206s \n",
            "                        Mean value loss: 6.8624\n",
            "                    Mean surrogate loss: -0.0005\n",
            "                      Mean entropy loss: 2.8424\n",
            "                          Mean rnd loss: 0.0001\n",
            "                     Mean symmetry loss: 0.0085\n",
            "                  Mean extrinsic reward: 121.11\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 121.11\n",
            "                    Mean episode length: 121.11\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 69/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13440 \n",
            "                       Steps per second: 714 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.192s \n",
            "                        Mean value loss: 20.9025\n",
            "                    Mean surrogate loss: 0.0087\n",
            "                      Mean entropy loss: 2.8431\n",
            "                          Mean rnd loss: 0.0007\n",
            "                     Mean symmetry loss: 0.0040\n",
            "                  Mean extrinsic reward: 122.16\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 122.16\n",
            "                    Mean episode length: 122.16\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 70/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13632 \n",
            "                       Steps per second: 726 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 6.4009\n",
            "                    Mean surrogate loss: -0.0070\n",
            "                      Mean entropy loss: 2.8432\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0017\n",
            "                  Mean extrinsic reward: 123.73\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 123.73\n",
            "                    Mean episode length: 123.73\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 71/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13824 \n",
            "                       Steps per second: 722 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 11.3323\n",
            "                    Mean surrogate loss: -0.0064\n",
            "                      Mean entropy loss: 2.8434\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0024\n",
            "                  Mean extrinsic reward: 125.23\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 125.23\n",
            "                    Mean episode length: 125.23\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 72/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14016 \n",
            "                       Steps per second: 593 \n",
            "                        Collection time: 0.081s \n",
            "                          Learning time: 0.242s \n",
            "                        Mean value loss: 8.9700\n",
            "                    Mean surrogate loss: -0.0020\n",
            "                      Mean entropy loss: 2.8434\n",
            "                          Mean rnd loss: 0.0008\n",
            "                     Mean symmetry loss: 0.0043\n",
            "                  Mean extrinsic reward: 126.50\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 126.50\n",
            "                    Mean episode length: 126.50\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.32s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 73/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14208 \n",
            "                       Steps per second: 640 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.220s \n",
            "                        Mean value loss: 13.4643\n",
            "                    Mean surrogate loss: 0.0028\n",
            "                      Mean entropy loss: 2.8436\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: 0.0043\n",
            "                  Mean extrinsic reward: 127.81\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 127.81\n",
            "                    Mean episode length: 127.81\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 74/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14400 \n",
            "                       Steps per second: 712 \n",
            "                        Collection time: 0.078s \n",
            "                          Learning time: 0.192s \n",
            "                        Mean value loss: 13.4119\n",
            "                    Mean surrogate loss: -0.0031\n",
            "                      Mean entropy loss: 2.8441\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0053\n",
            "                  Mean extrinsic reward: 129.28\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 129.28\n",
            "                    Mean episode length: 129.28\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 75/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14592 \n",
            "                       Steps per second: 711 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.194s \n",
            "                        Mean value loss: 15.8472\n",
            "                    Mean surrogate loss: 0.0013\n",
            "                      Mean entropy loss: 2.8446\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0062\n",
            "                  Mean extrinsic reward: 130.47\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 130.47\n",
            "                    Mean episode length: 130.47\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 76/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14784 \n",
            "                       Steps per second: 685 \n",
            "                        Collection time: 0.089s \n",
            "                          Learning time: 0.192s \n",
            "                        Mean value loss: 6.9516\n",
            "                    Mean surrogate loss: -0.0124\n",
            "                      Mean entropy loss: 2.8455\n",
            "                          Mean rnd loss: 0.0002\n",
            "                     Mean symmetry loss: 0.0085\n",
            "                  Mean extrinsic reward: 130.47\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 130.47\n",
            "                    Mean episode length: 130.47\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 77/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 14976 \n",
            "                       Steps per second: 726 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 8.8921\n",
            "                    Mean surrogate loss: 0.0007\n",
            "                      Mean entropy loss: 2.8462\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0032\n",
            "                  Mean extrinsic reward: 130.47\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 130.47\n",
            "                    Mean episode length: 130.47\n",
            "                  Mean action noise std: 1.00\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 78/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15168 \n",
            "                       Steps per second: 728 \n",
            "                        Collection time: 0.074s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 12.1830\n",
            "                    Mean surrogate loss: 0.1597\n",
            "                      Mean entropy loss: 2.8497\n",
            "                          Mean rnd loss: 0.0012\n",
            "                     Mean symmetry loss: 0.1585\n",
            "                  Mean extrinsic reward: 132.02\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 132.02\n",
            "                    Mean episode length: 132.02\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 79/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15360 \n",
            "                       Steps per second: 725 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.189s \n",
            "                        Mean value loss: 20.7352\n",
            "                    Mean surrogate loss: 0.0783\n",
            "                      Mean entropy loss: 2.8521\n",
            "                          Mean rnd loss: 0.0007\n",
            "                     Mean symmetry loss: 0.1742\n",
            "                  Mean extrinsic reward: 134.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 134.07\n",
            "                    Mean episode length: 134.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 80/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15552 \n",
            "                       Steps per second: 680 \n",
            "                        Collection time: 0.090s \n",
            "                          Learning time: 0.192s \n",
            "                        Mean value loss: 39.5382\n",
            "                    Mean surrogate loss: 0.0118\n",
            "                      Mean entropy loss: 2.8522\n",
            "                          Mean rnd loss: 0.0019\n",
            "                     Mean symmetry loss: 0.0185\n",
            "                  Mean extrinsic reward: 134.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 134.07\n",
            "                    Mean episode length: 134.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 81/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15744 \n",
            "                       Steps per second: 733 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.186s \n",
            "                        Mean value loss: 39.0876\n",
            "                    Mean surrogate loss: 0.0047\n",
            "                      Mean entropy loss: 2.8523\n",
            "                          Mean rnd loss: 0.0025\n",
            "                     Mean symmetry loss: 0.0131\n",
            "                  Mean extrinsic reward: 134.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 134.07\n",
            "                    Mean episode length: 134.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 82/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15936 \n",
            "                       Steps per second: 731 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 10.3903\n",
            "                    Mean surrogate loss: -0.0076\n",
            "                      Mean entropy loss: 2.8525\n",
            "                          Mean rnd loss: 0.0014\n",
            "                     Mean symmetry loss: 0.0143\n",
            "                  Mean extrinsic reward: 134.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 134.07\n",
            "                    Mean episode length: 134.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 83/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16128 \n",
            "                       Steps per second: 685 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.204s \n",
            "                        Mean value loss: 14.5196\n",
            "                    Mean surrogate loss: 0.0022\n",
            "                      Mean entropy loss: 2.8526\n",
            "                          Mean rnd loss: 0.0011\n",
            "                     Mean symmetry loss: 0.0092\n",
            "                  Mean extrinsic reward: 134.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 134.07\n",
            "                    Mean episode length: 134.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 84/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16320 \n",
            "                       Steps per second: 718 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.192s \n",
            "                        Mean value loss: 10.5138\n",
            "                    Mean surrogate loss: 0.0090\n",
            "                      Mean entropy loss: 2.8531\n",
            "                          Mean rnd loss: 0.0009\n",
            "                     Mean symmetry loss: 0.0095\n",
            "                  Mean extrinsic reward: 134.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 134.07\n",
            "                    Mean episode length: 134.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 85/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16512 \n",
            "                       Steps per second: 731 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.187s \n",
            "                        Mean value loss: 16.9141\n",
            "                    Mean surrogate loss: 0.0042\n",
            "                      Mean entropy loss: 2.8533\n",
            "                          Mean rnd loss: 0.0008\n",
            "                     Mean symmetry loss: 0.0041\n",
            "                  Mean extrinsic reward: 134.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 134.07\n",
            "                    Mean episode length: 134.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 86/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16704 \n",
            "                       Steps per second: 553 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.272s \n",
            "                        Mean value loss: 26.2634\n",
            "                    Mean surrogate loss: -0.0027\n",
            "                      Mean entropy loss: 2.8533\n",
            "                          Mean rnd loss: 0.0015\n",
            "                     Mean symmetry loss: 0.0023\n",
            "                  Mean extrinsic reward: 134.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 134.07\n",
            "                    Mean episode length: 134.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.35s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 87/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16896 \n",
            "                       Steps per second: 493 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.288s \n",
            "                        Mean value loss: 6.9749\n",
            "                    Mean surrogate loss: -0.0006\n",
            "                      Mean entropy loss: 2.8529\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0056\n",
            "                  Mean extrinsic reward: 134.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 134.07\n",
            "                    Mean episode length: 134.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.39s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 88/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17088 \n",
            "                       Steps per second: 537 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.263s \n",
            "                        Mean value loss: 29.8254\n",
            "                    Mean surrogate loss: -0.0048\n",
            "                      Mean entropy loss: 2.8522\n",
            "                          Mean rnd loss: 0.0009\n",
            "                     Mean symmetry loss: 0.0029\n",
            "                  Mean extrinsic reward: 134.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 134.07\n",
            "                    Mean episode length: 134.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.36s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 89/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17280 \n",
            "                       Steps per second: 528 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.262s \n",
            "                        Mean value loss: 6.9003\n",
            "                    Mean surrogate loss: -0.0007\n",
            "                      Mean entropy loss: 2.8521\n",
            "                          Mean rnd loss: 0.0013\n",
            "                     Mean symmetry loss: 0.0026\n",
            "                  Mean extrinsic reward: 134.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 134.07\n",
            "                    Mean episode length: 134.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.36s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 90/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17472 \n",
            "                       Steps per second: 459 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.318s \n",
            "                        Mean value loss: 8.1918\n",
            "                    Mean surrogate loss: -0.0038\n",
            "                      Mean entropy loss: 2.8528\n",
            "                          Mean rnd loss: 0.0012\n",
            "                     Mean symmetry loss: 0.0085\n",
            "                  Mean extrinsic reward: 134.07\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 134.07\n",
            "                    Mean episode length: 134.07\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.42s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 91/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17664 \n",
            "                       Steps per second: 497 \n",
            "                        Collection time: 0.113s \n",
            "                          Learning time: 0.273s \n",
            "                        Mean value loss: 5.0425\n",
            "                    Mean surrogate loss: 0.0182\n",
            "                      Mean entropy loss: 2.8541\n",
            "                          Mean rnd loss: 0.0006\n",
            "                     Mean symmetry loss: 0.0953\n",
            "                  Mean extrinsic reward: 138.71\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 138.71\n",
            "                    Mean episode length: 138.71\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.39s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 92/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 17856 \n",
            "                       Steps per second: 442 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.328s \n",
            "                        Mean value loss: 9.1276\n",
            "                    Mean surrogate loss: 0.0191\n",
            "                      Mean entropy loss: 2.8543\n",
            "                          Mean rnd loss: 0.0007\n",
            "                     Mean symmetry loss: 0.0367\n",
            "                  Mean extrinsic reward: 143.13\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 143.13\n",
            "                    Mean episode length: 143.13\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.43s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 93/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18048 \n",
            "                       Steps per second: 649 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.190s \n",
            "                        Mean value loss: 5.7791\n",
            "                    Mean surrogate loss: 0.0034\n",
            "                      Mean entropy loss: 2.8544\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: 0.0053\n",
            "                  Mean extrinsic reward: 152.31\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 152.31\n",
            "                    Mean episode length: 152.31\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 94/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18240 \n",
            "                       Steps per second: 709 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.194s \n",
            "                        Mean value loss: 1.8042\n",
            "                    Mean surrogate loss: -0.0050\n",
            "                      Mean entropy loss: 2.8543\n",
            "                          Mean rnd loss: 0.0005\n",
            "                     Mean symmetry loss: 0.0312\n",
            "                  Mean extrinsic reward: 152.31\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 152.31\n",
            "                    Mean episode length: 152.31\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 95/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18432 \n",
            "                       Steps per second: 715 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.193s \n",
            "                        Mean value loss: 2.3297\n",
            "                    Mean surrogate loss: 0.0152\n",
            "                      Mean entropy loss: 2.8546\n",
            "                          Mean rnd loss: 0.0004\n",
            "                     Mean symmetry loss: 0.0418\n",
            "                  Mean extrinsic reward: 159.97\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 159.97\n",
            "                    Mean episode length: 159.97\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 96/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18624 \n",
            "                       Steps per second: 650 \n",
            "                        Collection time: 0.077s \n",
            "                          Learning time: 0.219s \n",
            "                        Mean value loss: 2.2439\n",
            "                    Mean surrogate loss: 0.0050\n",
            "                      Mean entropy loss: 2.8544\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0195\n",
            "                  Mean extrinsic reward: 159.97\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 159.97\n",
            "                    Mean episode length: 159.97\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 97/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18816 \n",
            "                       Steps per second: 722 \n",
            "                        Collection time: 0.075s \n",
            "                          Learning time: 0.191s \n",
            "                        Mean value loss: 2.2228\n",
            "                    Mean surrogate loss: 0.0049\n",
            "                      Mean entropy loss: 2.8545\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0092\n",
            "                  Mean extrinsic reward: 159.97\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 159.97\n",
            "                    Mean episode length: 159.97\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 98/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 19008 \n",
            "                       Steps per second: 703 \n",
            "                        Collection time: 0.076s \n",
            "                          Learning time: 0.197s \n",
            "                        Mean value loss: 2.0668\n",
            "                    Mean surrogate loss: 0.0073\n",
            "                      Mean entropy loss: 2.8546\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0255\n",
            "                  Mean extrinsic reward: 159.97\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 159.97\n",
            "                    Mean episode length: 159.97\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:30\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 99/100                            \u001b[0m \n",
            "\n",
            "                            Total steps: 19200 \n",
            "                       Steps per second: 695 \n",
            "                        Collection time: 0.080s \n",
            "                          Learning time: 0.196s \n",
            "                        Mean value loss: 1.1878\n",
            "                    Mean surrogate loss: 0.0083\n",
            "                      Mean entropy loss: 2.8545\n",
            "                          Mean rnd loss: 0.0003\n",
            "                     Mean symmetry loss: 0.0086\n",
            "                  Mean extrinsic reward: 167.77\n",
            "                  Mean intrinsic reward: 0.00\n",
            "                            Mean reward: 167.77\n",
            "                    Mean episode length: 167.77\n",
            "                  Mean action noise std: 1.01\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:30\n",
            "                                    ETA: 00:00:00\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6KT0EUrmkh-9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "cNZ2S4AluCzJ",
        "_SHQZPZ4s9l9",
        "w5LlVSVlNy9v",
        "FgAzIhuRUJQ7",
        "ExFQ72cLiLzJ",
        "L5W0AuN9tffN"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}